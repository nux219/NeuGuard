{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import shutil\n",
    "\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from pruned_layers import *\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "# from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "from privacy_risk_score_utils import *\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "import urllib\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = './datasets/texas/'\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    mkdir_p(DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# # default way to load texas100 data\n",
    "# DATASET_FEATURES = os.path.join(DATASET_PATH,'texas/100/feats')\n",
    "# DATASET_LABELS = os.path.join(DATASET_PATH,'texas/100/labels')\n",
    "\n",
    "# if not os.path.isfile(DATASET_FEATURES):\n",
    "#     print(\"Dowloading the dataset...\")\n",
    "#     urllib.urlretrieve(\"https://www.comp.nus.edu.sg/~reza/files/dataset_texas.tgz\",os.path.join(DATASET_PATH,'tmp.tgz'))\n",
    "#     print('Dataset Downloaded')\n",
    "#     tar = tarfile.open(os.path.join(DATASET_PATH,'tmp.tgz'))\n",
    "#     tar.extractall(path=DATASET_PATH)\n",
    "    \n",
    "# X=[]\n",
    "# Y=[]\n",
    "\n",
    "# data_set_features =np.genfromtxt(DATASET_FEATURES,delimiter=',')\n",
    "# data_set_label =np.genfromtxt(DATASET_LABELS,delimiter=',')\n",
    "\n",
    "# X = data_set_features.astype(np.float64)\n",
    "# Y = data_set_label.astype(np.int32)-1\n",
    "\n",
    "# # # np.savez('texas100_data', x = X, y = Y)\n",
    "# # # np.savez('texas100_index', x = init_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# You can save and load a npz file for faster loading\n",
    "npzdata=np.load('./texas100_data.npz')\n",
    "X=npzdata['x'][:,:]\n",
    "Y=npzdata['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# np.array_equal(X, X_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67330, 6169)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # larger model\n",
    "# class Texas(nn.Module):\n",
    "#     def __init__(self,num_classes=100):\n",
    "#         super(Texas, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "# #             nn.Linear(6169,1024),\n",
    "#             PruneLinear(6169,2048),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(2048,1024),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(1024,512),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(512,256),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(256,128),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(128,num_classes)\n",
    "# #         for key in self.state_dict():\n",
    "# #             if key.split('.')[-1] == 'weight':    \n",
    "# #                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "# #                 print (key)\n",
    "                \n",
    "# #             elif key.split('.')[-1] == 'bias':\n",
    "# #                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         hidden_out = self.features(x)\n",
    "        \n",
    "#         return self.classifier(hidden_out),hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# larger model\n",
    "class Texas_layer_out(nn.Module):\n",
    "    def __init__(self,num_classes=100):\n",
    "        super(Texas_layer_out, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "        self.fc1 = PruneLinear(6169,1024)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "#         self.fc2 = PruneLinear(2048,1024)\n",
    "#         self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = PruneLinear(1024,512)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.fc4 = PruneLinear(512,256)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = PruneLinear(256,128)\n",
    "        self.tanh5 = nn.Tanh()\n",
    "#         )\n",
    "        self.classifier = nn.Linear(128,num_classes)\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.fc1(x)\n",
    "        y1 = self.tanh1(y)\n",
    "#         y = self.fc2(y1)\n",
    "#         y2 = self.tanh2(y)\n",
    "        y = self.fc3(y1)\n",
    "        y3 = self.tanh3(y)\n",
    "        y = self.fc4(y3)\n",
    "        y4 = self.tanh4(y)\n",
    "        y = self.fc5(y4)\n",
    "        y5 = self.tanh5(y)\n",
    "        output = self.classifier(y5)\n",
    "        \n",
    "        return output, y1, y3, y4, y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# larger model\n",
    "class Texas_layer_out_scale(nn.Module):\n",
    "    def __init__(self,num_classes=100, q = 100, alpha = 1):\n",
    "        super(Texas_layer_out_scale, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "        self.fc1 = PruneLinear(6169,1024)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "#         self.fc2 = PruneLinear(2048,1024)\n",
    "#         self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = PruneLinear(1024,512)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.fc4 = PruneLinear(512,256)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = PruneLinear(256,128)\n",
    "        self.tanh5 = nn.Tanh()\n",
    "#         )\n",
    "        self.classifier = nn.Linear(128,num_classes)\n",
    "    \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.fc1(x)\n",
    "        y1 = self.tanh1(y)\n",
    "        y1 = scale_by_percentage(y1, q=self.q, alpha = self.alpha)\n",
    "#         y = self.fc2(y1)\n",
    "#         y2 = self.tanh2(y)\n",
    "        y = self.fc3(y1)\n",
    "        y3 = self.tanh3(y)\n",
    "        y3 = scale_by_percentage(y3, q=self.q, alpha = self.alpha)\n",
    "        y = self.fc4(y3)\n",
    "        y4 = self.tanh4(y)\n",
    "        y4 = scale_by_percentage(y4, q=self.q, alpha = self.alpha)\n",
    "        y = self.fc5(y4)\n",
    "        y5 = self.tanh5(y)\n",
    "        y5 = scale_by_percentage(y5, q=self.q, alpha = self.alpha)\n",
    "        output = self.classifier(y5)\n",
    "        \n",
    "        return output, y1, y3, y4, y5\n",
    "\n",
    "    \n",
    "# def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "#     \"\"\"\n",
    "#     scale paramters by threshold.\n",
    "\n",
    "#     \"\"\"\n",
    "#     temp_shape = x.shape\n",
    "#     weight = x.data.cpu().numpy()\n",
    "#     flattened_weights = np.abs(weight.flatten())\n",
    "#     nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "#     percentile_value = np.percentile(nonzero, q)\n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "#     return x\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "#     flattened_weights = np.abs(weight)\n",
    "#     percentile_value = np.percentile(flattened_weights, q)\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value  = np.percentile(nonzero, q)\n",
    "    amp = alpha - 1.\n",
    "    \n",
    "    tweight = x.data\n",
    "    new_mask = (tweight >= percentile_value)*amp  + 1.\n",
    "    \n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "\n",
    "##     new_mask = (flattened_weights >= percentile_value)*amp + 1.\n",
    "\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    x.data  = new_mask * tweight\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# larger model\n",
    "class Texas_layer_out_scale2(nn.Module):\n",
    "    def __init__(self,num_classes=100, q = 100, alpha = 1):\n",
    "        super(Texas_layer_out_scale2, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "        self.fc1 = PruneLinear(6169,1024)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "#         self.fc2 = PruneLinear(2048,1024)\n",
    "#         self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = PruneLinear(1024,512)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.fc4 = PruneLinear(512,256)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = PruneLinear(256,128)\n",
    "        self.tanh5 = nn.Tanh()\n",
    "#         )\n",
    "        self.classifier = nn.Linear(128,num_classes)\n",
    "    \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.fc1(x)\n",
    "        y1 = self.tanh1(y)\n",
    "#         y1 = scale_by_percentage(y1, q=self.q, alpha = self.alpha)\n",
    "#         y = self.fc2(y1)\n",
    "#         y2 = self.tanh2(y)\n",
    "        y = self.fc3(y1)\n",
    "        y3 = self.tanh3(y)\n",
    "#         y3 = scale_by_percentage(y3, q=self.q, alpha = self.alpha)\n",
    "        y = self.fc4(y3)\n",
    "        y4 = self.tanh4(y)\n",
    "        y4 = scale_by_percentage(y4, q=self.q, alpha = self.alpha)\n",
    "        y = self.fc5(y4)\n",
    "        y5 = self.tanh5(y)\n",
    "        y5 = scale_by_percentage(y5, q=self.q, alpha = self.alpha)\n",
    "        output = self.classifier(y5)\n",
    "        \n",
    "        return output, y1, y3, y4, y5\n",
    "\n",
    "    \n",
    "# def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "#     \"\"\"\n",
    "#     scale paramters by threshold.\n",
    "\n",
    "#     \"\"\"\n",
    "#     temp_shape = x.shape\n",
    "#     weight = x.data.cpu().numpy()\n",
    "#     flattened_weights = np.abs(weight.flatten())\n",
    "#     nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "#     percentile_value = np.percentile(nonzero, q)\n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "#     return x\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "#     flattened_weights = np.abs(weight)\n",
    "#     percentile_value = np.percentile(flattened_weights, q)\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value  = np.percentile(nonzero, q)\n",
    "    amp = alpha - 1.\n",
    "    \n",
    "    tweight = x.data\n",
    "    new_mask = (tweight >= percentile_value)*amp  + 1.\n",
    "    \n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "\n",
    "##     new_mask = (flattened_weights >= percentile_value)*amp + 1.\n",
    "\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    x.data  = new_mask * tweight\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# larger model\n",
    "class Texas_layer_out_scale2_mid(nn.Module):\n",
    "    def __init__(self,num_classes=100, qfc_l = 100,qfc_h = 100, afc_l = 1, afc_h = 1):\n",
    "        super(Texas_layer_out_scale2_mid, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "        self.fc1 = PruneLinear(6169,1024)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "#         self.fc2 = PruneLinear(2048,1024)\n",
    "#         self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = PruneLinear(1024,512)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.fc4 = PruneLinear(512,256)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = PruneLinear(256,128)\n",
    "        self.tanh5 = nn.Tanh()\n",
    "#         )\n",
    "        self.classifier = nn.Linear(128,num_classes)\n",
    "    \n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.fc1(x)\n",
    "        y1 = self.tanh1(y)\n",
    "#         y1 = scale_by_percentage(y1, q=self.q, alpha = self.alpha)\n",
    "#         y = self.fc2(y1)\n",
    "#         y2 = self.tanh2(y)\n",
    "        y = self.fc3(y1)\n",
    "        y3 = self.tanh3(y)\n",
    "#         y3 = scale_by_percentage(y3, q=self.q, alpha = self.alpha)\n",
    "        y = self.fc4(y3)\n",
    "        y4 = self.tanh4(y)\n",
    "        y4 = scale_by_percentage_mid(y4, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        y = self.fc5(y4)\n",
    "        y5 = self.tanh5(y)\n",
    "        y5 = scale_by_percentage_mid(y5, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        output = self.classifier(y5)\n",
    "        \n",
    "        return output, y1, y3, y4, y5\n",
    "\n",
    "    \n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value = np.percentile(nonzero, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    \n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    p_value_low = np.percentile(nonzero, q_l)\n",
    "    p_value_high = np.percentile(nonzero, q_h)\n",
    "#     p_value_low = np.percentile(flattened_weights, q_l)\n",
    "#     p_value_high = np.percentile(flattened_weights, q_h)\n",
    "#     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# small model\n",
    "class Texas_1layer(nn.Module):\n",
    "    def __init__(self,num_classes=100):\n",
    "        super(Texas_1layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "            PruneLinear(6169,256),\n",
    "            nn.Tanh()\n",
    "#             PruneLinear(1024,512),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(512,256),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(512,128),\n",
    "#             nn.Tanh()\n",
    "        )\n",
    "#         self.classifier = nn.Linear(128,num_classes)\n",
    "        self.classifier = PruneLinear(256,num_classes)\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden_out = self.features(x)\n",
    "        \n",
    "        return self.classifier(hidden_out),hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# small model\n",
    "class Texas_2layer(nn.Module):\n",
    "    def __init__(self,num_classes=100):\n",
    "        super(Texas_2layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "            PruneLinear(6169,512),\n",
    "            nn.Tanh(),\n",
    "#             PruneLinear(1024,512),\n",
    "#             nn.Tanh(),\n",
    "#             PruneLinear(512,256),\n",
    "#             nn.Tanh(),\n",
    "            PruneLinear(512,128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "#         self.classifier = nn.Linear(128,num_classes)\n",
    "        self.classifier = PruneLinear(128,num_classes)\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden_out = self.features(x)\n",
    "        \n",
    "        return self.classifier(hidden_out),hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# small model\n",
    "class Texas(nn.Module):\n",
    "    def __init__(self,num_classes=100):\n",
    "        super(Texas, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "            PruneLinear(6169,512),\n",
    "            nn.Tanh(),\n",
    "            PruneLinear(1024,512),\n",
    "            nn.Tanh(),\n",
    "            PruneLinear(512,256),\n",
    "            nn.Tanh(),\n",
    "            PruneLinear(256,128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "#         self.classifier = nn.Linear(128,num_classes)\n",
    "        self.classifier = PruneLinear(128,num_classes)\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden_out = self.features(x)\n",
    "        \n",
    "        return self.classifier(hidden_out),hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# small model\n",
    "class Texas_drop(nn.Module):\n",
    "    def __init__(self,p=0.5,num_classes = 100):\n",
    "        self.num_classes = num_classes\n",
    "        self.p = p\n",
    "        super(Texas_drop, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "#             nn.Linear(6169,1024),\n",
    "            PruneLinear(6169,1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout2d(p),\n",
    "            PruneLinear(1024,512),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout2d(p),\n",
    "            PruneLinear(512,256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout2d(p),\n",
    "            PruneLinear(256,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout2d(p)\n",
    "        )\n",
    "#         self.classifier = nn.Linear(128,num_classes)\n",
    "        self.classifier = PruneLinear(128,num_classes)\n",
    "#         for key in self.state_dict():\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden_out = self.features(x)\n",
    "        \n",
    "        return self.classifier(hidden_out),hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class InferenceAttack_HZ(nn.Module):\n",
    "#     def __init__(self,num_classes):\n",
    "#         self.num_classes=num_classes\n",
    "#         super(InferenceAttack_HZ, self).__init__()\n",
    "#         self.features=nn.Sequential(\n",
    "#             nn.Linear(num_classes,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512,64),\n",
    "#             nn.ReLU(),\n",
    "#             )\n",
    "        \n",
    "  \n",
    "        \n",
    "#         self.labels=nn.Sequential(\n",
    "#            nn.Linear(num_classes,64),\n",
    "#             )\n",
    "#         self.combine=nn.Sequential(\n",
    "#             nn.Linear(64*2,64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64,1),\n",
    "#             )\n",
    "#         for key in self.state_dict():\n",
    "#             print (key)\n",
    "#             if key.split('.')[-1] == 'weight':    \n",
    "#                 nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "#                 print (key)\n",
    "                \n",
    "#             elif key.split('.')[-1] == 'bias':\n",
    "#                 self.state_dict()[key][...] = 0\n",
    "#         self.output= nn.Sigmoid()\n",
    "#     def forward(self,x1,x2,l):\n",
    "#         #print (l.size(),x.size())\n",
    "#         out_x1 = self.features(x1)\n",
    "        \n",
    "#         out_l = self.labels(l)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "#         is_member =self.combine( torch.cat((out_x1,out_l),1))\n",
    "        \n",
    "        \n",
    "#         return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defense model\n",
    "class Defense_Model(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super(Defense_Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(num_classes,256),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        \n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        is_member = self.features(x)\n",
    "        \n",
    "        return self.output(is_member), is_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(num_classes,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "  \n",
    "        \n",
    "        self.labels=nn.Sequential(\n",
    "           nn.Linear(num_classes,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.combine=nn.Sequential(\n",
    "            nn.Linear(64*2,512),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        for key in self.state_dict():\n",
    "            print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "                print (key)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x1,x2,l):\n",
    "        #print (l.size(),x.size())\n",
    "        out_x1 = self.features(x1)\n",
    "        \n",
    "        out_l = self.labels(l)\n",
    "\n",
    "            \n",
    "        is_member =self.combine( torch.cat((out_x1,out_l),1))\n",
    "        \n",
    "        return self.output(is_member), is_member\n",
    "#         return self.output(is_member)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# r = np.arange(len_train)\n",
    "# np.random.shuffle(r)\n",
    "# pickle.dump(r,open('./random_r_texas100','w'))\n",
    "# init_r=pickle.load(open('./random_r_texas100_prune')) #for py2\n",
    "init_r=pickle.load(open('./random_r_texas100_prune', 'rb'), encoding='latin1')# for py3\n",
    "X=X[init_r]\n",
    "Y=Y[init_r]\n",
    "\n",
    "float(X.shape[0])\n",
    "len_train =len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_classifier_ratio, train_attack_ratio = float(10000)/float(X.shape[0]),0.3\n",
    "train_classifier_data = X[:int(train_classifier_ratio*len_train)]\n",
    "train_attack_data = X[int(train_classifier_ratio*len_train):int((train_classifier_ratio+train_attack_ratio)*len_train)]\n",
    "test_data = X[int((train_classifier_ratio+train_attack_ratio)*len_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_classifier_data.shape\n",
    "# train_attack_data.shape\n",
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# train_classifier_ratio, train_attack_ratio = float(10000)/float(X.shape[0]),0.3\n",
    "# train_classifier_ratio = 0.4\n",
    "# train_attack_ratio = 0.3\n",
    "train_classifier_data_length  = 10000\n",
    "\n",
    "train_classifier_data = X[:train_classifier_data_length]\n",
    "train_attack_data = X[10000:20000]\n",
    "test_data = X[20000:]\n",
    "# attacker_train_member = X[30000:35000]\n",
    "# attacker_train_nonmember = X[35000:40000]\n",
    "# original_train_data = X[:int((train_classifier_ratio+train_attack_ratio)*len_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_classifier_data:  10000\n",
      "train_attack_data:  10000\n",
      "test_data:  47330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train_classifier_ratio, train_attack_ratio = float(10000)/float(X.shape[0]),0.3\n",
    "# print('train_classifier_ratio: ', train_classifier_ratio)\n",
    "# print('train_attack_ratio: ', train_attack_ratio)\n",
    "# train_classifier_ratio*len_train\n",
    "print('train_classifier_data: ', len(train_classifier_data))\n",
    "print('train_attack_data: ', len(train_attack_data))\n",
    "print('test_data: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_classifier_label = Y[:train_classifier_data_length]\n",
    "train_attack_label = Y[10000:20000]\n",
    "test_label = Y[20000:]\n",
    "# attacker_train_member_label = Y[30000:35000]\n",
    "# attacker_train_nonmember_label = Y[35000:40000]\n",
    "# original_train_label = Y[:int((train_classifier_ratio+train_attack_ratio)*len_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(init_r[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(train_data,labels, model, criterion, optimizer, epoch, use_cuda,num_batchs=999999):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  (len(train_data)//batch_size)-1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        if ind > num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_ = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%10==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(test_data,labels, model, criterion, epoch, use_cuda):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    len_t =  (len(test_data)//batch_size)\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = test_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_ = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 100==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=ind + 1,\n",
    "                        size=len(test_data),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_privatly(train_data,labels, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=10000,skip_batch=0,alpha=0.5):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    inference_model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    \n",
    "    len_t =  (len(train_data)//batch_size)-1\n",
    "    \n",
    "    for ind in range(skip_batch,len_t):\n",
    "        \n",
    "        if ind >= skip_batch+num_batchs:\n",
    "            break\n",
    "        \n",
    "        # measure data loading time\n",
    "        \n",
    "        #print (ind)\n",
    "        \n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        \n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((outputs.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, targets.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        inference_output = inference_model ( outputs,h_layer,infer_input_one_hot)\n",
    "        #print (inference_output.mean())\n",
    "        \n",
    "        relu = nn.ReLU()\n",
    "        loss = criterion(outputs, targets) + ((alpha)*(torch.mean((inference_output)) - 0.5))\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  (alpha, '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def outputres(test_data,labels, model, criterion, epoch, use_cuda):\n",
    "    \n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    len_t =  (len(test_data)//batch_size)-1\n",
    "    alloutputs = np.zeros((batch_size*(len_t),100))\n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = test_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_ = model(inputs)\n",
    "        \n",
    "        alloutputs[ind*batch_size:(ind+1)*batch_size,:]=F.softmax(outputs,dim=1).data.cpu().numpy()\n",
    "    return alloutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='./checkpoints_texas_10class_little_model_defense/', filename='checkpoint.pth.tar'):\n",
    "    if not os.path.isdir(checkpoint):\n",
    "        mkdir_p(checkpoint)\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda,num_batchs=100000,skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    #print (skip_batch, len_t)\n",
    "    \n",
    "    for ind in range(skip_batch, len_t):\n",
    "        \n",
    "        if ind >= skip_batch+num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "            \n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "        # compute output\n",
    "        outputs, h_layer = model(inputs)\n",
    "        outputs_non, h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        classifier_input = torch.cat((inputs,inputs_attack))\n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = comb_inputs #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "#         sf= nn.Softmax(dim=0)\n",
    "        \n",
    "#         att_inp=torch.stack([attack_input, infer_input_one_hot],1)\n",
    "        \n",
    "        \n",
    "#         att_inp = att_inp.view([attack_input.size()[0],1,2,attack_input.size(1)])\n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        \n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "#         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        #print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        #raise\n",
    "        \n",
    "        \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "        outputs_non,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = comb_inputs #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            \n",
    "#         break\n",
    "#     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "        \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_softmax(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda,num_batchs=100000,skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    #print (skip_batch, len_t)\n",
    "    \n",
    "    for ind in range(skip_batch, len_t):\n",
    "        \n",
    "        if ind >= skip_batch+num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "            \n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "        # compute output\n",
    "        outputs,_,_,_, h_layer = model(inputs)\n",
    "        outputs_non,_,_,_, h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        classifier_input = torch.cat((inputs,inputs_attack))\n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(comb_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "#         sf= nn.Softmax(dim=0)\n",
    "        \n",
    "#         att_inp=torch.stack([attack_input, infer_input_one_hot],1)\n",
    "        \n",
    "        \n",
    "#         att_inp = att_inp.view([attack_input.size()[0],1,2,attack_input.size(1)])\n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "#         attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "#         print(attack_input.shape, comb_inputs_h.shape, infer_input_one_hot.shape)\n",
    "        attack_output, _ = attack_model(attack_input,comb_inputs_h,infer_input_one_hot)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        \n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "#         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        #print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        #raise\n",
    "        \n",
    "        \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_softmax(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,h_layer = model(inputs)\n",
    "        outputs_non,_,_,_,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(comb_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input,comb_inputs_h,infer_input_one_hot)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            \n",
    "#         break\n",
    "#     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "        \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_softmax_modify(train_data,labels, train_logits,attack_data,attack_label,test_logits, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        train_attack_logits = train_logits[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        test_attack_logits = test_logits[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "            train_attack_logits, test_attack_logits = train_attack_logits.cuda(), test_attack_logits.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "        train_attack_logits , test_attack_logits = torch.autograd.Variable(train_attack_logits), torch.autograd.Variable(test_attack_logits)\n",
    "        \n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "        outputs_non,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        \n",
    "        comb_inputs = torch.cat((train_attack_logits,test_attack_logits))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(comb_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "#         attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input,comb_inputs_h,infer_input_one_hot)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((train_attack_logits.size()[0]+test_attack_logits.size()[0]))\n",
    "        att_labels [:train_attack_logits.size()[0]] =1.0\n",
    "        att_labels [train_attack_logits.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "    \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            \n",
    "#         break\n",
    "#     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "        \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attack_test_trainset(train_data,labels, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "#     len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))-1\n",
    "    len_t = len(train_data)//batch_size + 1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "#         inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "#         targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "#         #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#             inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "#         inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "#         outputs_non,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        comb_inputs_h = h_layer  #torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = outputs  #torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "            comb_targets= targets.view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            comb_targets= targets.view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = comb_inputs #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "#         target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, targets.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0])) #+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "#         att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "#         print('prec1: ', prec1)\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "#         break\n",
    "            \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attack_test_testset(attack_data,labels, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    end = time.time()\n",
    "#     len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))-1\n",
    "    len_t = len(attack_data)//batch_size + 1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "#         inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "#         targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "#         #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#             inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "#         inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "#         outputs_non,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        comb_inputs_h = h_layer  #torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = outputs  #torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "            comb_targets= targets.view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            comb_targets= targets.view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = comb_inputs #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "#         target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, targets.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0])) #+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =0.0\n",
    "#         att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() > 0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "#         print('prec1: ', prec1)\n",
    "        #raise\n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() > 0.5),(v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        sum_correct += correct\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "#         break\n",
    "    return (losses.avg, top1.avg, sum_correct) #, loss, attack_input)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_defense_softmax(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda,num_batchs=100000,skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    #print (skip_batch, len_t)\n",
    "    \n",
    "    for ind in range(skip_batch, len_t):\n",
    "        \n",
    "        if ind >= skip_batch+num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "            \n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "        # compute output\n",
    "        outputs, _,_,_, h_layer = model(inputs)\n",
    "        outputs_non, _,_,_, h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        classifier_input = torch.cat((inputs,inputs_attack))\n",
    "#         comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "#         if use_cuda:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "#         else:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "#         print (comb_inputs.size(),comb_targets.size())\n",
    "#         print('comb_inputs.shape:',comb_inputs.shape)\n",
    "#         print('comb_inputs:', comb_inputs[0,:])\n",
    "        sort_inputs, indices= torch.sort(comb_inputs)\n",
    "#         print('sort_inputs.shape:',sort_inputs.shape)\n",
    "#         print('sort_inputs:', sort_inputs[0,:])\n",
    "#         print(torch.sort(comb_inputs[0,:]))\n",
    "#         break\n",
    "        attack_input = softmax(sort_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "#         print('attack_input:', attack_input[0,:])\n",
    "#         print('attack_input.shape:',attack_input.shape)\n",
    "#         break\n",
    "#         one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "#         target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "#         infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "#         sf= nn.Softmax(dim=0)\n",
    "        \n",
    "#         att_inp=torch.stack([attack_input, infer_input_one_hot],1)\n",
    "        \n",
    "        \n",
    "#         att_inp = att_inp.view([attack_input.size()[0],1,2,attack_input.size(1)])\n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "#         attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "#         attack_output = attack_model(attack_input).view([-1])\n",
    "        attack_output,_ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        \n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "#         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        #print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        #raise\n",
    "        \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_defense_softmax(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "#     print(len_t)\n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs, _,_,_,h_layer = model(inputs)\n",
    "        outputs_non, _,_,_,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "#         comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "#         if use_cuda:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "#         else:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        sort_inputs, indices= torch.sort(comb_inputs)\n",
    "        attack_input = softmax(sort_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "#         one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "#         target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "#         infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "#         attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "#         attack_output = attack_model(attack_input).view([-1])\n",
    "        attack_output,_ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            \n",
    "#         break\n",
    "#     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "        \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_defense_softmax_unsort(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "#     print(len_t)\n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "        outputs_non,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "#         comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "#         if use_cuda:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "#         else:\n",
    "#             comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "#         ---------------sort-------------------------------\n",
    "#         sort_inputs, indices= torch.sort(comb_inputs)\n",
    "#         attack_input = softmax(sort_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "#         ---------------sort-------------------------------\n",
    "        attack_input = softmax(comb_inputs)\n",
    "        \n",
    "#         one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "#         target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "#         infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "         \n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "#         attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "#         attack_output = attack_model(attack_input).view([-1])\n",
    "        attack_output,_ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            \n",
    "#         break\n",
    "#     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "        \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classes = 100\n",
    "\n",
    "# Complement Entropy (CE)\n",
    "\n",
    "\n",
    "class ComplementEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ComplementEntropy, self).__init__()\n",
    "\n",
    "    # here we implemented step by step for corresponding to our formula\n",
    "    # described in the paper\n",
    "    def forward(self, yHat, y):\n",
    "        self.batch_size = len(y)\n",
    "        self.classes = classes\n",
    "        yHat = F.softmax(yHat, dim=1)\n",
    "        Yg = torch.gather(yHat, 1, torch.unsqueeze(y, 1))\n",
    "        Yg_ = (1 - Yg) + 1e-7  # avoiding numerical issues (first)\n",
    "        Px = yHat / Yg_.view(len(yHat), 1)\n",
    "        Px_log = torch.log(Px + 1e-10)  # avoiding numerical issues (second)\n",
    "        y_zerohot = torch.ones(self.batch_size, self.classes).scatter_(\n",
    "            1, y.view(self.batch_size, 1).data.cpu(), 0)\n",
    "        output = Px * Px_log * y_zerohot.cuda()\n",
    "        loss = torch.sum(output)\n",
    "        loss /= float(self.batch_size)\n",
    "        loss /= float(self.classes)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classes = 100\n",
    "\n",
    "class GuidedComplementEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        super(GuidedComplementEntropy, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # here we implemented step by step for corresponding to our formula\n",
    "    # described in the paper\n",
    "    def forward(self, yHat, y):\n",
    "        self.batch_size = len(y)\n",
    "        self.classes = classes\n",
    "        yHat = F.softmax(yHat, dim=1)\n",
    "        Yg = torch.gather(yHat, 1, torch.unsqueeze(y, 1))\n",
    "        Yg_ = (1 - Yg) + 1e-7  # avoiding numerical issues (first)\n",
    "        # avoiding numerical issues (second)\n",
    "        guided_factor = (Yg + 1e-7) ** self.alpha\n",
    "        Px = yHat / Yg_.view(len(yHat), 1)\n",
    "        Px_log = torch.log(Px + 1e-10)  # avoiding numerical issues (third)\n",
    "        y_zerohot = torch.ones(self.batch_size, self.classes).scatter_(\n",
    "            1, y.view(self.batch_size, 1).data.cpu(), 0)\n",
    "        output = Px * Px_log * y_zerohot.cuda()\n",
    "        guided_output = guided_factor.squeeze() * torch.sum(output, dim=1)\n",
    "        loss = torch.sum(guided_output)\n",
    "        loss /= float(self.batch_size)\n",
    "        loss /= math.log(float(self.classes))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_softmax_sort_mod(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda,num_batchs=100000,skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    #print (skip_batch, len_t)\n",
    "    \n",
    "    for ind in range(skip_batch, len_t):\n",
    "        \n",
    "        if ind >= skip_batch+num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "            \n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "        # compute output\n",
    "        outputs, h_layer = model(inputs)\n",
    "        outputs_non, h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        classifier_input = torch.cat((inputs,inputs_attack))\n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        \n",
    "        sort_inputs, indices= torch.sort(comb_inputs)\n",
    "        \n",
    "        attack_input = softmax(sort_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        for i in range(len(indices)):\n",
    "            target_one_hot_tr[i] = target_one_hot_tr[i][indices[i]]\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        \n",
    "#         sf= nn.Softmax(dim=0)\n",
    "        \n",
    "#         att_inp=torch.stack([attack_input, infer_input_one_hot],1)\n",
    "        \n",
    "        \n",
    "#         att_inp = att_inp.view([attack_input.size()[0],1,2,attack_input.size(1)])\n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "#         attack_output = attack_model(attack_input,comb_inputs_h,infer_input_one_hot).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input,comb_inputs_h,infer_input_one_hot)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        \n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "#         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        #print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        #raise\n",
    "        \n",
    "        \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_softmax_sort_mod(train_data,labels,attack_data,attack_label, model,attack_model, criterion,attack_criterion, optimizer,attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    sum_correct = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))+1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        inputs_attack = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets_attack = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        #print ( len(targets_attack), len(targets))\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs_attack , targets_attack = inputs_attack.cuda(), targets_attack.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack , targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h_layer = model(inputs)\n",
    "        outputs_non,h_layer_non = model(inputs_attack)\n",
    "        \n",
    "        comb_inputs_h = torch.cat((h_layer,h_layer_non))\n",
    "        comb_inputs = torch.cat((outputs,outputs_non))\n",
    "        \n",
    "        if use_cuda:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            comb_targets= torch.cat((targets,targets_attack)).view([-1,1]).type(torch.FloatTensor)\n",
    "            \n",
    "        #print (comb_inputs.size(),comb_targets.size())\n",
    "        sort_inputs, indices= torch.sort(comb_inputs)\n",
    "        \n",
    "        attack_input = softmax(sort_inputs) #torch.cat((comb_inputs,comb_targets),1)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((attack_input.size()[0],outputs.size(1))))).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, torch.cat((targets,targets_attack)).type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        \n",
    "        for i in range(len(indices)):\n",
    "            target_one_hot_tr[i] = target_one_hot_tr[i][indices[i]]\n",
    "        \n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        \n",
    "        #attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input,comb_inputs_h,infer_input_one_hot)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        #attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0]+inputs_attack.size()[0]))\n",
    "        att_labels [:inputs.size()[0]] =1.0\n",
    "        att_labels [inputs.size()[0]:] =0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "        \n",
    "        prec1=np.mean(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "        \n",
    "        correct = np.sum(np.equal((attack_output.data.cpu().numpy() >0.5),(v_is_member_labels.data.cpu().numpy()> 0.5)))\n",
    "        sum_correct += correct\n",
    "        #raise\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            \n",
    "#         break\n",
    "#     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "        \n",
    "    return (losses.avg, top1.avg, sum_correct)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_max_var(train_data,labels, model, criterion, optimizer, class_count, class_index, epoch, use_cuda,num_batchs=999999, alpha = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha)\n",
    "    end = time.time()\n",
    "    # len_t =  (len(train_classifier_label_tensor)//batch_size)-1\n",
    "    len_t = np.max(class_count)/10\n",
    "    batch_index = np.copy(class_index)\n",
    "    batch_data = torch.Tensor()\n",
    "    batch_label = torch.LongTensor()\n",
    "    for ind in range(len_t):\n",
    "        if ind > num_batchs:\n",
    "            break\n",
    "    #     # measure data loading time\n",
    "    #     inputs = train_classifier_data_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    #     targets = train_classifier_label_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "        inputs = torch.Tensor()\n",
    "        targets = torch.LongTensor()\n",
    "        for i in range(class_num):\n",
    "            if batch_index[i] + 10 > class_index[i+1]:\n",
    "            #             batch_data = sort_train_classifier_data[batch_index[i]: class_index[i+1]]\n",
    "                batch_index[i] = class_index[i]\n",
    "                inputs = torch.cat((inputs, train_classifier_data_tensor[batch_index[i]: batch_index[i] + 10]), 0)\n",
    "                targets = torch.cat((targets, train_classifier_label_tensor[batch_index[i]: batch_index[i] + 10]),0)\n",
    "                batch_index[i] = batch_index[i] + 10\n",
    "\n",
    "            else:\n",
    "                inputs = torch.cat((inputs, train_classifier_data_tensor[batch_index[i]: batch_index[i] + 10]), 0)\n",
    "                targets= torch.cat((targets, train_classifier_label_tensor[batch_index[i]: batch_index[i] + 10]),0)\n",
    "                batch_index[i] = batch_index[i] + 10\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_ = model(inputs)\n",
    "\n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "        var = []\n",
    "        for i in range(class_num):\n",
    "            mean_outputs = torch.mean(soft_outputs[i*10:(i+1)*10],0)\n",
    "            var.append(torch.mean(torch.sum((soft_outputs[i*10:(i+1)*10] - mean_outputs).pow(2),1)))\n",
    "        batch_var = torch.stack(var,0)\n",
    "        loss_var = alpha*(-torch.mean(batch_var))\n",
    "        loss = criterion(outputs, targets) + loss_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(loss_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%20==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s| Loss: {loss:.4f} | Var: {loss_var:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff(train_data,labels, model, criterion, optimizer,alpha, epoch, use_cuda,num_batchs=999999):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    print('alpha = ', alpha)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    diffes = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  (len(train_data)//batch_size)-1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        if ind > num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, y1,y2,y3,y4,y5 = model(inputs)\n",
    "        \n",
    "        out_list = [y1,y2,y3,y4,y5, outputs]\n",
    "#         out_list[0].shape\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        loss = criterion(outputs, targets) + diff_var\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        diffes.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | | diff: {diff:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    diff=diffes.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg, diffes.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_2(train_data,labels, model, criterion, optimizer,alpha, epoch, use_cuda,num_batchs=999999):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    diffes = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  (len(train_data)//batch_size)-1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        if ind > num_batchs:\n",
    "            break\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, hidden_outputs = model(inputs)\n",
    "        \n",
    "        hidden_map = torch.ones(256).cuda()\n",
    "        hidden_map[128:] = -1\n",
    "        torch.sum(hidden_map)\n",
    "        hd_diff_map = hidden_outputs * hidden_map\n",
    "        # hd_diff_map.shape\n",
    "        hd_diff = torch.mean(torch.sum(hd_diff_map, 1))** 2 / 256\n",
    "\n",
    "        output_map = torch.ones(10).cuda()\n",
    "        output_map[5:] = -1\n",
    "        output_diff_map = outputs * output_map\n",
    "        # hd_diff_map.shape\n",
    "        output_diff = torch.mean(torch.sum(output_diff_map , 1)) ** 2 / 10\n",
    "#         output_diff \n",
    "\n",
    "        diff_var = alpha * (hd_diff + output_diff) \n",
    "        \n",
    "        loss = criterion(outputs, targets) + diff_var\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        diffes.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | | diff: {diff:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    diff=diffes.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg, diffes.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_min_var(train_data,labels, model, criterion, optimizer,alpha, epoch, use_cuda, var_n,mean_class,beta):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    losses_var = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    diffes = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    len_t =  (len(train_data)//batch_size)-1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        \n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, y1,y3,y4,y5 = model(inputs)\n",
    "        \n",
    "        out_list = [y1,y3,y4,y5, outputs]\n",
    "#         out_list[0].shape\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "        \n",
    "        loss = criterion(outputs, targets) + diff_var + var\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        diffes.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])     \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%10==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} |var: {loss_var:.4f}| diff: {diff:.4f}|Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    diff=diffes.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg, diffes.avg, losses_var.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def _log_value(probs, small_value=1e-30):\n",
    "    return -torch.log(torch.max(probs, torch.tensor(small_value).cuda()))\n",
    "\n",
    "def _m_entr_comp(probs, true_labels):\n",
    "    log_probs = _log_value(probs)\n",
    "    reverse_probs = 1-probs\n",
    "    log_reverse_probs = _log_value(reverse_probs)\n",
    "#     modified_probs = np.copy(probs)\n",
    "    modified_probs = probs.detach().clone()\n",
    "    modified_probs[range(true_labels.size(0)), true_labels] = reverse_probs[range(true_labels.size(0)), true_labels]\n",
    "    modified_log_probs = log_reverse_probs.detach().clone()\n",
    "    modified_log_probs[range(true_labels.size(0)), true_labels] = log_probs[range(true_labels.size(0)), true_labels]\n",
    "    return torch.sum(torch.mul(modified_probs, modified_log_probs),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_min_var_me(train_data,labels, model, criterion, optimizer,alpha, epoch, use_cuda, var_n,mean_class,beta, gamma):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    losses_var = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    diffes = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    me_vars = AverageMeter()\n",
    "    end = time.time()\n",
    "    len_t =  (len(train_data)//batch_size)-1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        # measure data loading time\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, y1,y3,y4,y5 = model(inputs)\n",
    "        \n",
    "        out_list = [y1,y3,y4,y5, outputs]\n",
    "#         out_list[0].shape\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         s_tr_m_entr = _m_entr_comp(soft_outputs.data.cpu().numpy(), targets.data.cpu().numpy())\n",
    "        s_tr_m_entr = _m_entr_comp(soft_outputs, targets)\n",
    "        me_var = -torch.var(s_tr_m_entr) * gamma\n",
    "#         me_var = torch.tensor(me_var).cuda()\n",
    "\n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "        \n",
    "        loss = criterion(outputs, targets) + diff_var + var +  me_var\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        diffes.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        me_vars.update(me_var.data, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])     \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind%100==0:\n",
    "            print  ('({batch}/{size}) | Batch: {bt:.3f}s | | Loss: {loss:.4f} |var: {loss_var:.4f}| diff: {diff:.4f}|me_var: {me_vars:.4f}|Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=len_t,\n",
    "#                     data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    diff=diffes.avg,\n",
    "                    me_vars = me_vars.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg, diffes.avg, losses_var.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_classifier_data:  10000\n",
      "train_attack_data:  10000\n",
      "test_data:  47330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('train_classifier_data: ',len(train_classifier_data))\n",
    "\n",
    "print('train_attack_data: ',len(train_attack_data))\n",
    "\n",
    "print('test_data: ',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "small_train_classifier_data_length = 10000\n",
    "small_train_classifier_data = X[:small_train_classifier_data_length]\n",
    "small_train_classifier_label = Y[:small_train_classifier_data_length]\n",
    "\n",
    "len(small_train_classifier_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([93, 21, 97, 47, 30, 94, 14, 65, 72, 98])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(100)\n",
    "for i in range(len(Y)):\n",
    "    a[Y[i]] += 1\n",
    "\n",
    "sort_a = np.sort(a)\n",
    "class_index = np.argsort(a)\n",
    "select_class = class_index[90:]\n",
    "select_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23076,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class10_index = []\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] in select_class:\n",
    "        class10_index.append(i)\n",
    "\n",
    "class10_index = np.asarray(class10_index)\n",
    "class10_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23076,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 21, 30, 47, 65, 72, 93, 94, 97, 98]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ori_class10_label = Y[class10_index]\n",
    "print(ori_class10_label.shape)\n",
    "class10_data = X[class10_index]\n",
    "sort_select_class = np.sort(select_class).tolist()\n",
    "sort_select_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class10_label = np.zeros(len(ori_class10_label), dtype=int)\n",
    "for i in range(len(class10_label)):\n",
    "    class10_label[i] = sort_select_class.index(ori_class10_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class10_train_classifier_data:  7000\n",
      "class10_train_attack_data:  7000\n",
      "class10_test_data:  9076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class10_train_classifier_data = class10_data[:7000]\n",
    "class10_train_attack_data = class10_data[7000:14000]\n",
    "class10_test_data = class10_data[14000:]\n",
    "\n",
    "class10_train_classifier_label = class10_label[:7000]\n",
    "class10_train_attack_label = class10_label[7000:14000]\n",
    "class10_test_label = class10_label[14000:]\n",
    "\n",
    "print('class10_train_classifier_data: ',len(class10_train_classifier_data))\n",
    "\n",
    "print('class10_train_attack_data: ',len(class10_train_attack_data))\n",
    "\n",
    "print('class10_test_data: ',len(class10_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# npdata = np.load('texas10_sort_data.npz')\n",
    "# sort_class10_data = npdata['x'][:,:]\n",
    "# sort_class10_label = npdata['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class100_trainset_index = []\n",
    "class100_balance_test_index = []\n",
    "class100_testset_index = []\n",
    "class100_count = np.zeros(100)\n",
    "for i in range(len(Y)):\n",
    "    if(class100_count[Y[i]]<100):\n",
    "        class100_trainset_index.append(i)\n",
    "        class100_count[Y[i]] += 1\n",
    "    elif(100<= class100_count[Y[i]]<200):\n",
    "        class100_balance_test_index.append(i)\n",
    "        class100_testset_index.append(i)\n",
    "        class100_count[Y[i]] += 1\n",
    "    elif(200<= class100_count[Y[i]]):\n",
    "        class100_testset_index.append(i)\n",
    "        class100_count[Y[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n",
      "(57330,)\n"
     ]
    }
   ],
   "source": [
    "class100_trainset_index = np.asarray(class100_trainset_index)\n",
    "class100_balance_test_index  = np.asarray(class100_balance_test_index)\n",
    "class100_testset_index = np.asarray(class100_testset_index)\n",
    "\n",
    "print(class100_trainset_index.shape)\n",
    "print(class100_balance_test_index.shape)\n",
    "print(class100_testset_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class100_trainset_data = X[class100_trainset_index]\n",
    "class100_balance_test_data = X[class100_balance_test_index]\n",
    "class100_testset_data = X[class100_testset_index]\n",
    "\n",
    "class100_trainset_label = Y[class100_trainset_index]\n",
    "class100_balance_test_label = Y[class100_balance_test_index]\n",
    "class100_testset_label = Y[class100_testset_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6169)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class100_trainset_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start train\n",
    "lr=0.001\n",
    "epochs=20\n",
    "# p = 0.7\n",
    "state={}\n",
    "state['lr']=lr\n",
    "# net = Texas()\n",
    "# net = Texas_drop(p=p)\n",
    "# net = Texas_2layer()\n",
    "# net = Texas_1layer(num_classes=10)\n",
    "net = Texas_layer_out(num_classes=100)\n",
    "net = torch.nn.DataParallel(net).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [800, 1500]:\n",
    "#     if epoch in [100, 90]:\n",
    "        state['lr'] *= 10 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "    if epoch in [810,400,80,150, 1500]:\n",
    "#     if epoch in [100, 90]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "batch_size=256\n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "epochs = 200\n",
    "num_class = 100\n",
    "mean_class = np.zeros((num_class,num_class))\n",
    "var_n = np.zeros(num_class)\n",
    "alpha = 500\n",
    "beta = 3000\n",
    "train_time_list = []\n",
    "print('alpha = ', alpha, ' beta = ', beta)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "#     if epoch == 30:\n",
    "#         alpha = 3000\n",
    "#         print('alpha = ', alpha, ' beta = ', beta)\n",
    "    r= np.arange(len(train_classifier_data))\n",
    "    np.random.shuffle(r)\n",
    "    s_train_classifier_data = train_classifier_data[r]\n",
    "    s_train_classifier_label = train_classifier_label[r]\n",
    "    \n",
    "    train_classifier_data_tensor = torch.from_numpy(s_train_classifier_data).type(torch.FloatTensor)\n",
    "    train_classifier_label_tensor = torch.from_numpy(s_train_classifier_label).type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "    test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "    test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "    \n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_time = time.time()\n",
    "    train_loss, train_acc, diff, var = train_half_diff_min_var(train_classifier_data_tensor,train_classifier_label_tensor, net, criterion, optimizer, alpha, epoch, use_cuda, var_n,mean_class,beta)\n",
    "    train_time_list.append(time.time()-train_time)\n",
    "    print ('train acc: %.4f, train loss: %.4f, train diff: %.4f, last var: %.4f'%(train_acc, train_loss, diff, var))\n",
    "    test_loss, test_acc = test(test_data_tensor,test_label_tensor, net, criterion, epoch, use_cuda)\n",
    "    #privacy_loss, privacy_acc = privacy_train(trainloader,testloader,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda)\n",
    "    \n",
    "    # append logger file\n",
    "    \n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test acc: %.4f, test loss: %.4f, best acc: %.4f'%(test_acc, test_loss, best_acc))\n",
    "    \n",
    "        # save model\n",
    "    if is_best or epoch+1 == epochs:\n",
    "        if best_acc > 50:\n",
    "            best_epoch = epoch+1;\n",
    "            save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, is_best, filename='Texas_5layer_balance_output_partition_alpha500_min_var_beta3000_epoch_%d'%(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# lr = 0.001\n",
    "batch_size=256\n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "epochs = 100\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    r= np.arange(len(train_classifier_data))\n",
    "    np.random.shuffle(r)\n",
    "    s_train_classifier_data = train_classifier_data[r]\n",
    "    s_train_classifier_label = train_classifier_label[r]\n",
    "    \n",
    "    train_classifier_data_tensor = torch.from_numpy(s_train_classifier_data).type(torch.FloatTensor)\n",
    "    train_classifier_label_tensor = torch.from_numpy(s_train_classifier_label).type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "    test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "    test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "    \n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "        \n",
    "    train_loss, train_acc = train(train_classifier_data_tensor,train_classifier_label_tensor, net, criterion, optimizer, epoch, use_cuda)\n",
    "    print ('train acc: %.4f, train loss: %.4f'%(train_acc, train_loss))\n",
    "    test_loss, test_acc = test(test_data_tensor,test_label_tensor, net, criterion, epoch, use_cuda)\n",
    "    #privacy_loss, privacy_acc = privacy_train(trainloader,testloader,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda)\n",
    "    \n",
    "    # append logger file\n",
    "    \n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test acc: %.4f, test loss: %.4f, best acc: %.4f'%(test_acc, test_loss, best_acc))\n",
    "    \n",
    "    # save model\n",
    "    if is_best or epoch+1 == epochs:\n",
    "        if best_acc > 40:\n",
    "            best_epoch = epoch+1;\n",
    "            save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, False, filename='Texas100_5layer_No_defenss_epoch_%d'%(epoch+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_classifier_data = X[:int(train_classifier_ratio*len_train)]\n",
    "# train_attack_data = X[int(train_classifier_ratio*len_train):int((train_classifier_ratio+train_attack_ratio)*len_train)]\n",
    "# test_data = X[int((train_classifier_ratio+train_attack_ratio)*len_train):]\n",
    "\n",
    "# train_classifier_label = Y[:int(train_classifier_ratio*len_train)]\n",
    "# train_attack_label = Y[int(train_classifier_ratio*len_train):int((train_classifier_ratio+train_attack_ratio)*len_train)]\n",
    "# test_label = Y[int((train_classifier_ratio+train_attack_ratio)*len_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "len(train_attack_data)\n",
    "\n",
    "r = np.arange(len(train_attack_data))\n",
    "r1 = np.arange(len(train_classifier_data))\n",
    "np.random.shuffle(r)\n",
    "np.random.shuffle(r1)\n",
    "r2 = r1[r]\n",
    "\n",
    "print(len(r))\n",
    "print(len(r1))\n",
    "print(len(r2))\n",
    "\n",
    "print(max(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47330\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "\n",
    "# test_data_c = X[int((train_classifier_ratio+train_attack_ratio)*len_train):]\n",
    "# test_label_c = Y[int((train_classifier_ratio+train_attack_ratio)*len_train):]\n",
    "# test_data_c = X[40000:60000]\n",
    "# test_label_c = Y[40000:60000]\n",
    "# test_data_c = class10_data[14000:]\n",
    "# test_label_c = class10_label[14000:]\n",
    "test_data_c = test_data\n",
    "test_label_c = test_label\n",
    "epoch=0\n",
    "batch_size=128\n",
    "# net = Texas()\n",
    "# net = Texas_drop(p)\n",
    "# net = Texas_2layer()\n",
    "# net = Texas_1layer(10)\n",
    "net = Texas_layer_out(100)\n",
    "# net = Texas_layer_out_scale(100, q = 50, alpha = 20)\n",
    "# net = Texas_layer_out_scale2(100, q = 50, alpha = 200)\n",
    "\n",
    "\n",
    "net = torch.nn.DataParallel(net).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "print(len(test_data_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resumed from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "resume='./checkpoints_texas/Texas_5layer_balance_output_partition_alpha100_min_var_beta3000_epoch_50'\n",
    "\n",
    "\n",
    "checkpoint = os.path.dirname(resume)\n",
    "checkpoint = torch.load(resume)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "print('==> Resumed from checkpoint..')\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/47330) Data: 0.000s | Batch: 0.174s | Loss: 4.1361 | top1:  59.3750 | top5:  79.6875\n",
      "(101/47330) Data: 0.000s | Batch: 0.003s | Loss: 4.1397 | top1:  55.5925 | top5:  82.4876\n",
      "(201/47330) Data: 0.000s | Batch: 0.002s | Loss: 4.1393 | top1:  56.0090 | top5:  82.7736\n",
      "(301/47330) Data: 0.000s | Batch: 0.002s | Loss: 4.1394 | top1:  55.9567 | top5:  82.8748\n",
      "test_loss:  4.1397, test_acc:  55.8435\n",
      "(1/10000) Data: 0.000s | Batch: 0.002s | Loss: 4.0594 | top1:  70.3125 | top5:  93.7500\n",
      "trainset_loss:  4.0306, trainset_acc:  73.5877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(test_data_c).type(torch.FloatTensor) ,torch.from_numpy(test_label_c).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n",
    "# trainset_loss, trainset_acc = test(torch.from_numpy(small_train_classifier_data).type(torch.FloatTensor) ,torch.from_numpy(small_train_classifier_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "trainset_loss, trainset_acc = test(torch.from_numpy(train_classifier_data).type(torch.FloatTensor) ,torch.from_numpy(train_classifier_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('trainset_loss: {trainset_loss: .4f}, trainset_acc: {trainset_acc: .4f}'.format(trainset_loss=trainset_loss, trainset_acc=trainset_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(train_classifier_data.shape)\n",
    "print(test_data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_evaluate_attacker_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "y_evaluate_attacker_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "net.eval()\n",
    "\n",
    "f_evaluate_logits = []\n",
    "f_evaluate_prob = []\n",
    "hidden_layer_logits = []\n",
    "softmax = nn.Softmax()\n",
    "len_t =  (len(x_evaluate_attacker_tensor)//batch_size)+1\n",
    "print(len_t)\n",
    "for ind in range(len_t):\n",
    "    # measure data loading time\n",
    "    inputs = x_evaluate_attacker_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    targets = y_evaluate_attacker_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "    \n",
    "    # compute output\n",
    "    outputs,_,_,_,hidden_layer_outputs = net(inputs)\n",
    "    outputs_value = outputs.data.cpu().numpy()\n",
    "    hidden_layer_outputs_value = hidden_layer_outputs.data.cpu().numpy()\n",
    "#     loss = criterion(outputs, targets)\n",
    "    prob_outputs = softmax(outputs)\n",
    "    prob_outputs_value = prob_outputs.data.cpu().numpy()\n",
    "    for i in range(len(prob_outputs_value)):\n",
    "        f_evaluate_logits.append(outputs_value[i])\n",
    "        f_evaluate_prob.append(prob_outputs_value[i])\n",
    "        hidden_layer_logits.append(hidden_layer_outputs_value[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f_evaluate_logits_array = np.asarray(f_evaluate_logits)\n",
    "f_evaluate_prob_array = np.asarray(f_evaluate_prob)\n",
    "hidden_layer_logits_array = np.asarray(hidden_layer_logits)\n",
    "print(f_evaluate_logits_array.shape, f_evaluate_prob_array.shape)\n",
    "\n",
    "sort_f_evaluate=np.sort(f_evaluate_prob_array,axis=1)\n",
    "sort_f_evaluate_logits=np.sort(f_evaluate_logits_array,axis=1)\n",
    "sort_hidden_layer_logits = np.sort(hidden_layer_logits_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_evaluate_attacker_tensor = torch.from_numpy(test_data_c).type(torch.FloatTensor)\n",
    "y_evaluate_attacker_tensor = torch.from_numpy(test_label_c).type(torch.LongTensor)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "net.eval()\n",
    "\n",
    "test_prob = []\n",
    "test_logits = []\n",
    "test_hidden_layer_logits = []\n",
    "softmax = nn.Softmax()\n",
    "len_t =  (len(x_evaluate_attacker_tensor)//batch_size)+1\n",
    "print(len_t)\n",
    "for ind in range(len_t):\n",
    "    # measure data loading time\n",
    "    inputs = x_evaluate_attacker_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    targets = y_evaluate_attacker_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "    \n",
    "    # compute output\n",
    "    outputs,_,_,_,hidden_layer_outputs = net(inputs)\n",
    "    outputs_value = outputs.data.cpu().numpy()\n",
    "    hidden_layer_outputs_value = hidden_layer_outputs.data.cpu().numpy()\n",
    "#     loss = criterion(outputs, targets)\n",
    "    prob_outputs = softmax(outputs)\n",
    "    prob_outputs_value = prob_outputs.data.cpu().numpy()\n",
    "    for i in range(len(prob_outputs_value)):\n",
    "        test_logits.append(outputs_value[i])\n",
    "        test_prob.append(prob_outputs_value[i])\n",
    "        test_hidden_layer_logits.append(hidden_layer_outputs_value[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_prob_array = np.asarray(test_prob)\n",
    "test_logits_array = np.asarray(test_logits)\n",
    "test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(test_prob_array.shape, test_hidden_layer_logits_array.shape)\n",
    "\n",
    "sort_test_prob=np.sort(test_prob_array,axis=1)\n",
    "sort_test_logits=np.sort(test_logits_array,axis=1)\n",
    "sort_test_hidden_layer_logits = np.sort(test_hidden_layer_logits_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len(f_evaluate_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_sum = np.sum(test_logits_array,1)#.shape\n",
    "train_sum = np.sum(f_evaluate_logits_array,1) #.shape\n",
    "print(np.mean(test_sum), np.mean(train_sum))\n",
    "\n",
    "print(np.ptp(test_sum), np.ptp(train_sum))\n",
    "train_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_mean = np.mean(test_logits_array,0)#.shape\n",
    "train_mean = np.mean(f_evaluate_logits_array,0)#.shape\n",
    "train_mean.shape\n",
    "\n",
    "# fig = plt.figure(figsize=(30,20))\n",
    "x = np.arange(100)\n",
    "plt.plot(x,train_mean, label='train_mean')\n",
    "plt.plot(x,test_mean, label='test_mean')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_mean = np.mean(test_prob_array,0)#.shape\n",
    "train_mean = np.mean(f_evaluate_prob_array,0)#.shape\n",
    "train_mean.shape\n",
    "\n",
    "# fig = plt.figure(figsize=(30,20))\n",
    "x = np.arange(100)\n",
    "plt.plot(x,train_mean, label='train_mean')\n",
    "plt.plot(x,test_mean, label='test_mean')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_sum = np.sum(test_hidden_layer_logits_array,1)#.shape\n",
    "train_sum = np.sum(hidden_layer_logits_array,1) #.shape\n",
    "print(np.mean(test_sum), np.mean(train_sum))\n",
    "print(np.ptp(test_sum), np.ptp(train_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_mean = np.mean(test_hidden_layer_logits_array,0)#.shape\n",
    "train_mean = np.mean(hidden_layer_logits_array,0)#.shape\n",
    "train_mean \n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "x = np.arange(128)\n",
    "plt.plot(x,train_mean)\n",
    "plt.plot(x,test_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "x = np.arange(len(train_sum))\n",
    "plt.plot(x,train_sum)\n",
    "plt.plot(x,test_sum[:len(train_sum)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_hidden_layer_logits_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_sum\n",
    "np.sum(test_hidden_layer_logits_array[2])\n",
    "\n",
    "x = np.arange(128)\n",
    "plt.plot(x,test_hidden_layer_logits_array[2])\n",
    "plt.plot(x,np.sort(test_hidden_layer_logits_array[2]))\n",
    "\n",
    "np.sum(test_hidden_layer_logits_array[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt\n",
    "# pyplot.plot(predict_origin)\n",
    "# pyplot.plot(f_evaluate_origin[3])\n",
    "# pyplot.plot(f_evaluate_origin[4])\n",
    "# plt.plot(f_evaluate_origin[0])\n",
    "# plt.plot(f_evaluate_origin[1])\n",
    "# plt.plot(f_evaluate_origin[2])\n",
    "for i in range(100):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    plt.plot(f_evaluate_logits[i])\n",
    "#     plt.plot(result_array[i])\n",
    "#     plt.plot(sort_result_array[i])\n",
    "# for i in range(f_evaluate_origin.shape[0]):\n",
    "#     pyplot.plot(f_evaluate_origin[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(f_evaluate_prob)):\n",
    "    plt.plot(hidden_layer_logits[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(f_evaluate_prob)):\n",
    "    plt.plot(sort_hidden_layer_logits[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(sort_test_hidden_layer_logits)):\n",
    "    plt.plot(sort_test_hidden_layer_logits[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(len(f_evaluate_prob)):\n",
    "    plt.plot(sort_f_evaluate[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(10000):\n",
    "    plt.plot(sort_test_prob[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(f_evaluate_prob)):\n",
    "    plt.plot(sort_f_evaluate_logits[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(sort_test_hidden_layer_logits)):\n",
    "    plt.plot(sort_test_logits[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(len(f_evaluate_prob_array)):\n",
    "    plt.plot(f_evaluate_prob_array[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()\n",
    "\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(10000):\n",
    "    plt.plot(test_prob_array[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load membership inference attack\n",
    "\n",
    "\n",
    "r= np.arange(len(train_classifier_data))\n",
    "np.random.shuffle(r)\n",
    "\n",
    "train_classifier_data_tr_attack = train_classifier_data[r[:int(0.5*len(r))]]\n",
    "train_classifier_label_tr_attack = train_classifier_label[r[:int(0.5*len(r))]]\n",
    "\n",
    "train_classifier_data_te_attack = train_classifier_data[r[int(0.5*len(r)):]]\n",
    "train_classifier_label_te_attack = train_classifier_label[r[int(0.5*len(r)):]]\n",
    "\n",
    "len(train_classifier_data_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "labels.0.weight\n",
      "labels.0.weight\n",
      "labels.0.bias\n",
      "labels.2.weight\n",
      "labels.2.weight\n",
      "labels.2.bias\n",
      "combine.0.weight\n",
      "combine.0.weight\n",
      "combine.0.bias\n",
      "combine.2.weight\n",
      "combine.2.weight\n",
      "combine.2.bias\n",
      "combine.4.weight\n",
      "combine.4.weight\n",
      "combine.4.bias\n",
      "combine.6.weight\n",
      "combine.6.weight\n",
      "combine.6.bias\n",
      "combine.8.weight\n",
      "combine.8.weight\n",
      "combine.8.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "attack_model = InferenceAttack_HZ(100)\n",
    "attack_model = torch.nn.DataParallel(attack_model).cuda()\n",
    "attack_criterion = nn.MSELoss()\n",
    "attack_optimizer = optim.Adam(attack_model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_defense='./checkpoints_texas/Texas_softmax_NSH_attack_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "attack_model.load_state_dict(checkpoint_defense['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/40) Data: 0.000s | Batch: 0.009s | | Loss: 0.2525 | top1:  0.4961 \n",
      "test acc 0.511 test_loss:  tensor(0.2502, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "r= np.arange(len(train_classifier_data_te_attack))\n",
    "np.random.shuffle(r)\n",
    "train_classifier_data_te_attack = train_classifier_data_te_attack[r]\n",
    "train_classifier_label_te_attack = train_classifier_label_te_attack[r]\n",
    "test_data_attack = test_data[r]\n",
    "test_label_attack = test_label[r]\n",
    "train_classifier_data_te_attack_tensor = torch.from_numpy(train_classifier_data_te_attack).type(torch.FloatTensor)\n",
    "train_classifier_label_te_attack_tensor = torch.from_numpy(train_classifier_label_te_attack).type(torch.LongTensor)\n",
    "test_data_tensor = torch.from_numpy(test_data_attack).type(torch.FloatTensor)\n",
    "test_label_tensor = torch.from_numpy(test_label_attack).type(torch.LongTensor)\n",
    "test_loss, test_acc, sum_correct = test_attack_softmax(train_classifier_data_te_attack_tensor,train_classifier_label_te_attack_tensor\n",
    "                                     ,test_data_tensor,test_label_tensor,net,attack_model,criterion,attack_criterion,optimizer,attack_optimizer,epoch,use_cuda)\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_attack_data:  10000   test_data:  57330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# train_attack_data = train_attack_data\n",
    "\n",
    "# train_attack_label = train_attack_label\n",
    "\n",
    "# test_data = test_data\n",
    "# test_label = test_label\n",
    "\n",
    "print('train_attack_data: ', len(train_attack_data), '  test_data: ',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6169)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_classifier_data_te_attack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/5000) Data: 0.000s | Batch: 0.003s | Loss: 4.0594 | top1:  70.3125 | top5:  93.7500\n",
      "test_loss:  4.0301, test_acc:  73.6579\n",
      "(1/5000) Data: 0.000s | Batch: 0.002s | Loss: 4.0315 | top1:  78.9062 | top5:  94.5312\n",
      "test_loss:  4.0310, test_acc:  73.5377\n",
      "(1/10000) Data: 0.000s | Batch: 0.002s | Loss: 4.1383 | top1:  55.4688 | top5:  86.7188\n",
      "test_loss:  4.1413, test_acc:  56.0897\n",
      "(1/57330) Data: 0.000s | Batch: 0.003s | Loss: 4.1383 | top1:  55.4688 | top5:  86.7188\n",
      "(101/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.1420 | top1:  55.8632 | top5:  82.8357\n",
      "(201/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.1399 | top1:  55.9313 | top5:  82.8280\n",
      "(301/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.1396 | top1:  56.0216 | top5:  82.8852\n",
      "(401/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.1397 | top1:  55.9383 | top5:  82.9606\n",
      "test_loss:  4.1399, test_acc:  55.8952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(train_classifier_data_tr_attack).type(torch.FloatTensor) ,torch.from_numpy(train_classifier_label_tr_attack).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(train_classifier_data_te_attack).type(torch.FloatTensor) ,torch.from_numpy(train_classifier_label_te_attack).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(train_attack_data).type(torch.FloatTensor) ,torch.from_numpy(train_attack_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(test_data).type(torch.FloatTensor) ,torch.from_numpy(test_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_acc= 0.0\n",
    "batch_size=128\n",
    "epochs = 200\n",
    "for epoch in range(0, epochs):\n",
    "#     adjust_learning_rate_attack(attack_optimizer, epoch)\n",
    "    \n",
    "    r= np.arange(len(train_classifier_data_tr_attack))\n",
    "    np.random.shuffle(r)\n",
    "    r1= np.arange(len(train_attack_data))\n",
    "    np.random.shuffle(r1)\n",
    "    r2 = r1[r]\n",
    "\n",
    "    train_classifier_data_tr_attack = train_classifier_data_tr_attack[r]\n",
    "    train_classifier_label_tr_attack = train_classifier_label_tr_attack[r]\n",
    "\n",
    "    tr_attack_data = train_attack_data[r2]\n",
    "    tr_attack_label = train_attack_label[r2]\n",
    "\n",
    "\n",
    "    r= np.arange(len(train_classifier_data_te_attack))\n",
    "    np.random.shuffle(r)\n",
    "#     r1= np.arange(len(test_data)*0.5)\n",
    "#     np.random.shuffle(r1)\n",
    "\n",
    "\n",
    "    train_classifier_data_te_attack = train_classifier_data_te_attack[r]\n",
    "    train_classifier_label_te_attack = train_classifier_label_te_attack[r]\n",
    "\n",
    "    test_data_attack = test_data[r]\n",
    "    test_label_attack = test_label[r]\n",
    "    \n",
    "    train_classifier_data_tr_attack_tensor = torch.from_numpy(train_classifier_data_tr_attack).type(torch.FloatTensor)\n",
    "    train_classifier_label_tr_attack_tensor = torch.from_numpy(train_classifier_label_tr_attack).type(torch.LongTensor)\n",
    "\n",
    "    train_classifier_data_te_attack_tensor = torch.from_numpy(train_classifier_data_te_attack).type(torch.FloatTensor)\n",
    "    train_classifier_label_te_attack_tensor = torch.from_numpy(train_classifier_label_te_attack).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "#     r= np.arange(len(train_attack_data))\n",
    "#     np.random.shuffle(r)\n",
    "\n",
    "#     train_attack_data = train_attack_data[r]\n",
    "#     train_attack_label = train_attack_label[r]\n",
    "\n",
    "    train_attack_data_tensor = torch.from_numpy(tr_attack_data).type(torch.FloatTensor)\n",
    "    train_attack_label_tensor = torch.from_numpy(tr_attack_label).type(torch.LongTensor)\n",
    "\n",
    "    test_data_tensor = torch.from_numpy(test_data_attack).type(torch.FloatTensor)\n",
    "    test_label_tensor = torch.from_numpy(test_label_attack).type(torch.LongTensor)\n",
    "    print('\\nEpoch: [%d | %d] , lr : 0.0001'% (epoch + 1, epochs))\n",
    "\n",
    "\n",
    "    train_loss, train_acc = train_attack_softmax(train_classifier_data_tr_attack_tensor,train_classifier_label_tr_attack_tensor\n",
    "                                         ,train_attack_data_tensor,train_attack_label_tensor,net,attack_model,criterion,attack_criterion,optimizer,attack_optimizer,epoch,use_cuda)\n",
    "\n",
    "    print ('train acc',train_acc)\n",
    "    test_loss, test_acc, sum_correct = test_attack_softmax(train_classifier_data_te_attack_tensor,train_classifier_label_te_attack_tensor\n",
    "                                         ,test_data_tensor,test_label_tensor,net,attack_model,criterion,attack_criterion,optimizer,attack_optimizer,epoch,use_cuda)\n",
    "\n",
    "    is_best = test_acc>best_acc or (1-test_acc)>best_acc\n",
    "    \n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    best_acc = max((1-test_acc), best_acc)\n",
    "    if is_best or epoch+1 == epochs:\n",
    "        save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': attack_model.state_dict(),\n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : attack_optimizer.state_dict(),\n",
    "                }, False, filename='Texas_softmax_NSH_attack_best')\n",
    "    \n",
    "    print ('test acc',test_acc,best_acc)\n",
    "\n",
    "print('Best classification acc:%.4f'%(final_test_acc))\n",
    "print('Best attack acc:%.4f'%(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Defense_Model(\n",
       "    (features): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "    (output): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load NN attack model\n",
    "defense_model = Defense_Model(100)\n",
    "defense_model = torch.nn.DataParallel(defense_model).cuda()\n",
    "defense_criterion = nn.MSELoss()\n",
    "# defense_criterion = nn.CrossEntropyLoss()\n",
    "at_lr = 0.001\n",
    "state={}\n",
    "state['lr']=at_lr\n",
    "defense_optimizer = optim.Adam(defense_model.parameters(),lr=at_lr)\n",
    "defense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate_attack(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [40, 90]:\n",
    "#     if (epoch+1)%100 == 0:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_defense='./checkpoints_texas/Texas_softmax_sort_NN_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "defense_model.load_state_dict(checkpoint_defense['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/40) Data: 0.000s | Batch: 0.005s | | Loss: 0.2457 | top1:  0.5430 \n",
      "test acc 0.5505 test_loss:  tensor(0.2465, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "r= np.arange(len(train_classifier_data_te_attack))\n",
    "np.random.shuffle(r)\n",
    "#     r1= np.arange(len(test_data)*0.5)\n",
    "#     np.random.shuffle(r1)\n",
    "\n",
    "\n",
    "train_classifier_data_te_attack = train_classifier_data_te_attack[r]\n",
    "train_classifier_label_te_attack = train_classifier_label_te_attack[r]\n",
    "\n",
    "test_data_attack = test_data[r]\n",
    "test_label_attack = test_label[r]\n",
    "train_classifier_data_te_attack_tensor = torch.from_numpy(train_classifier_data_te_attack).type(torch.FloatTensor)\n",
    "train_classifier_label_te_attack_tensor = torch.from_numpy(train_classifier_label_te_attack).type(torch.LongTensor)\n",
    "\n",
    "test_data_tensor = torch.from_numpy(test_data_attack).type(torch.FloatTensor)\n",
    "test_label_tensor = torch.from_numpy(test_label_attack).type(torch.LongTensor)\n",
    "test_loss, test_acc, sum_correct = test_defense_softmax(train_classifier_data_te_attack_tensor,train_classifier_label_te_attack_tensor\n",
    "                                     ,test_data_tensor,test_label_tensor,net,defense_model,criterion,defense_criterion,optimizer,defense_optimizer,epoch,use_cuda)\n",
    "\n",
    "\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "\n",
    "best_acc= 0.0\n",
    "batch_size=128\n",
    "epochs = 100\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_attack(defense_optimizer, epoch)\n",
    "    \n",
    "    r= np.arange(len(train_classifier_data_tr_attack))\n",
    "    np.random.shuffle(r)\n",
    "    r1= np.arange(len(train_attack_data))\n",
    "    np.random.shuffle(r1)\n",
    "    r2 = r1[r]\n",
    "\n",
    "    train_classifier_data_tr_attack = train_classifier_data_tr_attack[r]\n",
    "    train_classifier_label_tr_attack = train_classifier_label_tr_attack[r]\n",
    "\n",
    "    tr_attack_data = train_attack_data[r2]\n",
    "    tr_attack_label = train_attack_label[r2]\n",
    "\n",
    "\n",
    "    r= np.arange(len(train_classifier_data_te_attack))\n",
    "    np.random.shuffle(r)\n",
    "#     r1= np.arange(len(test_data)*0.5)\n",
    "#     np.random.shuffle(r1)\n",
    "\n",
    "\n",
    "    train_classifier_data_te_attack = train_classifier_data_te_attack[r]\n",
    "    train_classifier_label_te_attack = train_classifier_label_te_attack[r]\n",
    "\n",
    "    test_data_attack = test_data[r]\n",
    "    test_label_attack = test_label[r]\n",
    "\n",
    "    \n",
    "    train_classifier_data_tr_attack_tensor = torch.from_numpy(train_classifier_data_tr_attack).type(torch.FloatTensor)\n",
    "    train_classifier_label_tr_attack_tensor = torch.from_numpy(train_classifier_label_tr_attack).type(torch.LongTensor)\n",
    "\n",
    "    train_classifier_data_te_attack_tensor = torch.from_numpy(train_classifier_data_te_attack).type(torch.FloatTensor)\n",
    "    train_classifier_label_te_attack_tensor = torch.from_numpy(train_classifier_label_te_attack).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "#     r= np.arange(len(train_attack_data))\n",
    "#     np.random.shuffle(r)\n",
    "\n",
    "#     train_attack_data = train_attack_data[r]\n",
    "#     train_attack_label = train_attack_label[r]\n",
    "\n",
    "    train_attack_data_tensor = torch.from_numpy(tr_attack_data).type(torch.FloatTensor)\n",
    "    train_attack_label_tensor = torch.from_numpy(tr_attack_label).type(torch.LongTensor)\n",
    "\n",
    "    test_data_tensor = torch.from_numpy(test_data_attack).type(torch.FloatTensor)\n",
    "    test_label_tensor = torch.from_numpy(test_label_attack).type(torch.LongTensor)\n",
    "    print('\\nEpoch: [%d | %d] , lr : %f'% (epoch + 1, epochs, state['lr']))\n",
    "\n",
    "\n",
    "    train_loss, train_acc = train_defense_softmax(train_classifier_data_tr_attack_tensor,train_classifier_label_tr_attack_tensor\n",
    "                                         ,train_attack_data_tensor,train_attack_label_tensor,net,defense_model,criterion,defense_criterion,optimizer,defense_optimizer,epoch,use_cuda)\n",
    "\n",
    "    print ('train acc',train_acc)\n",
    "    test_loss, test_acc, sum_correct = test_defense_softmax(train_classifier_data_te_attack_tensor,train_classifier_label_te_attack_tensor\n",
    "                                         ,test_data_tensor,test_label_tensor,net,defense_model,criterion,defense_criterion,optimizer,defense_optimizer,epoch,use_cuda)\n",
    "\n",
    "    is_best = test_acc>best_acc or (1-test_acc)>best_acc\n",
    "    \n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    best_acc = max((1-test_acc), best_acc)\n",
    "    if is_best or epoch+1 == epochs:\n",
    "        save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': defense_model.state_dict(),                                       \n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : defense_optimizer.state_dict(),\n",
    "                }, False, filename='Texas_softmax_sort_NN_best')\n",
    "    \n",
    "    print ('test acc',test_acc,best_acc)\n",
    "\n",
    "print('Best classification acc:%.4f'%(final_test_acc))\n",
    "print('Best attack acc:%.4f'%(best_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resumed from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# load for metric base attack\n",
    "# Here we load a model that uses balanced data to train and evaluated\n",
    "\n",
    "net = Texas_layer_out(100)\n",
    "net = torch.nn.DataParallel(net).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "resume='./checkpoints_texas_100_balance/Texas_5layer_balance_output_partition_alpha200_min_var_beta2000_epoch_69'\n",
    "# resume='./checkpoints_texas_100_balance/Texas_5layer_balance_output_partition_alpha300_min_var_beta2000_epoch_69'\n",
    "checkpoint = os.path.dirname(resume)\n",
    "checkpoint = torch.load(resume)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "print('==> Resumed from checkpoint..')\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier_data = class100_trainset_data\n",
    "train_classifier_label = class100_trainset_label\n",
    "test_data = class100_testset_data\n",
    "test_label = class100_testset_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/57330) Data: 0.000s | Batch: 0.003s | Loss: 4.3234 | top1:  25.7812 | top5:  68.7500\n",
      "(101/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.2414 | top1:  44.6627 | top5:  77.4752\n",
      "(201/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.2339 | top1:  46.5019 | top5:  78.2844\n",
      "(301/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.2300 | top1:  47.2903 | top5:  78.8414\n",
      "(401/57330) Data: 0.000s | Batch: 0.001s | Loss: 4.2268 | top1:  47.7381 | top5:  79.0835\n",
      "test_loss:  4.2258, test_acc:  47.9114\n",
      "(1/10000) Data: 0.000s | Batch: 0.002s | Loss: 4.3234 | top1:  25.7812 | top5:  68.7500\n",
      "class100_balance_test_loss:  4.1938, class100_balance_test_acc:  53.7861\n",
      "(1/10000) Data: 0.000s | Batch: 0.002s | Loss: 4.1721 | top1:  56.2500 | top5:  89.8438\n",
      "trainset_loss:  4.1008, trainset_acc:  73.7580\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(test_data).type(torch.FloatTensor) ,torch.from_numpy(test_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(torch.from_numpy(class100_balance_test_data).type(torch.FloatTensor) ,torch.from_numpy(class100_balance_test_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('class100_balance_test_loss: {test_loss: .4f}, class100_balance_test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=final_test_acc))\n",
    "\n",
    "trainset_loss, trainset_acc = test(torch.from_numpy(train_classifier_data).type(torch.FloatTensor) ,torch.from_numpy(train_classifier_label).type(torch.LongTensor), net, criterion, epoch, use_cuda)\n",
    "\n",
    "print ('trainset_loss: {trainset_loss: .4f}, trainset_acc: {trainset_acc: .4f}'.format(trainset_loss=trainset_loss, trainset_acc=trainset_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def test_loader(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, y1, y3, y4, y5 = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def testloader_by_class(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    class_count = np.zeros(100)\n",
    "    class_correct = np.zeros(100)\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, y1, y3, y4, y5 = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        _, pred = outputs.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        correct = correct.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        for i in range(len(targets)):\n",
    "            class_count[targets[i]] += 1\n",
    "            class_correct[targets[i]] += correct[0,i]\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg, class_count, class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class black_box_benchmarks(object):\n",
    "    \n",
    "    def __init__(self, shadow_train_performance, shadow_test_performance, \n",
    "                 target_train_performance, target_test_performance, num_classes):\n",
    "        '''\n",
    "        each input contains both model predictions (shape: num_data*num_classes) and ground-truth labels. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.s_tr_outputs, self.s_tr_labels = shadow_train_performance\n",
    "        self.s_te_outputs, self.s_te_labels = shadow_test_performance\n",
    "        self.t_tr_outputs, self.t_tr_labels = target_train_performance\n",
    "        self.t_te_outputs, self.t_te_labels = target_test_performance\n",
    "        \n",
    "        self.s_tr_corr = (np.argmax(self.s_tr_outputs, axis=1)==self.s_tr_labels).astype(int)\n",
    "        self.s_te_corr = (np.argmax(self.s_te_outputs, axis=1)==self.s_te_labels).astype(int)\n",
    "        self.t_tr_corr = (np.argmax(self.t_tr_outputs, axis=1)==self.t_tr_labels).astype(int)\n",
    "        self.t_te_corr = (np.argmax(self.t_te_outputs, axis=1)==self.t_te_labels).astype(int)\n",
    "        \n",
    "        self.s_tr_conf = np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "        self.s_te_conf = np.array([self.s_te_outputs[i, self.s_te_labels[i]] for i in range(len(self.s_te_labels))])\n",
    "        self.t_tr_conf = np.array([self.t_tr_outputs[i, self.t_tr_labels[i]] for i in range(len(self.t_tr_labels))])\n",
    "        self.t_te_conf = np.array([self.t_te_outputs[i, self.t_te_labels[i]] for i in range(len(self.t_te_labels))])\n",
    "        \n",
    "        self.s_tr_entr = self._entr_comp(self.s_tr_outputs)\n",
    "        self.s_te_entr = self._entr_comp(self.s_te_outputs)\n",
    "        self.t_tr_entr = self._entr_comp(self.t_tr_outputs)\n",
    "        self.t_te_entr = self._entr_comp(self.t_te_outputs)\n",
    "        \n",
    "        self.s_tr_m_entr = self._m_entr_comp(self.s_tr_outputs, self.s_tr_labels)\n",
    "        self.s_te_m_entr = self._m_entr_comp(self.s_te_outputs, self.s_te_labels)\n",
    "        self.t_tr_m_entr = self._m_entr_comp(self.t_tr_outputs, self.t_tr_labels)\n",
    "        self.t_te_m_entr = self._m_entr_comp(self.t_te_outputs, self.t_te_labels)\n",
    "        \n",
    "    \n",
    "    def _log_value(self, probs, small_value=1e-30):\n",
    "        return -np.log(np.maximum(probs, small_value))\n",
    "    \n",
    "    def _entr_comp(self, probs):\n",
    "        return np.sum(np.multiply(probs, self._log_value(probs)),axis=1)\n",
    "    \n",
    "    def _m_entr_comp(self, probs, true_labels):\n",
    "        log_probs = self._log_value(probs)\n",
    "        reverse_probs = 1-probs\n",
    "        log_reverse_probs = self._log_value(reverse_probs)\n",
    "        modified_probs = np.copy(probs)\n",
    "        modified_probs[range(true_labels.size), true_labels] = reverse_probs[range(true_labels.size), true_labels]\n",
    "        modified_log_probs = np.copy(log_reverse_probs)\n",
    "        modified_log_probs[range(true_labels.size), true_labels] = log_probs[range(true_labels.size), true_labels]\n",
    "        return np.sum(np.multiply(modified_probs, modified_log_probs),axis=1)\n",
    "    \n",
    "    def _thre_setting(self, tr_values, te_values):\n",
    "        value_list = np.concatenate((tr_values, te_values))\n",
    "        thre, max_acc = 0, 0\n",
    "        for value in value_list:\n",
    "            tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "            te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "            acc = 0.5*(tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        return thre\n",
    "    \n",
    "    def _mem_inf_via_corr(self):\n",
    "        # perform membership inference attack based on whether the input is correctly classified or not\n",
    "        t_tr_acc = np.sum(self.t_tr_corr)/(len(self.t_tr_corr)+0.0)\n",
    "        t_te_acc = np.sum(self.t_te_corr)/(len(self.t_te_corr)+0.0)\n",
    "        mem_inf_acc = 0.5*(t_tr_acc + 1 - t_te_acc)\n",
    "        print('With train acc {acc2:.3f} and test acc {acc3:.3f}\\nFor membership inference attack via correctness, the attack acc is {acc1:.3f} '.format(acc1=mem_inf_acc, acc2=t_tr_acc, acc3=t_te_acc) )\n",
    "        return\n",
    "    \n",
    "    def _mem_inf_thre(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        # perform membership inference attack by thresholding feature values: the feature can be prediction confidence,\n",
    "        # (negative) prediction entropy, and (negative) modified entropy\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        for num in range(self.num_classes):\n",
    "            thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "            t_tr_mem += np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "            t_te_non_mem += np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "        mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "        print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    def _mem_inf_benchmarks(self, all_methods=True, benchmark_methods=[]):\n",
    "        if (all_methods) or ('correctness' in benchmark_methods):\n",
    "            self._mem_inf_via_corr()\n",
    "        if (all_methods) or ('confidence' in benchmark_methods):\n",
    "            self._mem_inf_thre('confidence', self.s_tr_conf, self.s_te_conf, self.t_tr_conf, self.t_te_conf)\n",
    "        if (all_methods) or ('entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('entropy', -self.s_tr_entr, -self.s_te_entr, -self.t_tr_entr, -self.t_te_entr)\n",
    "        if (all_methods) or ('modified entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr, -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_classifier_data:  10000\n",
      "test_data:  10000\n",
      "shadow_train_data:  5000\n",
      "shadow_test_data:  5000\n",
      "Data loading finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tensor_data_create(features, labels):\n",
    "    tensor_x = torch.stack([torch.FloatTensor(i) for i in features]) # transform to torch tensors\n",
    "    tensor_y = torch.stack([torch.LongTensor([i]) for i in labels])[:,0]\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# train_datax = train_classifier_data\n",
    "# test_datax = test_data\n",
    "\n",
    "# train_labelx = train_classifier_label\n",
    "# test_labelx = test_label\n",
    "\n",
    "train_datax = class100_trainset_data\n",
    "test_datax = class100_balance_test_data \n",
    "\n",
    "train_labelx = class100_trainset_label\n",
    "test_labelx = class100_balance_test_label\n",
    "\n",
    "print('train_classifier_data: ', len(train_labelx))\n",
    "print('test_data: ', len(test_labelx))\n",
    "\n",
    "np.random.seed(100)\n",
    "train_len = train_datax.shape[0]\n",
    "r = np.arange(train_len)\n",
    "np.random.shuffle(r)\n",
    "shadow_indices = r[:train_len//2]\n",
    "target_indices = r[train_len//2:]\n",
    "\n",
    "shadow_train_data, shadow_train_label = train_datax[shadow_indices], train_labelx[shadow_indices]\n",
    "target_train_data, target_train_label = train_datax[target_indices], train_labelx[target_indices]\n",
    "\n",
    "test_len = 1*train_len\n",
    "r = np.arange(test_len)\n",
    "np.random.shuffle(r)\n",
    "shadow_indices = r[:test_len//2]\n",
    "target_indices = r[test_len//2:]\n",
    "\n",
    "shadow_test_data, shadow_test_label = test_datax[shadow_indices], test_labelx[shadow_indices]\n",
    "target_test_data, target_test_label = test_datax[target_indices], test_labelx[target_indices]\n",
    "\n",
    "print('shadow_train_data: ', len(shadow_train_data))\n",
    "print('shadow_test_data: ', len(shadow_test_data))\n",
    "\n",
    "\n",
    "shadow_train = tensor_data_create(shadow_train_data, shadow_train_label)\n",
    "shadow_train_loader = torch.utils.data.DataLoader(shadow_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "shadow_test = tensor_data_create(shadow_test_data, shadow_test_label)\n",
    "shadow_test_loader = torch.utils.data.DataLoader(shadow_test, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "target_train = tensor_data_create(target_train_data, target_train_label)\n",
    "target_train_loader = torch.utils.data.DataLoader(target_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "target_test = tensor_data_create(target_test_data, target_test_label)\n",
    "target_test_loader = torch.utils.data.DataLoader(target_test, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "print('Data loading finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/40) Data: 0.079s | Batch: 0.100s | Loss: 4.1253 | top1:  72.6562 | top5:  95.3125\n",
      "(21/40) Data: 0.005s | Batch: 0.008s | Loss: 4.1005 | top1:  73.8095 | top5:  96.0938\n",
      "Classification accuracy: 73.54\n",
      "(1/40) Data: 0.075s | Batch: 0.079s | Loss: 4.2343 | top1:  44.5312 | top5:  75.7812\n",
      "(21/40) Data: 0.005s | Batch: 0.007s | Loss: 4.2014 | top1:  52.3438 | top5:  81.9568\n",
      "Classification accuracy: 53.28\n",
      "(1/40) Data: 0.075s | Batch: 0.078s | Loss: 4.0939 | top1:  72.6562 | top5:  93.7500\n",
      "(21/40) Data: 0.005s | Batch: 0.007s | Loss: 4.0995 | top1:  74.0327 | top5:  96.5402\n",
      "Classification accuracy: 74.02\n",
      "(1/40) Data: 0.075s | Batch: 0.078s | Loss: 4.2060 | top1:  52.3438 | top5:  81.2500\n",
      "(21/40) Data: 0.005s | Batch: 0.007s | Loss: 4.1887 | top1:  54.6503 | top5:  83.3333\n",
      "Classification accuracy: 54.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, final_test_acc = test_loader(shadow_train_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test_loader(shadow_test_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test_loader(target_train_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test_loader(target_test_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/40) Data: 0.076s | Batch: 0.082s | Loss: 4.1255 | top1:  70.3125 | top5:  98.4375\n",
      "(21/40) Data: 0.005s | Batch: 0.008s | Loss: 4.1017 | top1:  74.3304 | top5:  96.2426\n",
      "Classification accuracy: 73.54\n",
      "(1/40) Data: 0.076s | Batch: 0.081s | Loss: 4.1097 | top1:  75.0000 | top5:  93.7500\n",
      "(21/40) Data: 0.005s | Batch: 0.008s | Loss: 4.1005 | top1:  73.9955 | top5:  95.6101\n",
      "Classification accuracy: 74.02\n",
      "(1/40) Data: 0.077s | Batch: 0.081s | Loss: 4.2034 | top1:  54.6875 | top5:  86.7188\n",
      "(21/40) Data: 0.005s | Batch: 0.008s | Loss: 4.2045 | top1:  52.6042 | top5:  81.9196\n",
      "Classification accuracy: 53.28\n",
      "(1/40) Data: 0.077s | Batch: 0.082s | Loss: 4.1856 | top1:  57.0312 | top5:  83.5938\n",
      "(21/40) Data: 0.005s | Batch: 0.008s | Loss: 4.1911 | top1:  54.7619 | top5:  83.5565\n",
      "Classification accuracy: 54.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc, s_tr_class, s_tr_correct = testloader_by_class(shadow_train_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_tr_class, t_tr_correct = testloader_by_class(target_train_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, s_te_class, s_te_correct = testloader_by_class(shadow_test_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_te_class, t_te_correct = testloader_by_class(target_test_loader, net, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "\n",
    "\n",
    "def _model_predictions(model, dataloader):\n",
    "    return_outputs, return_labels = [], []\n",
    "\n",
    "    for (inputs, labels) in dataloader:\n",
    "        return_labels.append(labels.numpy())\n",
    "        outputs, y1, y3, y4, y5 = model.forward(inputs.cuda()) \n",
    "        return_outputs.append( softmax_by_row(outputs.data.cpu().numpy()) )\n",
    "    return_outputs = np.concatenate(return_outputs)\n",
    "    return_labels = np.concatenate(return_labels)\n",
    "    return (return_outputs, return_labels)\n",
    "\n",
    "shadow_train_performance = _model_predictions(net, shadow_train_loader)\n",
    "shadow_test_performance = _model_predictions(net, shadow_test_loader)\n",
    "\n",
    "target_train_performance = _model_predictions(net, target_train_loader)\n",
    "target_test_performance = _model_predictions(net, target_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform membership inference attacks!!!\n",
      "With train acc 0.740 and test acc 0.544\n",
      "For membership inference attack via correctness, the attack acc is 0.598 \n",
      "For membership inference attack via confidence, the attack acc is 0.626\n",
      "For membership inference attack via entropy, the attack acc is 0.544\n",
      "For membership inference attack via modified entropy, the attack acc is 0.626\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Perform membership inference attacks!!!')\n",
    "MIA = black_box_benchmarks(shadow_train_performance,shadow_test_performance,\n",
    "                     target_train_performance,target_test_performance,num_classes=100)\n",
    "res = MIA._mem_inf_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "risk_score = calculate_risk_score(MIA.s_tr_m_entr, MIA.s_te_m_entr, MIA.s_tr_labels, MIA.s_te_labels, MIA.t_tr_m_entr, MIA.t_tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(risk_score)):\n",
    "#     print(risk_score[i])\n",
    "    if risk_score[i] is None:\n",
    "        risk_score[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x = np.sort(risk_score)\n",
    "y = range(risk_score.size)\n",
    "plt.plot(x,y, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "np.sum(risk_score<=0.5)\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*(i+1)))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load class wise \n",
    "\n",
    "self = res\n",
    "np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "\n",
    "t_tr_mem, t_te_non_mem = 0, 0\n",
    "class_mem_res = []\n",
    "class_nonmem_res = []\n",
    "class_attack_acc = []\n",
    "class_mem_num = []\n",
    "class_nonmem_num = []\n",
    "conf_thre = []\n",
    "s_tr_values = self.s_tr_conf\n",
    "s_te_values = self.s_te_conf\n",
    "t_tr_values = self.t_tr_conf\n",
    "t_te_values = self.t_te_conf\n",
    "v_name = 'confidence'\n",
    "for num in range(self.num_classes):\n",
    "    thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "    conf_thre.append(thre)\n",
    "    class_mem = np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "    class_nonmem = np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "    class_mem_res.append(class_mem)\n",
    "    class_nonmem_res.append(class_nonmem)\n",
    "    t_tr_mem += class_mem\n",
    "    t_te_non_mem += class_nonmem\n",
    "    class_mem_num.append(np.sum(self.t_tr_labels==num))\n",
    "    class_nonmem_num.append(np.sum(self.t_te_labels==num))\n",
    "    class_attack_acc.append( 0.5*(class_mem/(np.sum(self.t_tr_labels==num)+0.0) + class_nonmem/(np.sum(self.t_te_labels==num)+0.0)) )\n",
    "    \n",
    "mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "\n",
    "class_mem_res = np.asarray(class_mem_res)\n",
    "class_nonmem_res = np.asarray(class_nonmem_res)\n",
    "class_attack_acc = np.asarray(class_attack_acc)\n",
    "class_mem_num = np.asarray(class_mem_num)\n",
    "class_nonmem_num = np.asarray(class_nonmem_num)\n",
    "conf_thre = np.asarray(conf_thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset_correct = s_tr_correct + t_tr_correct\n",
    "tr_correct_index = np.argsort(trainset_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_conf),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_conf),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_conf),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_conf),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t_tr_acc =  t_tr_correct/ t_tr_class\n",
    "# print(t_tr_acc )\n",
    "t_te_acc = t_te_correct/t_te_class\n",
    "# print(t_te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.plot(np.sort(conf_thre), label = 'conf_thre')\n",
    "\n",
    "# plt.plot(class_attack_acc, label = 'class_attack_acc')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "conf_thre[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t_acc_gap = t_tr_acc - t_te_acc\n",
    "t_gap_index = np.argsort(t_acc_gap)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "# plt.plot(train_acc[gap_index], label = 'train_acc')\n",
    "# plt.plot(test_acc[gap_index],  label = 'test_acc')\n",
    "# plt.plot(t_tr_acc[t_gap_index], label = 't_tr_acc')\n",
    "# plt.plot(t_te_acc[t_gap_index], label = 't_te_acc')\n",
    "plt.plot(class_attack_acc[t_gap_index], label = 'class_attack_acc')\n",
    "plt.plot(t_acc_gap[t_gap_index],  label = 't_acc_gap')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load class wise  m_entr_thre\n",
    "\n",
    "self = res\n",
    "# np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "\n",
    "t_tr_mem, t_te_non_mem = 0, 0\n",
    "class_mem_res = []\n",
    "class_nonmem_res = []\n",
    "class_attack_acc = []\n",
    "class_mem_num = []\n",
    "class_nonmem_num = []\n",
    "m_entr_thre = []\n",
    "s_tr_values = -self.s_tr_m_entr\n",
    "s_te_values = -self.s_te_m_entr\n",
    "t_tr_values = -self.t_tr_m_entr\n",
    "t_te_values = -self.t_te_m_entr\n",
    "v_name = 'modified entropy'\n",
    "for num in range(self.num_classes):\n",
    "    thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "#     thre = -2\n",
    "    m_entr_thre.append(thre)\n",
    "    class_mem = np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "    class_nonmem = np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "    class_mem_res.append(class_mem)\n",
    "    class_nonmem_res.append(class_nonmem)\n",
    "    t_tr_mem += class_mem\n",
    "    t_te_non_mem += class_nonmem\n",
    "    class_mem_num.append(np.sum(self.t_tr_labels==num))\n",
    "    class_nonmem_num.append(np.sum(self.t_te_labels==num))\n",
    "    class_attack_acc.append( 0.5*(class_mem/(np.sum(self.t_tr_labels==num)+0.0) + class_nonmem/(np.sum(self.t_te_labels==num)+0.0)) )\n",
    "    \n",
    "mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "\n",
    "class_mem_res = np.asarray(class_mem_res)\n",
    "class_nonmem_res = np.asarray(class_nonmem_res)\n",
    "class_attack_acc = np.asarray(class_attack_acc)\n",
    "class_mem_num = np.asarray(class_mem_num)\n",
    "class_nonmem_num = np.asarray(class_nonmem_num)\n",
    "m_entr_thre = np.asarray(m_entr_thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(-res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(-res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(-res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(-res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t_tr_acc =  t_tr_correct/ t_tr_class\n",
    "# print(t_tr_acc )\n",
    "t_te_acc = t_te_correct/t_te_class\n",
    "# print(t_te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m_entr_index = np.argsort(m_entr_thre)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.plot(m_entr_thre[m_entr_index], label = 'm_entr_thre')\n",
    "\n",
    "# plt.plot(class_attack_acc, label = 'class_attack_acc')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "m_entr_thre[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "t_acc_gap = t_tr_acc - t_te_acc\n",
    "t_gap_index = np.argsort(t_acc_gap)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "# plt.plot(train_acc[gap_index], label = 'train_acc')\n",
    "# plt.plot(test_acc[gap_index],  label = 'test_acc')\n",
    "# plt.plot(t_tr_acc[t_gap_index], label = 't_tr_acc')\n",
    "# plt.plot(t_te_acc[t_gap_index], label = 't_te_acc')\n",
    "plt.plot(class_attack_acc[t_gap_index], label = 'class_attack_acc')\n",
    "plt.plot(t_acc_gap[t_gap_index],  label = 't_acc_gap')\n",
    "# plt.plot(class_attack_acc[m_entr_index], label = 'class_attack_acc')\n",
    "# plt.plot(t_acc_gap[m_entr_index],  label = 't_acc_gap')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class_attack_acc_index = np.argsort(class_attack_acc)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "# plt.plot(train_acc[gap_index], label = 'train_acc')\n",
    "# plt.plot(test_acc[gap_index],  label = 'test_acc')\n",
    "# plt.plot(t_tr_acc[t_gap_index], label = 't_tr_acc')\n",
    "# plt.plot(t_te_acc[t_gap_index], label = 't_te_acc')\n",
    "plt.plot(class_attack_acc[class_attack_acc_index], label = 'class_attack_acc')\n",
    "plt.plot(t_acc_gap[class_attack_acc_index],  label = 't_acc_gap')\n",
    "# plt.plot(class_attack_acc[m_entr_index], label = 'class_attack_acc')\n",
    "# plt.plot(t_acc_gap[m_entr_index],  label = 't_acc_gap')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
