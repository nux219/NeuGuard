{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from hop_skip_jump_attack import hop_skip_jump_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# torch.cuda.set_device(2)\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "global best_acc\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_fc(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_fc, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "#         x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_fc(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_fc(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_conv(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_conv, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.q, alpha = self.alpha)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.q, alpha = self.alpha)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.q, alpha = self.alpha)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.q, alpha = self.alpha)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "#         h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "#         h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value = np.percentile(nonzero, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_conv(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_conv(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv = 100,qfc = 100, aconv = 1, afc = 1):\n",
    "        super(AlexNet_comb, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv = qconv\n",
    "        self.qfc = qfc\n",
    "        self.aconv = aconv\n",
    "        self.afc = afc\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.qconv, alpha = self.aconv)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.qconv, alpha = self.aconv)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.qconv, alpha = self.aconv)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.qconv, alpha = self.aconv)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.qconv, alpha = self.aconv)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.qfc, alpha = self.afc)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.qfc, alpha = self.afc)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "#     print('q: ', q, 'percentile_value: ', percentile_value)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_comb(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_comb_mid(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv_l = 100,qconv_h = 100,qfc_l = 100,qfc_h = 100, aconv_l = 1,aconv_h = 1, afc_l = 1, afc_h = 1):\n",
    "        super(AlexNet_comb_mid, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv_l = qconv_l\n",
    "        self.qconv_h = qconv_h\n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.aconv_l = aconv_l\n",
    "        self.aconv_h = aconv_h\n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage_mid(h1, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage_mid(h2, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage_mid(h3, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage_mid(h4, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage_mid(x, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage_mid(h6, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage_mid(h7, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "#     p_value_low = np.percentile(flattened_weights, q_l)\n",
    "#     p_value_high = np.percentile(flattened_weights, q_h)\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    p_value_low = np.percentile(nonzero, q_l)\n",
    "    p_value_high = np.percentile(nonzero, q_h)\n",
    "#     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "        \n",
    "def alexnet_comb_mid(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb_mid(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_mod(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_mod, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.lin4 = nn.Linear(256*1*1, 10)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "        out_h5 = self.lin4(torch.flatten(h5, 1))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7, out_h5\n",
    "\n",
    "def alexnet_mod(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_mod(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(num_classes,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.labels=nn.Sequential(\n",
    "#            nn.Linear(num_classes,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.ReLU(),\n",
    "#             )\n",
    "            nn.Linear(num_classes,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.combine=nn.Sequential(\n",
    "#             nn.Linear(64*2,256),\n",
    "            nn.Linear(64*2,512),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        for key in self.state_dict():\n",
    "            print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "                print (key)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x,l):\n",
    "        \n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(l)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        is_member =self.combine( torch.cat((out_x  ,out_l),1))\n",
    "        \n",
    "        \n",
    "        return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defense model\n",
    "class Defense_Model(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Defense_Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(num_classes, 256),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        is_member = self.features(x)\n",
    "        return self.output(is_member), is_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "        diff_top = torch.mean(sort_diff_top) * alpha\n",
    "    #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "    #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_top.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_one(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_fc_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "#         fh1 = torch.sum(torch.sum(h1,2),2)\n",
    "#         fh2 = torch.sum(torch.sum(h2,2),2)\n",
    "#         fh3 = torch.sum(torch.sum(h3,2),2)\n",
    "#         fh4 = torch.sum(torch.sum(h4,2),2)\n",
    "#         fh5 = torch.sum(torch.sum(h5,2),2)\n",
    "        \n",
    "        out_list = [h6, h7, outputs]\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "# #         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "# #         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "#         sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "# #         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "# #         var = torch.mean(sort_var)*beta\n",
    "\n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "# #         var = sort_var * beta\n",
    "\n",
    "# #         s_var = sort_var * beta\n",
    "# #         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "# #         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "# #         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "# #         low8_loss = -torch.mean(low8_var) * alpha\n",
    "#         sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "#         diff_top = torch.mean(sort_diff_top) * alpha\n",
    "#     #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "#     #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# h2n = torch.norm(h2, dim = (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs ,_,_,_,_,_,_,_,_ = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            if epoch == 9:\n",
    "                print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "                print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train_softmax(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs) #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        \n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test_softmax(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs)#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "#             if epoch == 9:\n",
    "#                 print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#                 print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_sort_softmax(trainloader, model, attack_model, criterion,\n",
    "                              attack_criterion, optimizer, attack_optimizer, epoch, use_cuda, num_batchs=100000,\n",
    "                              skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // att_batch_size), (len(train_data) // att_batch_size)) + 1\n",
    "\n",
    "    # print (skip_batch, len_t)\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_  = model(inputs_attack)\n",
    "\n",
    "        # classifier_input = torch.cat((inputs, inputs_attack))\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        #         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "\n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        # print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_sort_softmax(testloader, model, attack_model, criterion, attack_criterion,\n",
    "                             optimizer, attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    sum_correct = 0.0\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // batch_size), (len(train_data) // batch_size)) + 1\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in testloader:\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_ = model(inputs_attack)\n",
    "\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        # attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        correct = np.sum(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        sum_correct += correct\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "\n",
    "    #         break\n",
    "    #     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "\n",
    "    return (losses.avg, top1.avg, sum_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset='cifar10'\n",
    "checkpoint_path='./checkpoints_cifar10'\n",
    "train_batch=200\n",
    "test_batch=100\n",
    "lr=0.001\n",
    "epochs=60\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))\n",
    "        \n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    mkdir_p(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# global best_acc\n",
    "start_epoch = 0  # start_ from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing dataset %s' % dataset)\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "\n",
    "# prepare test data parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mean = [0]\n",
    "# std = [1]\n",
    "# transform_test = transforms.Compose(\n",
    "#     [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    dataloader = datasets.CIFAR10\n",
    "    num_classes = 10\n",
    "else:\n",
    "    dataloader = datasets.CIFAR100\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> creating model \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"==> creating model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AlexNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = model.cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 20.13M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_attack = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_admm = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Resume\n",
    "title = 'cifar-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainset_private = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "trainloader_private = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_privacy=200\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(5000):\n",
    "#     private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "# for i in range(15000,30000):\n",
    "#     private_trainset_intest.append(trainset[r[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "r = np.arange(50000)\n",
    "for i in range(5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,30000):\n",
    "    private_testset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.shuffle(r)\n",
    "\n",
    "for i in range(5000):\n",
    "    private_trainset_intest.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# start train\n",
    "dataset='cifar10'\n",
    "checkpoint_path='./checkpoints_cifar10'\n",
    "# train_batch=400\n",
    "# test_batch=200\n",
    "lr=0.01\n",
    "epochs=100\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)\n",
    "model = AlexNet(num_classes)\n",
    "# model = AlexNet_comb_mid(num_classes, qconv_l = 60,qconv_h = 90,qfc_l = 80,qfc_h = 90,\n",
    "#                          aconv_l = 2.2,aconv_h = 1.2, afc_l = 1, afc_h = 1.2)\n",
    "# model = AlexNet_b(num_classes)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (re1): ReLU(inplace=True)\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re2): ReLU(inplace=True)\n",
      "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re5): ReLU(inplace=True)\n",
      "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (d1): Dropout(p=0.5, inplace=False)\n",
      "  (lin1): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (re6): ReLU(inplace=True)\n",
      "  (d2): Dropout(p=0.5, inplace=False)\n",
      "  (lin2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (re7): ReLU(inplace=True)\n",
      "  (lin3): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [60]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "#     elif epoch == 60:\n",
    "#         state['lr'] = 0.005\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# code for training defenseive NeuGuard model\n",
    "\n",
    "\n",
    "#  with random clip\n",
    "is_best=False\n",
    "best_acc=0.0\n",
    "start_epoch=0\n",
    "alpha = 0\n",
    "beta =250\n",
    "num_class = 10\n",
    "epochs=100\n",
    "mean_class = np.zeros((num_class,num_class))\n",
    "mean_class_h5 = np.zeros((num_class,num_class))\n",
    "var_n = np.zeros(num_class)\n",
    "\n",
    "test_acc_res = []\n",
    "test_loss_res = []\n",
    "train_acc_res = []\n",
    "train_loss_res = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "#     adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "#     train_enum = enumerate(trainloader)\n",
    "#     train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "    train_loss,train_var,train_svar, train_acc = train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=num_class, alpha = alpha, beta = beta)\n",
    "\n",
    "    train_acc_res.append(train_acc.item())\n",
    "    train_loss_res.append(train_loss.item())\n",
    "    \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "    test_acc_res.append(test_acc.item())\n",
    "    test_loss_res.append(test_loss.item())\n",
    "    \n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}, best_acc: {best_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc, best_acc=best_acc))\n",
    "    \n",
    "#     # save model\n",
    "#     if test_acc > 70:\n",
    "#         if (is_best) or epoch+1 == epochs:\n",
    "#             save_checkpoint({\n",
    "#                     'epoch': epoch + 1,\n",
    "#                     'state_dict': model.state_dict(),\n",
    "#                     'acc': test_acc,\n",
    "#                     'best_acc': best_acc,\n",
    "#                     'optimizer' : optimizer.state_dict(),\n",
    "#                 }, is_best, checkpoint=checkpoint_path,filename='cifar10_min_var_beta250_epoch%d'%(epoch+1))\n",
    "\n",
    "    \n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n",
    "test_loss, test_acc = test(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc))\n",
    "\n",
    "test_loss, test_acc = test(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from matplotlib import pyplot\n",
    "# from matplotlib.pylab import plt\n",
    "\n",
    "# for i in range(len(train_acc_res)):\n",
    "#     plt.plot(train_acc_res[i])\n",
    "#     plt.plot(test_acc_res[i])\n",
    "    \n",
    "# # plt.ylim(0, 0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "# model = AlexNet(num_classes)\n",
    "# model = AlexNet_fc(num_classes, q = 70, alpha = 5)\n",
    "# model = AlexNet_conv(num_classes, q = 65, alpha = 1.5)\n",
    "# model = AlexNet_comb(num_classes, qconv = 100, qfc = 50, aconv = 1, afc = 20)\n",
    "model = AlexNet_comb_mid(num_classes, qconv_l = 100,qconv_h = 65,qfc_l = 100,qfc_h = 100,\n",
    "                         aconv_l = 1,aconv_h = 1.5, afc_l = 1, afc_h = 1)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training and testing the attack result from the checkpoint\n",
    "\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta200_conv60a2.2_90a1.2_fc90a1.2_defense_epoch69'\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta200_alpha2_conv90_defense_epoch95'\n",
    "resume='./checkpoints_cifar10/cifar10_min_var_beta200_conv90a1.5_nonz_defense_epoch68'\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta250_defense_epoch66'\n",
    "# resume='./checkpoints_cifar10/cifar10_base_epoch79'\n",
    "# resume='./checkpoints_cifar10/cifar10_alexnet_100_epoch14'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load(resume)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer']) \n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/298495222.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/100) Data: 0.097s | Batch: 0.185s | Loss: 2.0511 | top1:  83.0000 | top5:  99.0000\n",
      "(21/100) Data: 0.006s | Batch: 0.062s | Loss: 2.0633 | top1:  76.3333 | top5:  96.0952\n",
      "(41/100) Data: 0.004s | Batch: 0.059s | Loss: 2.0661 | top1:  74.8537 | top5:  96.2439\n",
      "(61/100) Data: 0.003s | Batch: 0.059s | Loss: 2.0652 | top1:  74.9672 | top5:  96.4098\n",
      "(81/100) Data: 0.003s | Batch: 0.058s | Loss: 2.0656 | top1:  74.5556 | top5:  96.5432\n",
      "Classification accuracy: 74.31\n",
      "(1/500) Data: 0.105s | Batch: 0.267s | Loss: 1.9931 | top1:  97.0000 | top5:  100.0000\n",
      "(21/500) Data: 0.006s | Batch: 0.066s | Loss: 2.0111 | top1:  92.9048 | top5:  99.5238\n",
      "(41/500) Data: 0.004s | Batch: 0.063s | Loss: 2.0100 | top1:  92.9756 | top5:  99.5366\n",
      "(61/500) Data: 0.003s | Batch: 0.060s | Loss: 2.0099 | top1:  92.7377 | top5:  99.5246\n",
      "(81/500) Data: 0.003s | Batch: 0.059s | Loss: 2.0096 | top1:  92.8148 | top5:  99.5556\n",
      "(101/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0091 | top1:  92.9010 | top5:  99.5743\n",
      "(121/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0091 | top1:  93.0578 | top5:  99.6033\n",
      "(141/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0091 | top1:  92.9716 | top5:  99.5816\n",
      "(161/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9938 | top5:  99.5590\n",
      "(181/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0092 | top1:  92.9779 | top5:  99.5414\n",
      "(201/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0091 | top1:  93.0249 | top5:  99.5522\n",
      "(221/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0087 | top1:  93.0679 | top5:  99.5566\n",
      "(241/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0089 | top1:  93.0249 | top5:  99.5560\n",
      "(261/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0090 | top1:  93.0153 | top5:  99.5556\n",
      "(281/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0090 | top1:  92.9573 | top5:  99.5445\n",
      "(301/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0092 | top1:  92.9036 | top5:  99.5515\n",
      "(321/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0091 | top1:  92.9439 | top5:  99.5608\n",
      "(341/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0093 | top1:  92.9150 | top5:  99.5543\n",
      "(361/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9224 | top5:  99.5679\n",
      "(381/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0094 | top1:  92.8950 | top5:  99.5617\n",
      "(401/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9152 | top5:  99.5636\n",
      "(421/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9026 | top5:  99.5653\n",
      "(441/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.8798 | top5:  99.5669\n",
      "(461/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.8655 | top5:  99.5683\n",
      "(481/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0092 | top1:  92.8836 | top5:  99.5613\n",
      "Trainset Classification accuracy: 92.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "train_loss, final_train_acc = test(trainloader, model, criterion, epoch, use_cuda)\n",
    "print ('Trainset Classification accuracy: %.2f'%(final_train_acc))\n",
    "\n",
    "# q:  75 percentile_value:  0.3447321504354477\n",
    "# q:  75 percentile_value:  0.18002179265022278\n",
    "# q:  75 percentile_value:  0.01930836820974946\n",
    "# q:  75 percentile_value:  0.01867781998589635\n",
    "# q:  75 percentile_value:  0.005449312971904874\n",
    "# q:  50 percentile_value:  0.0015738015063107014\n",
    "# q:  50 percentile_value:  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load for c&w label-only attack\n",
    "\n",
    "def get_max_accuracy(y_true, probs, thresholds=None):\n",
    "    \n",
    "    \"\"\"Return the max accuracy possible given the correct labels and guesses. Will try all thresholds unless passed.\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        y_true: True label of `in' or `out' (member or non-member, 1/0)\n",
    "        probs: The scalar to threshold\n",
    "        thresholds: In a blackbox setup with a shadow/source model, the threshold obtained by the source model can be passed\n",
    "          here for attackin the target model. This threshold will then be used.\n",
    "\n",
    "      Returns: max accuracy possible, accuracy at the threshold passed (if one was passed), the max precision possible,\n",
    "       and the precision at the threshold passed.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    for thresh in thresholds:\n",
    "        accuracy_scores.append(accuracy_score(y_true,\n",
    "                                              [1 if m > thresh else 0 for m in probs]))\n",
    "        precision_scores.append(precision_score(y_true, [1 if m > thresh else 0 for m in probs]))\n",
    "\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    precisions = np.array(precision_scores)\n",
    "    max_accuracy = accuracies.max()\n",
    "    max_precision = precisions.max()\n",
    "    max_accuracy_threshold = thresholds[accuracies.argmax()]\n",
    "    max_precision_threshold = thresholds[precisions.argmax()]\n",
    "    return max_accuracy, max_accuracy_threshold, max_precision, max_precision_threshold\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold(source_m, source_stats, target_m, target_stats):\n",
    "    \"\"\" Train a threshold attack model and get teh accuracy on source and target models.\n",
    "\n",
    "  Args:\n",
    "    source_m: membership labels for source dataset (1 for member, 0 for non-member)\n",
    "    source_stats: scalar values to threshold (attack features) for source dataset\n",
    "    target_m: membership labels for target dataset (1 for member, 0 for non-member)\n",
    "    target_stats: scalar values to threshold (attack features) for target dataset\n",
    "\n",
    "  Returns: best acc from source thresh, precision @ same threshold, threshold for best acc,\n",
    "    precision at the best threshold for precision. all tuned on source model.\n",
    "\n",
    "    \"\"\"\n",
    "    # find best threshold on source data\n",
    "    acc_source, t, prec_source, tprec = get_max_accuracy(source_m, source_stats)\n",
    "\n",
    "    # find best accuracy on test data (just to check how much we overfit)\n",
    "    acc_test, _, prec_test, _ = get_max_accuracy(target_m, target_stats)\n",
    "\n",
    "    # get the test accuracy at the threshold selected on the source data\n",
    "    acc_test_t, _, _, _ = get_max_accuracy(target_m, target_stats, thresholds=[t])\n",
    "    _, _, prec_test_t, _ = get_max_accuracy(target_m, target_stats, thresholds=[tprec])\n",
    "    print(\"acc src: {}, acc test (best thresh): {}, acc test (src thresh): {}, thresh: {}\".format(acc_source, acc_test,\n",
    "                                                                                                acc_test_t, t))\n",
    "    print(\n",
    "    \"prec src: {}, prec test (best thresh): {}, prec test (src thresh): {}, thresh: {}\".format(prec_source, prec_test,\n",
    "                                                                                               prec_test_t, tprec))\n",
    "\n",
    "    return acc_test_t, prec_test_t, t, tprec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#  max_iterations is set to be 100 to make the attack faster, the orginal results are done with max_iterations=1000\n",
    "\n",
    "INF = float(\"inf\")\n",
    "\n",
    "\n",
    "def carlini_wagner_l2(\n",
    "    model_fn,\n",
    "    x,\n",
    "    n_classes,\n",
    "    y=None,\n",
    "    targeted=False,\n",
    "    lr=5e-3,\n",
    "    confidence=0,\n",
    "    clip_min=0,\n",
    "    clip_max=1,\n",
    "    initial_const=1e-2,\n",
    "    binary_search_steps=5,\n",
    "    max_iterations=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    This attack was originally proposed by Carlini and Wagner. It is an\n",
    "    iterative attack that finds adversarial examples on many defenses that\n",
    "    are robust to other attacks.\n",
    "    Paper link: https://arxiv.org/abs/1608.04644\n",
    "\n",
    "    At a high level, this attack is an iterative attack using Adam and\n",
    "    a specially-chosen loss function to find adversarial examples with\n",
    "    lower distortion than other attacks. This comes at the cost of speed,\n",
    "    as this attack is often much slower than others.\n",
    "\n",
    "    :param model_fn: a callable that takes an input tensor and returns\n",
    "              the model logits. The logits should be a tensor of shape\n",
    "              (n_examples, n_classes).\n",
    "    :param x: input tensor of shape (n_examples, ...), where ... can\n",
    "              be any arbitrary dimension that is compatible with\n",
    "              model_fn.\n",
    "    :param n_classes: the number of classes.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true,\n",
    "              then provide the target label. Otherwise, only provide\n",
    "              this parameter if you'd like to use true labels when\n",
    "              crafting adversarial samples. Otherwise, model predictions\n",
    "              are used as labels to avoid the \"label leaking\" effect\n",
    "              (explained in this paper:\n",
    "              https://arxiv.org/abs/1611.01236). If provide y, it\n",
    "              should be a 1D tensor of shape (n_examples, ).\n",
    "              Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or\n",
    "              untargeted? Untargeted, the default, will try to make the\n",
    "              label incorrect. Targeted will instead try to move in the\n",
    "              direction of being more like y.\n",
    "    :param lr: (optional) float. The learning rate for the attack\n",
    "              algorithm. Default is 5e-3.\n",
    "    :param confidence: (optional) float. Confidence of adversarial\n",
    "              examples: higher produces examples with larger l2\n",
    "              distortion, but more strongly classified as adversarial.\n",
    "              Default is 0.\n",
    "    :param clip_min: (optional) float. Minimum float value for\n",
    "              adversarial example components. Default is 0.\n",
    "    :param clip_max: (optional) float. Maximum float value for\n",
    "              adversarial example components. Default is 1.\n",
    "    :param initial_const: The initial tradeoff-constant to use to tune the\n",
    "              relative importance of size of the perturbation and\n",
    "              confidence of classification. If binary_search_steps is\n",
    "              large, the initial constant is not important. A smaller\n",
    "              value of this constant gives lower distortion results.\n",
    "              Default is 1e-2.\n",
    "    :param binary_search_steps: (optional) int. The number of times we\n",
    "              perform binary search to find the optimal tradeoff-constant\n",
    "              between norm of the perturbation and confidence of the\n",
    "              classification. Default is 5.\n",
    "    :param max_iterations: (optional) int. The maximum number of\n",
    "              iterations. Setting this to a larger value will produce\n",
    "              lower distortion results. Using only a few iterations\n",
    "              requires a larger learning rate, and will produce larger\n",
    "              distortion results. Default is 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def compare(pred, label, is_logits=False):\n",
    "        \"\"\"\n",
    "        A helper function to compare prediction against a label.\n",
    "        Returns true if the attack is considered successful.\n",
    "\n",
    "        :param pred: can be either a 1D tensor of logits or a predicted\n",
    "                class (int).\n",
    "        :param label: int. A label to compare against.\n",
    "        :param is_logits: (optional) bool. If True, treat pred as an\n",
    "                array of logits. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert logits to predicted class if necessary\n",
    "        if is_logits:\n",
    "            pred_copy = pred.clone().detach()\n",
    "            pred_copy[label] += -confidence if targeted else confidence\n",
    "            pred = torch.argmax(pred_copy)\n",
    "\n",
    "        return pred == label if targeted else pred != label\n",
    "\n",
    "    if y is None:\n",
    "        # Using model predictions as ground truth to avoid label leaking\n",
    "        pred = model_fn(x)\n",
    "        y = torch.argmax(pred, 1)\n",
    "\n",
    "    # Initialize some values needed for binary search on const\n",
    "    lower_bound = [0.0] * len(x)\n",
    "    upper_bound = [1e10] * len(x)\n",
    "    const = x.new_ones(len(x), 1) * initial_const\n",
    "\n",
    "    o_bestl2 = [INF] * len(x)\n",
    "    o_bestscore = [-1.0] * len(x)\n",
    "    x = torch.clamp(x, clip_min, clip_max)\n",
    "    ox = x.clone().detach()  # save the original x\n",
    "    o_bestattack = x.clone().detach()\n",
    "\n",
    "    # Map images into the tanh-space\n",
    "    x = (x - clip_min) / (clip_max - clip_min)\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    x = x * 2 - 1\n",
    "    x = torch.arctanh(x * 0.999999)\n",
    "    # x = torch.atanh(x * 0.999999)\n",
    "\n",
    "    # Prepare some variables\n",
    "    modifier = torch.zeros_like(x, requires_grad=True)\n",
    "    y_onehot = torch.nn.functional.one_hot(y, n_classes).to(torch.float)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    f_fn = lambda real, other, targeted: torch.max(\n",
    "        ((other - real) if targeted else (real - other)) + confidence,\n",
    "        torch.tensor(0.0).to(real.device),\n",
    "    )\n",
    "    l2dist_fn = lambda x, y: torch.pow(x - y, 2).sum(list(range(len(x.size())))[1:])\n",
    "    optimizer = torch.optim.Adam([modifier], lr=lr)\n",
    "\n",
    "    # Outer loop performing binary search on const\n",
    "    for outer_step in range(binary_search_steps):\n",
    "        # Initialize some values needed for the inner loop\n",
    "        bestl2 = [INF] * len(x)\n",
    "        bestscore = [-1.0] * len(x)\n",
    "\n",
    "        # Inner loop performing attack iterations\n",
    "        for i in range(max_iterations):\n",
    "            # One attack step\n",
    "            new_x = (torch.tanh(modifier + x) + 1) / 2\n",
    "            new_x = new_x * (clip_max - clip_min) + clip_min\n",
    "            logits,_,_,_,_,_,_,_ = model_fn(new_x)\n",
    "#             print(logits.shape)\n",
    "#             print(logits[0])\n",
    "#             print(y_onehot.shape)\n",
    "#             print(y_onehot[0])\n",
    "            real = torch.sum(y_onehot * logits, 1)\n",
    "            other, _ = torch.max((1 - y_onehot) * logits - y_onehot * 1e4, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            f = f_fn(real, other, targeted)\n",
    "            l2 = l2dist_fn(new_x, ox)\n",
    "            loss = (const * f + l2).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update best results\n",
    "            for n, (l2_n, logits_n, new_x_n) in enumerate(zip(l2, logits, new_x)):\n",
    "                y_n = y[n]\n",
    "                succeeded = compare(logits_n, y_n, is_logits=True)\n",
    "                if l2_n < o_bestl2[n] and succeeded:\n",
    "                    pred_n = torch.argmax(logits_n)\n",
    "                    o_bestl2[n] = l2_n\n",
    "                    o_bestscore[n] = pred_n\n",
    "                    o_bestattack[n] = new_x_n\n",
    "                    # l2_n < o_bestl2[n] implies l2_n < bestl2[n] so we modify inner loop variables too\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = pred_n\n",
    "                elif l2_n < bestl2[n] and succeeded:\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = torch.argmax(logits_n)\n",
    "\n",
    "        # Binary search step\n",
    "        for n in range(len(x)):\n",
    "            y_n = y[n]\n",
    "\n",
    "            if compare(bestscore[n], y_n) and bestscore[n] != -1:\n",
    "                # Success, divide const by two\n",
    "                upper_bound[n] = min(upper_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "            else:\n",
    "                # Failure, either multiply by 10 if no solution found yet\n",
    "                # or do binary search with the known upper bound\n",
    "                lower_bound[n] = max(lower_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "                else:\n",
    "                    const[n] *= 10\n",
    "\n",
    "    return o_bestattack.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dists(model_fn, dataloader, attack=\"CW\", max_samples=100, input_dim=[None, 32, 32, 3], n_classes=10):\n",
    "    \"\"\"Calculate untargeted distance to decision boundary for Adv-x MI attack.\n",
    "      :param model: model to approximate distances on (attack).\n",
    "      :param ds: tf dataset should be either the training set or the test set.\n",
    "      :param attack: \"CW\" for carlini wagner or \"HSJ\" for hop skip jump\n",
    "      :param max_samples: maximum number of samples to take from the ds\n",
    "      :return: an array of the first samples from the ds, of len max_samples, with the untargeted distances. \n",
    "    \"\"\"\n",
    "#   # switch to TF1 style\n",
    "#   sess = K.get_session()\n",
    "#   x = tf.placeholder(dtype=tf.float32, shape=input_dim)\n",
    "#   y = tf.placeholder(dtype=tf.int32, shape=[None, n_classes])\n",
    "#   output = model_(x)\n",
    "#   model = CallableModelWrapper(lambda x: model_(x), \"logits\")\n",
    "    \n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "    if attack == \"CW\":\n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "                    x_adv_curr = carlini_wagner_l2(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "                                                   10, targeted=False, y=targets[i].reshape(1),clip_min=inputs[i].min(),\n",
    "                                                   clip_max = inputs[i].max())\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "            if num_samples > max_samples:\n",
    "                return dist_adv[:max_samples]\n",
    "            \n",
    "    elif attack == \"HSJ\":\n",
    "        \n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "#                     stime = time.time()\n",
    "#                     x_adv_curr = hop_skip_jump_attack(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "#                                                       verbose=False,clip_min=inputs[i].min(),clip_max = inputs[i].max())\n",
    "#                     print(i)\n",
    "                    x_adv_curr = hop_skip_jump_attack(model, inputs[i:i+1], \n",
    "                                                      verbose=False,clip_min=inputs[i:i+1].min(),clip_max = inputs[i:i+1].max())\n",
    "#                     print('generate one data takes: ', time.time()-stime)\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unknown attack {}\".format(attack))\n",
    "\n",
    "#   next_element = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return dist_adv[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load attack data\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "# np.random.seed(200)\n",
    "r = np.arange(50000)\n",
    "batch_privacy = 50\n",
    "\n",
    "np.random.shuffle(r)\n",
    "for i in range(1000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,26000): \n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.seed(300)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(1000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,6000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================attack==============================\n",
    "\n",
    "source_train_ds = private_trainloader_intrain \n",
    "target_train_ds = private_trainloader_intest \n",
    "source_test_ds = private_testloader_intrain \n",
    "target_test_ds = private_testloader_intest\n",
    "\n",
    "source_model = model\n",
    "target_model = model\n",
    "\n",
    "input_dim = [None, 32,32,3]\n",
    "max_samples = 1000\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/298495222.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/20) Data: 0.065s | Batch: 0.161s | Loss: 2.0252 | top1:  86.0000 | top5:  100.0000\n",
      "train_loss:  2.0152, train_acc:  91.7000\n",
      "(1/20) Data: 0.076s | Batch: 0.122s | Loss: 2.0045 | top1:  94.0000 | top5:  100.0000\n",
      "test_loss:  2.0077, test_acc:  92.4000\n",
      "(1/20) Data: 0.079s | Batch: 0.129s | Loss: 2.0436 | top1:  76.0000 | top5:  100.0000\n",
      "train_loss:  2.0640, train_acc:  74.0000\n",
      "(1/20) Data: 0.089s | Batch: 0.143s | Loss: 2.0654 | top1:  78.0000 | top5:  94.0000\n",
      "test_loss:  2.0703, test_acc:  74.2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, train_acc1 = test(source_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc1))\n",
    "\n",
    "test_loss, train_acc2 = test(target_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc2))\n",
    "\n",
    "test_loss, test_acc1 = test(source_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc1))\n",
    "\n",
    "test_loss, test_acc2 = test(target_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58.8500, device='cuda:0')\n",
      "tensor(59.1000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print((train_acc1 - test_acc1)/2 + 50)\n",
    "\n",
    "print((train_acc2 - test_acc2)/2 + 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/2863644652.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 50 examples\n",
      "processed 100 examples\n",
      "processed 50 examples\n",
      "processed 100 examples\n",
      "threshold on C&W:\n",
      "acc src: 0.56, acc test (best thresh): 0.56, acc test (src thresh): 0.56, thresh: 0.10890922437978046\n",
      "prec src: 1.0, prec test (best thresh): 1.0, prec test (src thresh): 1.0, thresh: 0.3162307242000317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTICE:  max_samples is set to be 50 to do a quick attack, launch attack on 1000 samples will take a long time\n",
    "# max_samples=1000 \n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 50\n",
    "n_classes = 10\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw1 = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw1 = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw1 = np.concatenate([dists_source_in_cw1, dists_source_out_cw1], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw1 = dists_source_in_cw1\n",
    "dists_target_out_cw1 = dists_source_out_cw1\n",
    "dists_target_cw1 = np.concatenate([dists_target_in_cw1, dists_target_out_cw1], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc11, prec11, _, _ = get_threshold(source_m, dists_source_cw1, target_m, dists_target_cw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed 50 examples\n",
    "# processed 100 examples\n",
    "# processed 50 examples\n",
    "# processed 100 examples\n",
    "# threshold on C&W:\n",
    "# acc src: 0.53, acc test (best thresh): 0.53, acc test (src thresh): 0.53, thresh: 0.20805074739664173\n",
    "# prec src: 0.7142857142857143, prec test (best thresh): 0.7142857142857143, prec test (src thresh): 0.7142857142857143, thresh: 0.20805074739664173\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "labels.0.weight\n",
      "labels.0.weight\n",
      "labels.0.bias\n",
      "labels.2.weight\n",
      "labels.2.weight\n",
      "labels.2.bias\n",
      "combine.0.weight\n",
      "combine.0.weight\n",
      "combine.0.bias\n",
      "combine.2.weight\n",
      "combine.2.weight\n",
      "combine.2.bias\n",
      "combine.4.weight\n",
      "combine.4.weight\n",
      "combine.4.bias\n",
      "combine.6.weight\n",
      "combine.6.weight\n",
      "combine.6.bias\n",
      "combine.8.weight\n",
      "combine.8.weight\n",
      "combine.8.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228316/2337453466.py:41: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(self.state_dict()[key], std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load membership inference attack\n",
    "\n",
    "inferenece_model = InferenceAttack_HZ(10).cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "optimizer_mem = optim.Adam(inferenece_model.parameters(), lr=at_lr )\n",
    "criterion_attack = nn.MSELoss()\n",
    "best_acc= 0.0\n",
    "batch_size=100\n",
    "epochs= 100\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_train)\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,25000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(25000,50000):\n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "# private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "# private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_tr_attack = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "train_te_attack = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "test_tr_attack = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "test_te_attack = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_nsh(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [30]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_defense='./checkpoints_cifar10/cifar10_NSH_attack_softmax_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "inferenece_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228316/3046540025.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attack_model_input = softmax(pred_outputs)#torch.cat((pred_outputs,infer_input_one_hot),1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy_test--(0/100) Data: 0.043s | Batch: 0.215s | | Loss: 0.2499 | top1:  0.4750 \n",
      "privacy_test--(10/100) Data: 0.006s | Batch: 0.124s | | Loss: 0.2501 | top1:  0.4964 \n",
      "privacy_test--(20/100) Data: 0.005s | Batch: 0.120s | | Loss: 0.2501 | top1:  0.4938 \n",
      "privacy_test--(30/100) Data: 0.004s | Batch: 0.118s | | Loss: 0.2501 | top1:  0.4977 \n",
      "privacy_test--(40/100) Data: 0.004s | Batch: 0.117s | | Loss: 0.2500 | top1:  0.5010 \n",
      "test acc 0.5017 test_loss:  tensor(0.2500, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# code for training attack model\n",
    "\n",
    "best_acc = 0\n",
    "epochs= 200\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_nsh(optimizer_mem, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_attack_enum = enumerate(zip(train_tr_attack,test_tr_attack))\n",
    "    \n",
    "    train_loss, train_acc = privacy_train_softmax(train_attack_enum,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda, 10000)\n",
    "    \n",
    "    test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "    \n",
    "    print ('train acc', train_acc)\n",
    "    test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "    \n",
    "    is_best = test_acc>best_acc or (1-test_acc)>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    best_acc = max(1-test_acc, best_acc)\n",
    "\n",
    "    \n",
    "    print ('test acc', test_acc, best_acc)\n",
    "\n",
    "    # save model\n",
    "    if is_best:\n",
    "        best_epoch = epoch+1;\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': inferenece_model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer_mem.state_dict(),\n",
    "            }, False, checkpoint=checkpoint_path,filename='cifar10_NSH_attack_softmax_best')\n",
    "        \n",
    "print('model train acc: ', final_train_acc, '  model test acc: ', final_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Defense_Model(\n",
      "    (features): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load NN attack model\n",
    "\n",
    "defense_model = Defense_Model(10)\n",
    "defense_model = torch.nn.DataParallel(defense_model).cuda()\n",
    "defense_criterion = nn.MSELoss()\n",
    "# defense_criterion = nn.CrossEntropyLoss()\n",
    "att_batch_size = 100\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "defense_optimizer = torch.optim.Adam(defense_model.parameters(), lr=at_lr)\n",
    "print(defense_model)\n",
    "best_acc = 0.0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_attack(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [40, 80]:\n",
    "        #     if (epoch+1)%100 == 0:\n",
    "        state['lr'] *= 0.1\n",
    "        #         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228316/1530275313.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/25) Data: 0.011s | Batch: 0.144s | | Loss: 0.2504 | top1:  0.5200 \n",
      "(11/25) Data: 0.004s | Batch: 0.122s | | Loss: 0.2452 | top1:  0.5459 \n",
      "(21/25) Data: 0.003s | Batch: 0.119s | | Loss: 0.2456 | top1:  0.5479 \n",
      "(31/25) Data: 0.003s | Batch: 0.118s | | Loss: 0.2448 | top1:  0.5548 \n",
      "(41/25) Data: 0.003s | Batch: 0.118s | | Loss: 0.2448 | top1:  0.5541 \n",
      "test acc 0.5507 test_loss:  tensor(0.2454, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resume_defense='./checkpoints_cifar10/cifar10_softmax_sort_NN_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "defense_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for training attack model\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_attack(defense_optimizer, epoch)\n",
    "\n",
    "    print('\\nEpoch: [%d | %d] , lr : %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_attack_enum = enumerate(zip(train_tr_attack, test_tr_attack))\n",
    "    train_loss, train_acc = train_attack_sort_softmax(train_attack_enum, model,\n",
    "                                                      defense_model, criterion, defense_criterion, optimizer,\n",
    "                                                      defense_optimizer, epoch, use_cuda)\n",
    "\n",
    "    print('train acc:', train_acc)\n",
    "    test_attack_enum = enumerate(zip(train_te_attack, test_te_attack))\n",
    "    test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "# save model\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    if is_best or epoch + 1 == epochs:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': defense_model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': defense_optimizer.state_dict(),\n",
    "        }, False, checkpoint=checkpoint_path, filename='cifar10_softmax_sort_NN_best')\n",
    "\n",
    "    print('test acc', test_acc, best_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load for metric base attack\n",
    "\n",
    "def test_by_class(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    class_count = np.zeros(10)\n",
    "    class_correct = np.zeros(10)\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        _, pred = outputs.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        correct = correct.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        for i in range(len(targets)):\n",
    "            class_count[targets[i]] += 1\n",
    "            class_correct[targets[i]] += correct[0,i]\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg, class_count, class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class black_box_benchmarks(object):\n",
    "    \n",
    "    def __init__(self, shadow_train_performance, shadow_test_performance, \n",
    "                 target_train_performance, target_test_performance, num_classes):\n",
    "        '''\n",
    "        each input contains both model predictions (shape: num_data*num_classes) and ground-truth labels. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.s_tr_outputs, self.s_tr_labels = shadow_train_performance\n",
    "        self.s_te_outputs, self.s_te_labels = shadow_test_performance\n",
    "        self.t_tr_outputs, self.t_tr_labels = target_train_performance\n",
    "        self.t_te_outputs, self.t_te_labels = target_test_performance\n",
    "        \n",
    "        self.s_tr_corr = (np.argmax(self.s_tr_outputs, axis=1)==self.s_tr_labels).astype(int)\n",
    "        self.s_te_corr = (np.argmax(self.s_te_outputs, axis=1)==self.s_te_labels).astype(int)\n",
    "        self.t_tr_corr = (np.argmax(self.t_tr_outputs, axis=1)==self.t_tr_labels).astype(int)\n",
    "        self.t_te_corr = (np.argmax(self.t_te_outputs, axis=1)==self.t_te_labels).astype(int)\n",
    "        \n",
    "        self.s_tr_conf = np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "        self.s_te_conf = np.array([self.s_te_outputs[i, self.s_te_labels[i]] for i in range(len(self.s_te_labels))])\n",
    "        self.t_tr_conf = np.array([self.t_tr_outputs[i, self.t_tr_labels[i]] for i in range(len(self.t_tr_labels))])\n",
    "        self.t_te_conf = np.array([self.t_te_outputs[i, self.t_te_labels[i]] for i in range(len(self.t_te_labels))])\n",
    "        \n",
    "        self.s_tr_entr = self._entr_comp(self.s_tr_outputs)\n",
    "        self.s_te_entr = self._entr_comp(self.s_te_outputs)\n",
    "        self.t_tr_entr = self._entr_comp(self.t_tr_outputs)\n",
    "        self.t_te_entr = self._entr_comp(self.t_te_outputs)\n",
    "        \n",
    "        self.s_tr_m_entr = self._m_entr_comp(self.s_tr_outputs, self.s_tr_labels)\n",
    "        self.s_te_m_entr = self._m_entr_comp(self.s_te_outputs, self.s_te_labels)\n",
    "        self.t_tr_m_entr = self._m_entr_comp(self.t_tr_outputs, self.t_tr_labels)\n",
    "        self.t_te_m_entr = self._m_entr_comp(self.t_te_outputs, self.t_te_labels)\n",
    "        \n",
    "    \n",
    "    def _log_value(self, probs, small_value=1e-30):\n",
    "        return -np.log(np.maximum(probs, small_value))\n",
    "    \n",
    "    def _entr_comp(self, probs):\n",
    "        return np.sum(np.multiply(probs, self._log_value(probs)),axis=1)\n",
    "    \n",
    "    def _m_entr_comp(self, probs, true_labels):\n",
    "        log_probs = self._log_value(probs)\n",
    "        reverse_probs = 1-probs\n",
    "        log_reverse_probs = self._log_value(reverse_probs)\n",
    "        modified_probs = np.copy(probs)\n",
    "        modified_probs[range(true_labels.size), true_labels] = reverse_probs[range(true_labels.size), true_labels]\n",
    "        modified_log_probs = np.copy(log_reverse_probs)\n",
    "        modified_log_probs[range(true_labels.size), true_labels] = log_probs[range(true_labels.size), true_labels]\n",
    "        return np.sum(np.multiply(modified_probs, modified_log_probs),axis=1)\n",
    "    \n",
    "    def _thre_setting(self, tr_values, te_values):\n",
    "        value_list = np.concatenate((tr_values, te_values))\n",
    "        thre, max_acc = 0, 0\n",
    "        for value in value_list:\n",
    "            tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "            te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "            acc = 0.5*(tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        return thre\n",
    "    \n",
    "    def _mem_inf_via_corr(self):\n",
    "        # perform membership inference attack based on whether the input is correctly classified or not\n",
    "        t_tr_acc = np.sum(self.t_tr_corr)/(len(self.t_tr_corr)+0.0)\n",
    "        t_te_acc = np.sum(self.t_te_corr)/(len(self.t_te_corr)+0.0)\n",
    "        mem_inf_acc = 0.5*(t_tr_acc + 1 - t_te_acc)\n",
    "        print('With train acc {acc2:.3f} and test acc {acc3:.3f}\\nFor membership inference attack via correctness, the attack acc is {acc1:.3f} '.format(acc1=mem_inf_acc, acc2=t_tr_acc, acc3=t_te_acc) )\n",
    "        return\n",
    "    \n",
    "    def _mem_inf_thre(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        # perform membership inference attack by thresholding feature values: the feature can be prediction confidence,\n",
    "        # (negative) prediction entropy, and (negative) modified entropy\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        for num in range(self.num_classes):\n",
    "            thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "            t_tr_mem += np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "            t_te_non_mem += np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "        mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "        print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    def _mem_inf_benchmarks(self, all_methods=True, benchmark_methods=[]):\n",
    "        if (all_methods) or ('correctness' in benchmark_methods):\n",
    "            self._mem_inf_via_corr()\n",
    "        if (all_methods) or ('confidence' in benchmark_methods):\n",
    "            self._mem_inf_thre('confidence', self.s_tr_conf, self.s_te_conf, self.t_tr_conf, self.t_te_conf)\n",
    "        if (all_methods) or ('entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('entropy', -self.s_tr_entr, -self.s_te_entr, -self.t_tr_entr, -self.t_te_entr)\n",
    "        if (all_methods) or ('modified entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr, -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load me evaluation\n",
    "\n",
    "# np.random.seed(100)\n",
    "r = np.arange(25000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_trainset_intest.append(trainset[r[i]+25000])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/629789647.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/25) Data: 0.070s | Batch: 0.236s | Loss: 2.0066 | top1:  93.5000 | top5:  99.5000\n",
      "(21/25) Data: 0.005s | Batch: 0.121s | Loss: 2.0059 | top1:  93.7381 | top5:  99.5714\n",
      "Classification accuracy: 93.44\n",
      "(1/25) Data: 0.097s | Batch: 0.245s | Loss: 2.0043 | top1:  95.5000 | top5:  100.0000\n",
      "(21/25) Data: 0.007s | Batch: 0.120s | Loss: 2.0085 | top1:  93.3095 | top5:  99.6667\n",
      "Classification accuracy: 93.30\n",
      "(1/25) Data: 0.083s | Batch: 0.233s | Loss: 2.0711 | top1:  74.5000 | top5:  96.5000\n",
      "(21/25) Data: 0.006s | Batch: 0.123s | Loss: 2.0683 | top1:  74.0000 | top5:  96.2143\n",
      "Classification accuracy: 73.98\n",
      "(1/25) Data: 0.085s | Batch: 0.228s | Loss: 2.0735 | top1:  71.5000 | top5:  97.0000\n",
      "(21/25) Data: 0.006s | Batch: 0.127s | Loss: 2.0636 | top1:  75.0952 | top5:  96.8810\n",
      "Classification accuracy: 74.84\n",
      "[1045.  994.  963.  994. 1007.  991. 1027.  994.  970. 1015.]\n",
      "[504. 492. 400. 440. 436. 418. 487. 493. 508. 494.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc, s_tr_class, s_tr_correct = test_by_class(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_tr_class, t_tr_correct = test_by_class(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, s_te_class, s_te_correct = test_by_class(private_testloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_te_class, t_te_correct = test_by_class(private_testloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "print( s_tr_class + t_tr_class)\n",
    "print(s_tr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97511962 0.99295775 0.85669782 0.87625755 0.89076465 0.8506559\n",
      " 0.96689387 0.97585513 0.97731959 0.96945813]\n",
      "[0.811 0.903 0.609 0.625 0.626 0.662 0.832 0.776 0.792 0.805]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_acc = (s_tr_correct + t_tr_correct)/(s_tr_class + t_tr_class)\n",
    "print(train_acc)\n",
    "test_acc = (s_te_correct + t_te_correct)/(s_te_class+t_te_class)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "\n",
    "\n",
    "def _model_predictions(model, dataloader):\n",
    "    return_outputs, return_labels = [], []\n",
    "\n",
    "    for (inputs, labels) in dataloader:\n",
    "        return_labels.append(labels.numpy())\n",
    "        outputs,_,_,_,_,_,_,_ = model.forward(inputs.cuda()) \n",
    "        return_outputs.append( softmax_by_row(outputs.data.cpu().numpy()) )\n",
    "    return_outputs = np.concatenate(return_outputs)\n",
    "    return_labels = np.concatenate(return_labels)\n",
    "    return (return_outputs, return_labels)\n",
    "\n",
    "shadow_train_performance = _model_predictions(model, private_trainloader_intrain)\n",
    "shadow_test_performance = _model_predictions(model, private_testloader_intrain)\n",
    "\n",
    "target_train_performance = _model_predictions(model, private_trainloader_intest)\n",
    "target_test_performance = _model_predictions(model, private_testloader_intest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform membership inference attacks!!!\n",
      "With train acc 0.931 and test acc 0.748\n",
      "For membership inference attack via correctness, the attack acc is 0.591 \n",
      "For membership inference attack via confidence, the attack acc is 0.607\n",
      "For membership inference attack via entropy, the attack acc is 0.564\n",
      "For membership inference attack via modified entropy, the attack acc is 0.607\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Perform membership inference attacks!!!')\n",
    "MIA = black_box_benchmarks(shadow_train_performance,shadow_test_performance,\n",
    "                     target_train_performance,target_test_performance,num_classes=10)\n",
    "res = MIA._mem_inf_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABN4AAAOCCAYAAACoGh8kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC240lEQVR4nOzdfXgU9b3//1c2yJJwQuwhauSmaKwsiJW2uRErMWkMpASqTa225oRgjKVSOBzv2RSU0EJDemdbFW3VSqBpvKlGvwdRMPGHYqUm21ClSA2a2lbIIkQbUMySkPn9wcnKkt0ku9m7SZ6P69rrYmY/k3nN7ubN5r2f2YkxDMMQAAAAAAAAgKCyRDoAAAAAAAAAMBTReAMAAAAAAABCgMYbAAAAAAAAEAI03gAAAAAAAIAQoPEGAAAAAAAAhACNNwAAAAAAACAEaLwBAAAAAAAAIUDjDQAAAAAAAAiBEZEOMBCjR49WR0eHYmNjdeaZZ0Y6DoAQev/993X8+HGNGjVKH3/8caTjDAq1Cxhehkr9onYBw8tQqV0S9QsYTsxUu2IMwzAiHaI/sbGx6u7ujnQMAGFksVh0/PjxSMcYFGoXMDyZvX5Ru4Dhyey1S6J+AcORGWqXKWa89RRQi8Wis88+O9JxAIRQa2ururu7FRsbG+kog0btAoaXoVK/qF3A8DJUapdE/QKGEzPVLlM03s4880zt27dPZ599tt57771IxwEQQhMmTNC+ffuGxOkB1C5geBkq9YvaBQwvQ6V2SdQvYDgxU+3i4goAho1jx46prKxMI0aM0Lvvvtvv+FdeeUUzZsxQVlaWZsyYoe3bt4c+JAAAAABgyDDFjDcAGKx3331X1157rSZPnjyg7wD4xz/+oblz5+qZZ55Rdna2XnrpJc2bN09vvPGGJk2aFIbEAAAAAACzY8YbgGHho48+0saNG1VSUjKg8b/61a80ZcoUZWdnS5KysrJks9l0zz33hDAlAAAAAGAoofEGYFi48MIL9bnPfW7A4+vq6pSenu6xLj09XXV1dcGOBgAAAAAYomi8AYAXLS0tSk5O9liXnJyslpYWn9u4XC4dPnzYfTMMI9QxAQAAAABRjMYbAHhx9OhRWa1Wj3VWq1VHjx71uU1FRYUSExPdt/3794c6JgAAAAAgitF4AwAv4uPj5XK5PNa5XC7Fx8f73KasrEzt7e3u27hx40IdEwAAAAAQxbiqKQB4kZKSIqfT6bHO6XQqJSXF5zZWq9VjllxMTEzI8gEYPmpra7VmzRrFxcXJYrFo3bp1mjZtmtexPReEOdmhQ4d0+PBh/fOf/wxxUgAAAJxqSDbeyssjnSAwZs0NDEWXX365Xn31VY91DodDubm5IdunWWuAWXMDZtDQ0KDi4mI5HA7ZbDZt2LBBeXl52rNnjxISErxus23bNo/l2267LeQfBJi1Dpg1N4DgMGsNMGtuYLjiVFMAkFRSUqL58+e7l//nf/5He/bs0csvvyxJ2r59u/bs2aP//u//jlREAMNQZWWl8vPzZbPZJElFRUXq6upSVVWV1/GPPPKIx/Lx48dVXV2tkpKSkGcFAABAb0NyxhsAnOrYsWOaPXu2/v3vf0uSvv3tb2vixIl64oknJEkdHR3q7Ox0j580aZI2bdqk22+/XSNHjpTL5dKzzz6rSZMmRSI+gGGqvr5eK1ascC9bLBalpqaqrq5OS5Ys6TX+3HPP9Vh+/vnnNWnSJF1wwQUhzwoAAIDeaLwBGBZGjhzZ6/Srk9XU1PRal5mZqT/96U8hTAUAvrW1tam9vV3Jycke65OTk9XY2Dign7F+/fp+Z7u5XC6Pi8kYhuF/WAAAAHjFqaYAAABR6OjRo5LkcdGWnuWe+/ry4Ycfqq6uTt/+9rf7HFdRUaHExET3bf/+/YGHBgAAgAcabwAAAFEoPj5ekjxmo/Us99zXl5qaGuXn5ysxMbHPcWVlZWpvb3ffxo0bF3hoAAAAeOBUUwAAgCg0duxYJSYmyul0eqx3Op1KSUnpd/v169eroqKi33FWq9VjVl2or4AKAAAwnDDjDQAAIErl5OTI4XC4lw3DUFNTk3Jzc/vcbs+ePXr//feVk5MT6ogAAADoA403AACAKGW327V582Y1NzdLkqqrqxUbG6sFCxZIkkpKSjR//vxe261fv14LFixg9hoAAECEcaopAABAlMrIyFBVVZUKCwsVFxcni8WiLVu2KCEhQZLU0dGhzs5Oj22OHz+u6upqbd++PRKRAQAAcBIabwAAAFGsoKBABQUFXu+rqanptS42NlbvvfdeqGMBAABgADjVFAAAAAAAAAgBGm8AAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNAAAAAAAACAEabwAAAAAAAEAI0HgDAAAAAAAAQoDGGwAAAAAAABACNN4AAAAAAACAEKDxBgAAAAAAAIQAjTcAAAAAAAAgBGi8AQAAAAAAACFA4w0AAAAAEHS1tbVKS0tTZmamsrKytHv37gFtt2nTJsXExGj9+vWhDQgAYTAi0gEAAAAAAENLQ0ODiouL5XA4ZLPZtGHDBuXl5WnPnj1KSEjwud3HH3+sFStWhDEpAIQWM94AAAAAAEFVWVmp/Px82Ww2SVJRUZG6urpUVVXV53Z33XWXFi1aFI6IABAWNN4AAAAAAEFVX1+v9PR097LFYlFqaqrq6up8brNz5041NDRo4cKF4YgIAGERUOPNn3P1DcPQ6tWrNX36dGVlZSktLU2/+c1vAg4MAAAAAIhebW1tam9vV3Jyssf65ORktbS0eN2mu7tbixcv1n333aeYmJgB7cflcunw4cPum2EYg84OAMHm93e8+Xuu/m9/+1v95Cc/0Ztvvqnx48frX//6ly688EKNHz9ec+fODcpBAAAADFW1tbVas2aN4uLiZLFYtG7dOk2bNs3n+EOHDslut+vtt9/WRx99pI6ODt1555361re+FcbUAIazo0ePSpKsVqvHeqvV6r7vVPfee69mzpypiy66aMD7qaio0KpVqwIPCgBh4PeMN3/P1f/LX/6iKVOmaPz48ZKkiRMnymazaevWrYOIDQAAMPT1fOBZXV2t7du3q7S0VHl5eTpy5IjX8ceOHVNubq4uu+wybdu2TQ6HQ3PmzFFjY2OYkwMYzuLj4yWdmJF2MpfL5b7vZPv27dNDDz2klStX+rWfsrIytbe3u2/jxo0LPDQAhIjfjTd/z9W/8sortWfPHu3atUuS9Prrr+uvf/2rzjrrrAAjAwAADA/+fuD50EMPadSoUSouLnavW7ZsmUpLS8OSFwAkaezYsUpMTJTT6fRY73Q6lZKS0mt8z6SMuXPnKjs7W9nZ2ZKktWvXKjs7W6+88orX/VitVo0ZM8Z9G+gpqgAQTn413gI5Vz83N1ePPPKIcnJydMEFF+hLX/qSMjIy9L3vfc/nfjhXHwAAwP8PPJ988kllZWV5rEtKStLUqVNDmhMATpWTkyOHw+FeNgxDTU1Nys3N7TW2pKREb7zxhrZt2+a+SZLdbte2bds0c+bMcMUGgKDzq/EWyLn6mzZt0sKFC/X888/rzTff1N69e/XVr37V6xTjHhUVFUpMTHTf9u/f709MAAAA0wvkA89du3YpLi5OixYt0qWXXqqvfOUreuCBB/r8EJMPPAGEgt1u1+bNm9Xc3CxJqq6uVmxsrBYsWCDpRLNt/vz5kYwIAGHhV+PN33P1JWn58uX6xje+odTUVElSSkqK9u7dqyVLlvjcD+fqAwCA4S6QDzw//PBDVVRU6IorrtAf//hH/eY3v1F5ebl+/OMf+9wPH3gCCIWMjAxVVVWpsLBQmZmZevDBB7Vlyxb3Bfk6Ojr0ySef9Nqu5/TSU/8NAGblV+PN33P1JWnv3r0655xzPNade+65+sMf/uBzP5yrDwAAhrtAPvC0WCzKyMjQnDlzJEnnn3++rr/+et19990+98MHngBCpaCgQA6HQ9u3b9dLL73kcUXmmpoar38T9pxeahiG/va3v7lPOwUAs/L74gr+nKsvSePHj1dra6vHutbWVsXFxfm7awAAgGEjkA88J06cqAkTJnismzRpkg4cOOB1ZonEB54AAAChNMLfDex2u3Jzc9Xc3KzJkyd7PVe/q6tLGzdulCRdf/31+vnPf6477rhDn/3sZ/WPf/xDjz76qMfVtgAgHGpra7VmzRrFxcXJYrFo3bp1Hp+8nswwDK1Zs0ZPPPGETj/9dH388cdauHChFi5cGLJ82dvKQ/azQ6s80gGAIcvXB57Lly/3Oj4zM1N///vfPdYdOHBASUlJfOgJAAAQAX433k4+V7/nj9dTz9Xv7Ox0j7/99tsVExOjr3/964qPj9fhw4e1aNEirVixInhHAQD9aGhoUHFxsRwOh2w2mzZs2KC8vDzt2bPHXb9O9tvf/lY/+clP9Oabb2r8+PH617/+pQsvvFDjx4/X3LlzI3AEAIYjfz/wvPnmmzVjxgw1NjYqPT1dH3zwgTZs2KClS5dG8jAAAACGLb8bb9KJc/ULCgq83ldTU+O5gxEjZLfbZbfbA9kVAARFZWWl8vPzZbPZJElFRUW64447VFVV5fViL3/5y180ZcoUjR8/XtKJ07dsNpu2bt1K4w1A2Pj7gedFF12k2tpaLV68WKeddpq6urq0cOFC3XrrrZE6BAAAgGEtoMYbAJhNfX29x0xbi8Wi1NRU1dXVeW28XXnllaqqqtKuXbv0+c9/Xq+//rr++te/6utf/3oYUwOAfx94SlJeXp7y8vJCHQsAAAADQOMNwJDX1tam9vZ2JScne6xPTk5WY2Oj121yc3P1yCOPKCcnR2eccYbeeustZWZm6nvf+57P/bhcLo+rDxqGEZwDAAAAAACYkt9XNQUAszl69KikE1fuO5nVanXfd6pNmzZp4cKFev755/Xmm29q7969+upXv6r4+Hif+6moqFBiYqL7tn///uAdBAAAAADAdGi8ARjyepplJ89G61n21Uhbvny5vvGNbyg1NVWSlJKSor1793o9LbVHWVmZ2tvb3bdx48YF6QgAAAAAAGZE4w3AkDd27FglJibK6XR6rHc6nUpJSfG6zd69e3XOOed4rDv33HP1hz/8wed+rFarxowZ477FxMQMOjsAAAAAwLxovAEYFnJycuRwONzLhmGoqalJubm5XsePHz9era2tHutaW1sVFxcX0pwAAAAAgKGDxhuAYcFut2vz5s1qbm6WJFVXVys2NlYLFiyQJJWUlGj+/Pnu8ddff70ee+wx/fOf/5Qk/eMf/9Cjjz6qa665JvzhAQAAAACmxFVNAQwLGRkZqqqqUmFhoeLi4mSxWLRlyxYlJCRIkjo6OtTZ2ekef/vttysmJkZf//rXFR8fr8OHD2vRokVasWJFpA4BAAAAUHl5pBMExqy5gcGi8QZg2CgoKFBBQYHX+2pqajyWR4wYIbvdLrvdHo5oAAAAAIAhiFNNAQAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNAAAAAAAACAEabwAAAAAAAEAIjIh0gFDI3lYe6QgBKo90AAAAAAAAAAQJM94AAAAAAACAEKDxBgAAAAAAAIQAjTcAAAAAAAAgBGi8AQAAAAAAACFA4w0AAAAAAAAIARpvAAAAAAAAQAjQeAMAAAAAAABCYESkAwAAAMC32tparVmzRnFxcbJYLFq3bp2mTZvmdez69eu1du1aJScne6x/7rnnFBcXF464AAAAOAmNNwAAgCjV0NCg4uJiORwO2Ww2bdiwQXl5edqzZ48SEhK8bmO323XdddeFNygAAAC84lRTAACAKFVZWan8/HzZbDZJUlFRkbq6ulRVVRXhZAAAABgIGm8AAABRqr6+Xunp6e5li8Wi1NRU1dXVRTAVAAAABorGGwAAQBRqa2tTe3t7r+9rS05OVktLi8/tNm3apJycHM2cOVPXXHONdu7c2ed+XC6XDh8+7L4ZhhGU/AAAAKDxBgAAEJWOHj0qSbJarR7rrVar+75TnXXWWTr//PP13HPP6ZVXXtGcOXN08cUX99l8q6ioUGJiovu2f//+4B0EAADAMEfjDQAAIArFx8dLOjEj7WQul8t936nmzJmjiooKd7OupKRE06dP109/+lOf+ykrK1N7e7v7Nm7cuCAdAQAAALiqKQAAQBQaO3asEhMT5XQ6PdY7nU6lpKQM+Oecd955euedd3zeb7VaPWbVxcTE+B8WAAAAXjHjDQAAIErl5OTI4XC4lw3DUFNTk3Jzc72OLysr63Ua6r59+zRx4sSQ5gQAAIB3NN4AAACilN1u1+bNm9Xc3CxJqq6uVmxsrBYsWCDpxKmk8+fPd4/fsWOHHn74YffyCy+8oFdffVU33nhjeIMDAABAEqeaAgAARK2MjAxVVVWpsLBQcXFxslgs2rJlixISEiRJHR0d6uzsdI9ftmyZ7r33Xj3xxBM6fvy4uru79dRTT+nyyy+P1CEAAAAMazTeAAAAolhBQYEKCgq83ldTU+OxPGfOHM2ZMyccsQAAADAAnGoKAAAAAAAAhACNNwAAAABA0NXW1iotLU2ZmZnKysrS7t27fY59+eWXdfXVVysnJ0eXXXaZpk+frvvuuy+MaQEgNDjVFAAAAAAQVA0NDSouLpbD4ZDNZtOGDRuUl5enPXv2uL+n8mS///3v9fnPf1533XWXJOn111/Xl770JZ1zzjmaO3duuOMDQNAw4w0AAAAAEFSVlZXKz8+XzWaTJBUVFamrq0tVVVVexy9dulQ333yze3n69Ok6/fTT3Vd1BgCzCmjGW21trdasWeO+uta6des0bdo0n+MPHToku92ut99+Wx999JE6Ojp055136lvf+lbAwQEAABAdsreVRzpCgMojHQAYsurr67VixQr3ssViUWpqqurq6rRkyZJe4y+44AL3v7u7u/Xwww/LarXq6quvDkteAAgVv2e89UwZrq6u1vbt21VaWqq8vDwdOXLE6/hjx44pNzdXl112mbZt2yaHw6E5c+aosbFx0OEBAAAAANGlra1N7e3tSk5O9lifnJyslpaWPrddvXq1zj77bP3iF7/Q1q1bNWHCBJ9jXS6XDh8+7L4ZhhGU/AAQTH433vydMvzQQw9p1KhRKi4udq9btmyZSktLA4wMAAAAAIhWR48elSRZrVaP9Var1X2fLytWrJDT6dRNN92krKws7dq1y+fYiooKJSYmum/79+8ffHgACDK/G2/19fVKT0//9AecNGXYmyeffFJZWVke65KSkjR16lR/dw0AAAAAiHLx8fGSTsxIO5nL5XLf15eYmBh95zvf0dSpU/WDH/zA57iysjK1t7e7b+PGjRtccAAIAb8ab4FMGd61a5fi4uK0aNEiXXrppfrKV76iBx54oM9pwEwZBgAAAABzGjt2rBITE+V0Oj3WO51OpaSkeN3m2LFjvdbZbDa9+eabPvdjtVo1ZswY9y0mJmZwwQEgBPxqvAUyZfjDDz9URUWFrrjiCv3xj3/Ub37zG5WXl+vHP/6xz/0wZRgAAAAAzCsnJ0cOh8O9bBiGmpqalJub63V8ampqr3Wtra3MYgNgen413gKZMmyxWJSRkaE5c+ZIks4//3xdf/31uvvuu33uhynDAAAAAGBedrtdmzdvVnNzsySpurpasbGxWrBggSSppKRE8+fPd48/cuSI1q1b515+6aWXtHXrVl1//fXhDQ4AQTbCn8GBTBmeOHFiryvRTJo0SQcOHNAnn3yiuLi4XttYrVaPWXVMGQYQDLW1tVqzZo3i4uJksVi0bt06TZs2zef4Q4cOyW636+2339ZHH32kjo4O3XnnnfrWt74VxtQAAADmk5GRoaqqKhUWFrrfe23ZskUJCQmSpI6ODnV2drrH/+hHP9KDDz6o3/3ud7JYLHK5XHr44Yd17bXXRuoQACAo/Gq8Sb6nDC9fvtzr+MzMTP3973/3WHfgwAElJSV5bboBQCg0NDSouLhYDodDNptNGzZsUF5envbs2eN+A3iyY8eOKTc3V7fccoseeughSdLtt9+uxsZGGm8AAAADUFBQoIKCAq/31dTUeCwXFhaqsLAwHLEAIKz8vqqpv1OGb775ZjU0NKixsVGS9MEHH2jDhg1aunRpMPIDwIBUVlYqPz9fNptNklRUVKSuri5VVVV5Hf/QQw9p1KhRKi4udq9btmyZSktLw5IXAAAAAGB+fs9483fK8EUXXaTa2lotXrxYp512mrq6urRw4ULdeuutwTsKAOhHfX29VqxY4V62WCxKTU1VXV2dlixZ0mv8k08+qaysLI91SUlJSkpKCnlWAAAAAMDQ4HfjTfJvyrAk5eXlKS8vL5BdAcCgtbW1qb29XcnJyR7rk5OT3bNxT7Vr1y5lZmZq0aJFeuONNzRy5Eh961vf0ne/+12f3zvpcrk8Lj5jGEbwDgIAAAAAYDoBNd4AwEyOHj0qSR4XbelZ7rnvVB9++KEqKir09NNP6/7779fevXuVmZmp9vZ2LVu2zOs2FRUVWrVqVXDDAwAAAABMy+/veAMAs4mPj5ckj9loPcs9953KYrEoIyNDc+bMkSSdf/75uv7663X33Xf73E9ZWZna29vdt3HjxgXpCAAAAAAAZsSMNwBD3tixY5WYmCin0+mx3ul0KiUlxes2EydO1IQJEzzWTZo0SQcOHNAnn3zi9arMVqvVY1adr1NSAQAAAADDAzPeAAwLOTk5cjgc7mXDMNTU1KTc3Fyv4zMzM9Xa2uqx7sCBA0pKSvLadAMAAAAA4FTMeAMwLNjtduXm5qq5uVmTJ09WdXW1YmNjtWDBAklSSUmJurq6tHHjRknSzTffrBkzZqixsVHp6en64IMPtGHDBi1dujSShwEAAIAgyd5WHukIAdmWXR7pCAD8QOMNwLCQkZGhqqoqFRYWKi4uThaLRVu2bFFCQoIkqaOjQ52dne7xF110kWpra7V48WKddtpp6urq0sKFC3XrrbdG6hAAAAAAACZD4w3AsFFQUKCCggKv99XU1PRal5eXp7y8vFDHAoA+1dbWas2aNe4PDdatW6dp06b1u92mTZv0ta99TY888oiuu+660AcFAABALzTeAAAAolRDQ4OKi4vlcDhks9m0YcMG5eXlac+ePe4Zu958/PHHWrFiRRiTAgAAwBsurgAAABClKisrlZ+fL5vNJkkqKipSV1eXqqqq+tzurrvu0qJFi8IREQAAAH2g8QYAABCl6uvrlZ6e7l62WCxKTU1VXV2dz2127typhoYGLVy4MBwRAQAA0AdONQUAAIhCbW1tam9vV3Jyssf65ORkNTY2et2mu7tbixcv1gMPPKCYmJgB7cflcsnlcrmXDcMIPDQAAAA8MOMNAAAgCh09elSSZLVaPdZbrVb3fae69957NXPmTF100UUD3k9FRYUSExPdt/379wceGgAAAB5ovAEAAESh+Ph4SfKYjdaz3HPfyfbt26eHHnpIK1eu9Gs/ZWVlam9vd9/GjRsXeGgAAAB44FRTAACAKDR27FglJibK6XR6rHc6nUpJSek1fuvWrZKkuXPneqxfu3at1q9fr9WrV2vmzJm9trNarR6z6gZ6iioAAAD6R+MNAAAgSuXk5MjhcLiXDcNQU1OTli9f3mtsSUmJSkpKPNbFxMTIbrfruuuuC3VUAAAAeMGppgAAAFHKbrdr8+bNam5uliRVV1crNjZWCxYskHSi2TZ//vxIRgQAAEAfmPEGAAAQpTIyMlRVVaXCwkLFxcXJYrFoy5YtSkhIkCR1dHSos7Oz13Zr167V888/7/73+vXrtW3btnBGBwAAgGi8AQAARLWCggIVFBR4va+mpsbrervdLrvdHspYAAAAGABONQUAAAAAAABCgMYbAAAAAAAAEAI03gAAAAAAAIAQoPEGAAAAAAAAhAAXVwAAAAAAwCSyt5VHOkKAyiMdAIgIZrwBAAAAAAAAIUDjDQAAAAAAAAgBGm8AAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARGRDoAAAAAAAAY4srLI50gMGbNjajBjDcAAAAAAAAgBGi8AQAAAAAAACFA4w0AAAAAAAAIARpvAAAAAAAAQAjQeAMAAAAAAABCgMYbAAAAACDoamtrlZaWpszMTGVlZWn37t0+x9bV1emKK65QTk6OLrnkEs2ePVs7d+4MY1oACA0abwAAAACAoGpoaFBxcbGqq6u1fft2lZaWKi8vT0eOHPE6/sYbb9TXvvY1vfjii9qxY4dmzJihWbNm6f333w9zcgAIroAab/58cnGyTZs2KSYmRuvXrw9ktwAAAMOOP++7Xn75ZV199dXKycnRZZddpunTp+u+++4LY1oAOKGyslL5+fmy2WySpKKiInV1damqqsrr+LS0NJWWlrqXly5dqra2NtXV1YUlLwCEit+NN38/uejx8ccfa8WKFQEHBQAAGG78fd/1+9//Xp///Of14osv6uWXX9aGDRu0dOlSPfvss2FODmC4q6+vV3p6unvZYrEoNTXVZyPt0UcflcXy6Z+no0aNkiQdO3YstEEBIMT8brz5+8lFj7vuukuLFi0KLCUAAMAw5O/7rqVLl+rmm292L0+fPl2nn366mpubw5IXACSpra1N7e3tSk5O9lifnJyslpaWAf2MHTt2KC4uTvPmzfM5xuVy6fDhw+6bYRiDyg0AoeB3483fTy4kaefOnWpoaNDChQsDSwkAADAM+fu+64ILLlBCQoIkqbu7Ww8++KCsVquuvvrqsOQFAEk6evSoJMlqtXqst1qt7vv6YhiGVq9erR/+8IdKSkryOa6iokKJiYnu2/79+wcXHABCwK/GWyCfXHR3d2vx4sW67777FBMTM6D98MkFgFDg+ykBmMlgZoysXr1aZ599tn7xi19o69atmjBhgs+xvO8CEGzx8fGSTtSXk7lcLvd9fSkvL9f48eN166239jmurKxM7e3t7tu4ceMCDw0AIeJX4y2QTy7uvfdezZw5UxdddNGA98MnFwCCje+nBGA2g5kxsmLFCjmdTt10003KysrSrl27fI7lfReAYBs7dqwSExPldDo91judTqWkpPS57a9//Ws1NjYO6ANPq9WqMWPGuG8DnegBAOHkV+PN308u9u3bp4ceekgrV670KxSfXAAINr6fEoDZDHbGSExMjL7zne9o6tSp+sEPfuBzHO+7AIRCTk6OHA6He9kwDDU1NSk3N9fnNjU1NXrsscf05JNPauTIkWppaeGqpgBMz6/Gm7+fXGzdulWSNHfuXGVnZys7O1uStHbtWmVnZ+uVV17xuh8+uQAQbHw/JQCzCWTGiLer/9lsNr355ps+98P7LgChYLfbtXnzZvfFXaqrqxUbG6sFCxZIkkpKSjR//nz3+E2bNslut+vOO+/U7t275XA49MILL/j8mxEAzGKEvxv4+uRi+fLlvcaWlJSopKTEY11MTIzsdruuu+46/9MCQAD6+p6kxsZGr9v0fD/lAw884Nf3U548M4XvSQIwWP6875Kk1NTUXqeVtra2MosNQNhlZGSoqqpKhYWFiouLk8Vi0ZYtW9wXgOno6FBnZ6d7fElJiQ4dOqScnByPn+Pv2VMAEG38brzZ7Xbl5uaqublZkydP9vrJRVdXlzZu3Bj0sAAQiHB+P+WqVasCDwoAp/D3fdeRI0e0bt06fe9735MkvfTSS9q6dSvvywBEREFBgQoKCrzeV1NT47F88ODBcEQCgLDzu/Hm7ycXPdauXavnn3/e/e/169dr27Ztg0sPAAMQ6PdT7tixw6/9lJWV6ZZbbnEvT506lS8pBzAo/r7v+tGPfqQHH3xQv/vd72SxWORyufTwww/r2muvjdQhAAAADGt+N94k/z656GG322W32wPZHQAMymC+n/JkPR8arF69WjNnzuy1ndVq9ZhVx/ckAQgGf953FRYWqrCwMByxAAAAMAABNd4AwGz4fkoAAAAAQLj5dVVTADArf6+sBQAAAADAYDHjDcCwwPdTAgAAAADCjcYbgGGD76cEAAAAAIQTp5oCAAAAAAAAIUDjDQAAAAAAAAgBGm8AAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARGRDoAAAAAAABAVCovj3SCwJg19xDEjDcAAAAAAAAgBGi8AQAAAAAAACFA4w0AAAAAAAAIARpvAAAAAAAAQAjQeAMAAAAAAABCgMYbAAAAAAAAEAI03gAAAAAAAIAQoPEGAAAAAAAAhACNNwAAAAAAACAEaLwBAABEsdraWqWlpSkzM1NZWVnavXu3z7F1dXW64oorlJOTo0suuUSzZ8/Wzp07w5gWAAAAJ6PxBgAAEKUaGhpUXFys6upqbd++XaWlpcrLy9ORI0e8jr/xxhv1ta99TS+++KJ27NihGTNmaNasWXr//ffDnBwAAAASjTcAAICoVVlZqfz8fNlsNklSUVGRurq6VFVV5XV8WlqaSktL3ctLly5VW1ub6urqwpIXAAAAnmi8AQAARKn6+nqlp6e7ly0Wi1JTU3020h599FFZLJ++vRs1apQk6dixY6ENCgAAAK9GRDoAAAAAemtra1N7e7uSk5M91icnJ6uxsXFAP2PHjh2Ki4vTvHnzfI5xuVxyuVzuZcMwAgsMAACAXpjxBgAAEIWOHj0qSbJarR7rrVar+76+GIah1atX64c//KGSkpJ8jquoqFBiYqL7tn///sEFBwAAgBuNNwAAgCgUHx8vSR6z0XqWe+7rS3l5ucaPH69bb721z3FlZWVqb29338aNGxd4aAAAAHjgVFMAAIAoNHbsWCUmJsrpdHqsdzqdSklJ6XPbX//612psbNTTTz/d736sVqvHrLqYmJiA8gIAAKA3ZrwBAABEqZycHDkcDveyYRhqampSbm6uz21qamr02GOP6cknn9TIkSPV0tLCVU0BAAAihBlvAAAAUcputys3N1fNzc2aPHmyqqurFRsbqwULFkiSSkpK1NXVpY0bN0qSNm3aJLvdrvXr12v37t2SpD//+c9qbW3ts1kHAACA0KDxBgAAEKUyMjJUVVWlwsJCxcXFyWKxaMuWLUpISJAkdXR0qLOz0z2+pKREhw4dUk5OjsfPWblyZVhzA0NKeXmkE/jPjJkBYIii8QYAABDFCgoKVFBQ4PW+mpoaj+WDBw+GIxIAAAAGiMYbAAAAAAAIqW3bIp0gMNnZkU4As+PiCgAAAAAAAEAI0HgDAAAAAAAAQoDGGwAAAAAAABACNN4AAAAAAEFXW1urtLQ0ZWZmKisrS7t37+5zfHd3t37+858rLi5O28z6hWAAcIqAGm/+FNC6ujpdccUVysnJ0SWXXKLZs2dr586dAQcGAAAAAES3hoYGFRcXq7q6Wtu3b1dpaany8vJ05MgRr+M//PBDzZo1S3/729/U0dER5rQAEDp+X9W0p4A6HA7ZbDZt2LBBeXl52rNnjxISEnqNv/HGG7Vs2TJ95zvfkSTdddddmjVrlt58802deeaZgz8CAAAAAEBUqaysVH5+vmw2mySpqKhId9xxh6qqqrRkyZJe4z/++GNVVlYqKSlJDz74YLjjAkNPeXmkEwTGrLn74PeMN28FtKurS1VVVV7Hp6WlqbS01L28dOlStbW1qa6uLsDIABAYZusCAACER319vdLT093LFotFqampPv8OnDBhgtLS0sIVDwDCxu/Gm78F9NFHH5XF8uluRo0aJUk6duyYv7sGgID5e7rDjTfeqK997Wt68cUXtWPHDs2YMUOzZs3S+++/H+bkAAAA5tLW1qb29nYlJyd7rE9OTlZLS0vQ9uNyuXT48GH3zTCMoP1sAAgWvxpvwSigO3bsUFxcnObNm+dzDAUUQLAxWxcAACA8jh49KkmyWq0e661Wq/u+YKioqFBiYqL7tn///qD9bAAIFr8ab4MtoIZhaPXq1frhD3+opKQkn+MooACCjdm6AAAA4REfHy/pxISKk7lcLvd9wVBWVqb29nb3bdy4cUH72QAQLH413gZbQMvLyzV+/HjdeuutfY6jgAIIJmbrAgAAhM/YsWOVmJgop9Ppsd7pdColJSVo+7FarRozZoz7FhMTE7SfDQDB4tdVTQdTQH/961+rsbFRTz/9dL/7sVqtHrPqKKAABiOcs3VXrVo1uLAAAAxVQ/BKdfAtJydHDofDvWwYhpqamrR8+fIIpgKA8PP74gq+Cmhubq7PbWpqavTYY4/pySef1MiRI9XS0sL3JAEIG2brAgAAhJfdbtfmzZvV3NwsSaqurlZsbKwWLFggSSopKdH8+fMjGREAwsKvGW/SiQKam5ur5uZmTZ482WsB7erq0saNGyVJmzZtkt1u1/r167V7925J0p///Ge1trb22awDgGBhti4AAEB4ZWRkqKqqSoWFhYqLi5PFYtGWLVuUkJAgSero6FBnZ6fHNt/4xjfc3+9900036fTTT1d9fb1iY2PDnh8AgsXvxpu/BbSkpESHDh1STk6Ox89ZuXLlIKMDwMAFcrpDz2zdZ5991j1bt6WlhQ8NAAAABqCgoEAFBQVe76upqem17qmnngp1JAAIO78bb5J/BfTgwYOB7AIAgorZugAAAACAcAuo8QYAZsNsXQAAAABAuNF4AzBsMFsXAAAAABBOfl/VFAAAAAAAAED/aLwBAAAAAAAAIcCppgAAABiWyssjnSAwZs0NAMBwROMNAAAgitXW1mrNmjXuC8OsW7dO06ZN8zm+u7tbv/jFL7R8+XI999xzys7ODl9YhAedNwAATIPGGwAAQJRqaGhQcXGxHA6HbDabNmzYoLy8PO3Zs8d9VeaTffjhh/rmN7+p8847Tx0dHRFIDAAAgJPxHW8AAABRqrKyUvn5+bLZbJKkoqIidXV1qaqqyuv4jz/+WJWVlfr+978fzpgAAADwgcYbAABAlKqvr1d6erp72WKxKDU1VXV1dV7HT5gwQWlpaeGKBwAAgH5wqikAAEAUamtrU3t7u5KTkz3WJycnq7GxMWj7cblccrlc7mXDMIL2swEAAIY7ZrwBAABEoaNHj0qSrFarx3qr1eq+LxgqKiqUmJjovu3fvz9oPxsAAGC4Y8YbYDZmvZKZWXMDQITEx8dLksdstJ7lnvuCoaysTLfccot7eerUqTTfAAAAgoTGGwAAQBQaO3asEhMT5XQ6PdY7nU6lpKQEbT9Wq9VjVl1MTEzQfjYAAMBwx6mmAAAAUSonJ0cOh8O9bBiGmpqalJubG8FUAAAAGChmvAEAAEQpu92u3NxcNTc3a/LkyaqurlZsbKwWLFggSSopKVFXV5c2btwY4aTmlL2tPNIRApMd6QCB2bYt0gkCk50d6QQAADOj8QYAABClMjIyVFVVpcLCQsXFxclisWjLli1KSEiQJHV0dKizs9Njm2984xvu72i76aabdPrpp6u+vl6xsbFhzw8AADDc0XgDAACIYgUFBSooKPB6X01NTa91Tz31VKgjAQAAYID4jjcAAAAAAAAgBGi8AQAAAAAAACFA4w0AAAAAAAAIARpvAAAAAAAAQAhwcQUAAAAAAAAvtm2LdILAZGdHOgF6MOMNAAAAAAAACAEabwAAAAAAAEAIcKopAAAAYCJmPe0JAIDhiBlvAAAAAAAAQAjQeAMAAAAAAABCgMYbAAAAAAAAEAI03gAAAAAAAIAQ4OIK0aS8PNIJhhcebwAAAAAAEEI03gAAMBOzfmhg1twAAADAINB4AwAMjlkbKmbNDQAAAMA0aLxh+OKPbgAAAAAAEEJcXAEAAAAAAAAIAWa8AQCA0DPrLGOz5gYQNNu2RTqB/7IjHQAA4EbjLYqY8T91ScrOjnQCAAAAAACA6MOppgAAAAAAAEAIMOMNADA8cQohAAAAhijTnlEX6QAhwIw3AAAAAAAAIAQCmvFWW1urNWvWKC4uThaLRevWrdO0adN8jn/llVd02223yWq1yuVy6Sc/+YkyMzMDDo3oYtpOenakEyDcqF0AzIjaBcCsqF8AEEDjraGhQcXFxXI4HLLZbNqwYYPy8vK0Z88eJSQk9Br/j3/8Q3PnztUzzzyj7OxsvfTSS5o3b57eeOMNTZo0KSgHAQD9oXYBMCNqFwCzon4BwAl+n2paWVmp/Px82Ww2SVJRUZG6urpUVVXldfyvfvUrTZkyRdn/N70oKytLNptN99xzT+CpAcBP1C4AZkTtAmBW1C8AOMHvGW/19fVasWKFe9lisSg1NVV1dXVasmRJr/F1dXW9pgenp6errq4ugLgATHtqb4T3T+0KHdO+JrMjnQDoH7ULgFlRvwDgBL8ab21tbWpvb1dycrLH+uTkZDU2NnrdpqWlRVdffXWv8S0tLT7343K55HK53Mvvv/++JKm1tVUTJkzoN6fr4OF+xwDWpkgnCMxJvxqmYp3w0IDGtba2Svr09z4YqF3wakekA8AMrA8NrHZJwa9fZqldEvULiDYDfd8l8d4LQHSJ5N+NoeJX4+3o0aOSJKvV6rHearW67/O2jT/jJamiokKrVq3qtb67u1v79u3zJzLg27FIBxhm9h3xa/jx48eDtmtqF4CA+Vm7pODVL2oXgIBFsHZJ1C8AgxDBvxtDxa/GW3x8vCR5fKrQs9xzn7dt/BkvSWVlZbrlllvcy8nJyXK5XIqNjdWZZ57pT+SwO3DggM4666xIxxg0sx6HGXObJXO4cr7//vs6fvy4Ro0aFbSfGe21yyyvAV/Mmt9suc2QN9ozhjpfsOtXtNeuaBXtr8NADbXjGirHY+bj6Mk+HN97DYSZn9v+DLVjGyrHY+bjiET2UNSuUPGr8TZ27FglJibK6XR6rHc6nUpJSfG6TUpKil/jpROfbJz8aUdfn3JEmwsuuEBvvvlmpGMMmlmPw4y5zZLZLDm9ifbaZebHVjJvfrPlNkPeaM8Y7flOFe21K1qZ7XkeqKF2XEPleMx8HKHMPhTql5mf2/4MtWMbKsdj5uMwc/Zw8Puqpjk5OXI4HO5lwzDU1NSk3Nxcr+Mvv/xyj/GS5HA4fI43u8WLF0c6QlCY9TjMmNssmc2S05dorl1mf2zNmt9suc2QN9ozRns+b6K5dkUrMz7PAzHUjmuoHI+ZjyPU2c1ev8z83PZnqB3bUDkeMx+HmbOHheGn1157zUhISDDeeustwzAMY+PGjcb48eONw4cPG4ZhGNddd51RVFTkHv/uu+8aY8aMMV566SXDMAzj5ZdfNhISEox3333X310DQMCoXQDMiNoFwKyoXwBwgl+nmkpSRkaGqqqqVFhYqLi4OFksFm3ZskUJCQmSpI6ODnV2drrHT5o0SZs2bdLtt9+ukSNHyuVy6dlnn9WkSZOC1z0EgH5QuwCYEbULgFlRvwDghBjDMIxIhwAAAAAAAACGGr+/4w0AAAAAAABA//w+1RTR4dlnn9Xzzz+v0aNH65xzztGNN94Y6Uh+M8MxmCHjQJjpOMyU1UyGwuNq1mOIxtzRmMkf0Z4/2vMhOMz+PJs9/0ANleM083GYOXs4DOXHx6zHZtbcA2Xm4zNt9kh/yRz8d+TIEWPy5MlGZ2enYRiGMWPGDOOdd96JcCr/mOEYzJBxIMx0HGbKaiZD4XE16zFEY+5ozOSPaM8f7fkQHGZ/ns2ef6CGynGa+TjMnD0chvLjY9ZjM2vugTLz8Zk5+5A+1fTYsWMqKyvTiBEj9O677/Y51uVy6eabb9YXvvAFZWVl6eKLL1ZtbW3EctXW1iotLU2XXnqpPvvZz3qM/dOf/qTPfe5zGjHixITF9PR0bd26VZLU2dmp1atXa8aMGbrkkks0Y8YMvfzyyxE5jp5jyMzMVFZWlnbv3i1Jevzxx5WVlaW2tjZdcskluuqqqzR58mT3MYQr38kZp0yZos985jOaMWOG0tPTddVVV6m2ttbn4+zPPoJ9LPfee69mz56tyy+/3J31/vvv9/pYS1JlZaU++OAD5eXlKT09XQcPHlR1dXXIc/b3mHvL2tdrOxz8fT6nTJmi7Oxsj9vnPvc5XXbZZWHPFkjNiKa6d+zYMV111VWKiYlRenp6v6+NL33pS7rpppuiqtYFWkeCnUeSbrvtNo0ZM0ann366EhISNGvWLLW0tEjq+/csXDXt97//vc4991zFxMRo+vTpuuqqq9z5pN514mc/+5m77qWmpqq9vV3//Oc/e+UPBrPXsWjj72vKDO9jJN/P8+OPP97r/+iTX9vhyu5P/scff1z79u1Tfn6+X5kj8R4omDU4XM/VQI7h8ccf1xe+8AWNGTNGCQkJOuOMM/T888+77/f2evvJT36iefPmadasWZo5c6ZSU1P1+OOPRyR/OGriUPi7ztfjk5KSojvvvFMjRoyQzWbr9/Exa508/fTTNW/ePFPUx9GjRys/P1+XX365WlpaTFUX/a0n4Xpe+svdY6i+xxqyjbd3331XWVlZ2r9/v44fP97v+NWrV+uZZ57R9u3b9dJLL+mBBx7Qt7/9bb3++uthz9XQ0KDi4mJVVlaqu7tbn/3sZ3X8+HF99NFHkqSDBw+6rwYkSWPGjNHBgwclScuXL1dNTY22bNmiHTt2qLy8XHPmzNE777wT1uPoOYbq6mpt375dpaWlysvL05EjR1RUVKTZs2crNzdXr732mhISErRp0ybt378/bPlOzdjS0qIbbrhB7733nurq6pSQkKA77rhD8fHx7vEnP84D3UcojuWWW27Rbbfdpvr6er322mtyuVxavHixfvvb3/Z6rCWpoqJC06ZNc48fPXq0fvnLX6qjoyOkOb3p63Uh9f3aDrVAns/k5GRt27bN4/aFL3xB3/rWt8KaLdCaES11791331Vqaqr+3//7f5KkJ554ot/XRlNTk/70pz9FVa0LpI4EO09Ppp/97Ge6++679e9//1v33HOPXn31Vc2ePVsdHR0+Xw/hrGlFRUX63Oc+J+nEG6yEhAR99atfVUdHh9c6cdttt+l73/ue6uvrdddddykuLs49Pph1wux1LNoE8poyw/sYyffzXFRU5PF/9Mmv7XBl9zd/UVGRZs6cqf/6r/8acOZIvQc61WBqcDieq4EcgyT913/9l9566y01Njaqvb1dU6ZM0bx589xZvb3e6urqdM011+iFF17QK6+8ovLycl177bXatWtXWPOHoyYOhb/rfD0+b775phobG90/4z/+4z/6fXzMWie3bt2q9PR0U9TH119/3V0fbrjhBkkyTV30t56E43kZSG5paL/HGrKNt48++kgbN25USUnJgMb/5S9/UXp6uvuJ/OIXv6jExES9+OKLXscbhqEbbrhBb731Vq/7fvnLX+qxxx4LOFdlZaXy8/N11llnaePGjfrBD34gSXryySclSWeccYb7xSdJhw8f1hlnnKHu7m7dd999Ki0tVWJioqQTBeKcc87Rr371q6Adw0COo+cYbDabpBNvbrq6ulRVVaUrr7xSl19+uY4cOSKLxaIlS5bogw8+0CeffBK0jP48zjabTVdeeaUqKyvV1dWljRs3asmSJXI6ndq3b597fM/j7M8+Bnsc3vaTm5ur2bNnS5IsFouOHj0qwzDcTZaTH2tJ+vKXv6zRo0e7x0+ZMkVtbW1qamoKalZ/H3NvWX29tsPB35ohSY888ojH8gcffKAXXnhBhYWFvcaG4/fN35oRLXXvo48+0rhx4zRz5kz3ur5eG93d3XI4HLrsssuiqtb5U0dC/Xs2YcIElZaWSpKKi4s1atQovfPOO2pqavL5eghnTcvLy9Py5cslyf3/wN69e9XU1OS1TlitVr333nuSpLPOOkvjxo1zj/dWJ4ZrHYs2/tZVs7yPkXw/z1deeaXH/9Env7aDlT/Yr9Mrr7xSSUlJOuOMM/rN7E+GwR7nQPYzmPdy/jxXgzmOgTxWZ511lq644grZbDZZLBb97Gc/0/Hjx1VRUSHJ++vtm9/8psf7jezsbHV3d+vtt98OWvaB5A9HTRwKf9f5enzi4uJ04YUXun/GRx991OfjY+Y6mZKSoksuuURS9NfHM888010fjhw5oq9+9aumqYv+1hN/npfBZB/u77GGbOPtwgsvdH+aPhBXXXWVtm/f7n5jv2XLFh08eFBnnXWW1/ExMTFKSkrSrFmz9K9//cu9fsOGDbrzzjt1zjnnBJyrvr5e6enp7rEWy4mn6Y9//KMkacaMGXr77bfV1dUlSXI4HJo9e7YOHTqko0eP9so8btw4r9OPAz2GgRxHzzH0sFgsSk1NVV1dnZ544gmPYxg1apSkE/8pBiujP4+zdGKGzckZezL961//6vU4+7OPwR6Ht/2sW7fO4/7GxkZJJ6buSp6PtSRt3rzZ4/XSUyR7xgcrq7+Pubesvl7b4eBvzZCkc88912O5pqZGc+bM0Wc+85leY8Px++ZvzYiWunfhhRfqtdde05QpU9zr+nptHDp0SF1dXbr00ks9fk6ka50/dSTUv2f/8z//4162WCyaNm2apBO/975eD+Gsac8995zHup7H6tixY17rxOWXX+7xWuj5Q/rYsWNe68RwrWPRxt+6apb3MZLv5/mJJ57w+Dknv7aDlT/Yr9MnnnjC43XaV2Z/Mgz2OAeyn8G8l/PnuRrMcQzksfroo488nq+eWXoNDQ2SvL/evvOd77hPt+rs7NRPfvITXXDBBZo1a1bQsg8kfzhq4lD4u67HqY/Ptddeq9bWVvcMoDfeeKPPx8fMdfKMM87wOLZoro9jxozxeM32fI2MGeqiv/XEn+dlMNmH+3ssrmr6f6677jp99NFHuvDCC3X22Wfrrbfe0lVXXaWrr77a5zZr165VW1ubZs+erVdeeUV//OMftWjRIj3zzDO6+OKLA8rR1tam9vZ2JScn97qv5/ts/uM//kM/+9nPdNttt2n06NEqLi5WSkqKDMPQ6NGj3eN6vPfeezp06FDEjyE5OdndJDr5GJqbm5WYmOjzsY5Exh07dmjcuHG65557ej3OgQrVcXz00Uf6zGc+49GE8PVYjx49WhdeeKEOHjzYq2kRjqz+vC6C8ZiH2/r167V69Wqf90dbzUhJSYmquvef//mfHut9vTbi4+NltVp7TYGPtlrXXx0JZ6bu7m6NGDFCl156qU477bSg/J4FM3/PYzVlypQB1Ymvfe1reuSRR/T888/7zE8dM58zzjjDlO9j+nqee17bvv7Pjcb8/WUORDTW4FMN5LjDdRw7duxQfHy82tvbJfX9fC1evFjV1dWaNm2atmzZov/4j/+IaHYp8jUx2v+u8/b43H///ZJONA37enyGUp00U3187733TFsX/aknPeOjpRZKka8nQRPGCzlExP/3//1/hiTj73//e5/jHnjgAWPixInG22+/bRiGYfzlL38x7r77bqO7u7vP7bq6uoxvfOMbxrRp04zRo0cbf/jDHwaV65///KchyXj88cd7jf3sZz/b78+12+3GpEmTjH/+85+GYRjG7373O+O0004zPvOZzwT9GHwdh7djMAzDWLRokXHeeed5rOvo6DDOP//8fvcZjse5J2NKSsqAMvW3j2Aeh6/97N2715Bk3HLLLb2O49TH2jAG/ngPJmsgj7m3rJHiz/N5st27dxsTJkwwjh8/3ue4cPy+DbRmRFvdW7lypcf9fb02or3WDbSOhOP3rKOjw0hMTDTOOuusQf3sYOY/eT9/+9vf3I/VQOoEdcx8/HlNRfvvtj/PczS+x+kvvz+/X31l8CYaa3CPaKorPVlmz5494NdbV1eXceeddxqf/exnjf379wc9u6/84a6JQ+HvOsPw/vgMtzpppvpo5rrobz2JplrYY6i8xxqyp5r6wzAM2e12ffe739V5550nSZo+fbr+93//130utC+xsbG67bbbtHv3bp133nmaN2/eoLL0TAV1uVy97ouLi+t3+9WrV+u///u/VVhYqMzMTO3cuVOLFi3yeupbj3Adg8vl8viCW0n67ne/q29+85u66qqr+vyZ4cz4wQcfDChTIIJ9HHfeeaek3qfpenuspYE/3qHI6s/rwozWr1+v4uJi92mevkRLzYjGutfZ2emxvq/XRrTXuoHWkXBk+u53v6tJkybpzDPPHNTP9iYY+ZcvX+5+rAZSJ6hjQ1u0/2778zxH43uc/vL78/vlr2iswT2iqa70ZJkwYcKAX2+xsbEqLy+XYRj6+c9/HrHsPSJZE6Px/U2wH5+hUCfNVB/NXBf9rSfRVAt7DJX3WDTedOLqGP/+9797nY987rnn6g9/+EOf2+7du1df//rXVV5eLqvVqsLCwkFdxWTs2LFKTEyU0+nsdd9nP/vZfrePjY3Vrbfequ3bt2v79u366U9/qvb2dn3+85+P+DE4nU6PqaB2u10jRozQmjVr+v2Z4cq4bds2jRkzZkCZAhHM47Db7Ro9erTGjBnT72PdM36gj3ews0oDf12Y0fHjx1VdXT2gL1ONlpoRjXXvgw8+8Fjf12sjmmudP3Uk1Jl6fu/Hjx8fkt+zYOSPjY11P1b91Qnq2NAXzb/b/jzP0fgep7/8/v5++Ssaa7AUXe+PTs7S3/N16vcvWSwWnX/++XrzzTcjkv1kkayJ0fj+JtiPj9nrpJnqo5nroj/1RIquWniyofIei8abpKSkJFmtVrW2tnqsb21t7XPGyL59+zRr1ixdd911WrlypZ577jn97W9/03e/+91B5cnJyZHD4XAvG4YhSQM6p/yNN97w+OPVMAxt375d3/zmNyN+DE1NTcrNzZV04ool7777rn7zm98oJiZGf/7zn/XnP/85ohnXrl2r/fv369Zbb+03UyCCeRwnP36XX365tm7d6s566mN96viBHFukXhdmtXXrVp133nn9fmFoNNWMaKx7J18dqb/XRrTWOn/qSKgz9fze//rXv9bOnTs1efLkqKppNTU1kqSKigqPx8pXnTAMgzo2DETr77Y/z3M0vsfpL7+/7xP8FY01WIqu90cbN250Z5Gk1157TZMnT/a5zZe+9KVe61pbWzVu3LiwZ4+mmhiN72+C/fiYuU6aqT6auS76W0+iqRZGUz0JqvCd1RoZvs4hXrlypZGZmeleXrhwoWGz2YwPPvjAMAzD+POf/2ycdtppxi9+8QuvP7e7u9v44he/aJSWlnqsf++994xzzjnH+OlPfxpQLsMwjNdee81ISEgw3nrrLcMwDOP73/++IcnYtWtXn8dgGIaxePFiY+XKle7lX/7yl8aMGTOMrq6uoB9DX8dx6jFs3LjRGD9+vHH48GHj/vvvN6ZNm2a8+uqrRmNjo9HY2GisXLnSeOSRR4KecaCP8/3332+MHz/eSEpKMrZt29Yrk7fHeiD7CNZxnLyf1atXezx+69evN0aOHGlUVFQYhmEYBQUFxsiRI43Dhw8bhmH49XgHI6s/r+2TXxfRYqA142TXXHON8dvf/rbPnxvO37eB1oxoq3txcXHu+09+bZil1vVXR4KZqb/H0mq1Gueff77x6quvGqtWrTLOOOMMw263R01Nu//++41zzjnHkGQ888wzHo/Va6+9ZowcOdJIS0szDONEnTj99NONqVOnUsdMyp+6Go2/24Yx8Oc5Gt/j9Je/v8yRrhd97Wcw7+Wi6f3RsmXLDIvFYjz66KNGY2OjsWrVKiMhIcG4//77vWY3DMOIiYkxNm3a5F7euHGjYbFYjO3btwc9e1/5w1kTh8Lfdb4en+FQJ81UH81cF/2tJ9FUC4fye6wh23hzuVxGVlaWMX36dEOScfHFFxvf/OY33fcvW7bMSE1NdS9//PHHxu2332588YtfNC699FLjoosuMn72s5/1+SWcO3bs8Frg3n77baO1tTWgXD2eeuop44tf/KIxZswYY/To0QM6BsMwjA0bNhhTpkwx0tPTjZkzZxrf/e53jQ8//DCoxzDQ43jqqaeM1NRUY+bMmcZll11m/PWvfzUOHz5sWCwWQ1Kvm69f7lA/zl/4whe85jk5k7fHeqD7GMxxeNuPr9ukSZOMmTNnGhMnTjQuuOACwzCMgB7vQLP685if+rqIBv7WjB4ffvihMXbsWOPIkSP97iPUv2/+1oxoqXs99/c0YhISEoykpCT3a8MMtW4gdSQYmQaSZyC/95GsaYcOHer3sSooKDDi4+ONmTNnGl/+8pepYyYVSF2Ntt9tf57naHyP01/+aK8XwazBJx9HNL0/CvQ5+NWvfmVccsklxsyZM41LLrnE+PKXv+zRiAtG9oHkN4zQ18Sh8Hedr8dnuNRJM9VHM9dFf7NHUy3sMVTfY8UYxv+dkwQAAAAAAAAgaPiONwAAAAAAACAEaLwBAAAAAAAAIUDjDQAAAAAAAAgBGm8AAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNAAAAAAAACAEabwAAAAAAAEAI0HgDAAAAAAAAQoDGGwAAAAAAABACNN4AAAAAAACAEKDxBgAAAAAAAIQAjTcAAAAAAAAgBGi8AQAAAAAAACFA4w0AAAAAAAAIARpvAAAAAAAAQAjQeAMAAAAAAABCgMYbAAAAAAAAEAI03gAAAAAAAIAQoPEGAAAAAAAAhACNNwAAAAAAACAEaLwBAAAAAAAAIUDjDQAAAAAAAAiBEZEOMBCjR49WR0eHYmNjdeaZZ0Y6DoAQev/993X8+HGNGjVKH3/8caTjDAq1Cxhehkr9onYBw8tQqV0S9QsYTsxUu2IMwzAiHaI/sbGx6u7ujnQMAGFksVh0/PjxSMcYFGoXMDyZvX5Ru4Dhyey1S6J+AcORGWqXKWa89RRQi8Wis88+O9JxAIRQa2ururu7FRsbG+kog0btAoaXoVK/qF3A8DJUapdE/QKGEzPVLlM03s4880zt27dPZ599tt57771IxwEQQhMmTNC+ffuGxOkB1C5geBkq9YvaBQwvQ6V2SdQvYDgxU+3i4goAAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEYNmpra5WWlqbMzExlZWVp9+7dfY4/dOiQbrjhBmVnZystLU0XXnihHnvssTClBQAAAACYHY03AMNCQ0ODiouLVV1dre3bt6u0tFR5eXk6cuSI1/HHjh1Tbm6uLrvsMm3btk0Oh0Nz5sxRY2NjmJMDAAAAAMyKxhuAYaGyslL5+fmy2WySpKKiInV1damqqsrr+IceekijRo1ScXGxe92yZctUWloalrwAAAAAAPMbEekAoVBeHukEgTFrbsAM6uvrtWLFCveyxWJRamqq6urqtGTJkl7jn3zySWVlZXmsS0pKUlJSUsgymrUGmDU3gOAxax0wa24AwWHWGmDW3MBwxYw3AENeW1ub2tvblZyc7LE+OTlZLS0tXrfZtWuX4uLitGjRIl166aX6yle+ogceeECGYfjcj8vl0uHDh923vsYCAAAAAIa+ITnjDQBOdvToUUmS1Wr1WG+1Wt33nerDDz9URUWFnn76ad1///3au3evMjMz1d7ermXLlnndpqKiQqtWrQpueAAAAACAaTHjDcCQFx8fL+nEjLSTuVwu932nslgsysjI0Jw5cyRJ559/vq6//nrdfffdPvdTVlam9vZ2923cuHFBOgIAAAAAgBkx4w3AkDd27FglJibK6XR6rHc6nUpJSfG6zcSJEzVhwgSPdZMmTdKBAwf0ySefKC4urtc2VqvVY1ZdTExMENIDAAAAAMyKGW8AhoWcnBw5HA73smEYampqUm5urtfxmZmZam1t9Vh34MABJSUleW26AQAAAABwKhpvAIYFu92uzZs3q7m5WZJUXV2t2NhYLViwQJJUUlKi+fPnu8fffPPNamhoUGNjoyTpgw8+0IYNG7R06dLwhwcAAAAAmBKnmgIYFjIyMlRVVaXCwkLFxcXJYrFoy5YtSkhIkCR1dHSos7PTPf6iiy5SbW2tFi9erNNOO01dXV1auHChbr311kgdAgAAAADAZGi8ARg2CgoKVFBQ4PW+mpqaXuvy8vKUl5cX6lgAAAAAgCGKU00BAAAAAACAEKDxBgAAAAAAAIRAQI232tpapaWlKTMzU1lZWdq9e3ef4w8dOqQbbrhB2dnZSktL04UXXqjHHnssoMAAAAAAAACAGfjdeGtoaFBxcbGqq6u1fft2lZaWKi8vT0eOHPE6/tixY8rNzdVll12mbdu2yeFwaM6cOe4rBQIAAAAAAABDkd+Nt8rKSuXn58tms0mSioqK1NXVpaqqKq/jH3roIY0aNUrFxcXudcuWLVNpaWmAkQEAAAAAAIDo53fjrb6+Xunp6Z/+AItFqampqqur8zr+ySefVFZWlse6pKQkTZ061d9dAwAAAAAAAKbhV+Otra1N7e3tSk5O9lifnJyslpYWr9vs2rVLcXFxWrRokS699FJ95Stf0QMPPCDDMHzux+Vy6fDhw+5bX2MBAAAAAACAaDTCn8FHjx6VJFmtVo/1VqvVfd+pPvzwQ1VUVOjpp5/W/fffr7179yozM1Pt7e1atmyZ120qKiq0atUqf6IBAAAAAAAAUcWvGW/x8fGSTsxIO5nL5XLf12sHFosyMjI0Z84cSdL555+v66+/XnfffbfP/ZSVlam9vd19GzdunD8xAQAAAAAAgIjza8bb2LFjlZiYKKfT6bHe6XQqJSXF6zYTJ07UhAkTPNZNmjRJBw4c0CeffKK4uLhe21itVo9ZdTExMf7EBAAAAAAAACLO74sr5OTkyOFwuJcNw1BTU5Nyc3O9js/MzFRra6vHugMHDigpKclr0w0AAAAAAAAYCvxuvNntdm3evFnNzc2SpOrqasXGxmrBggWSpJKSEs2fP989/uabb1ZDQ4MaGxslSR988IE2bNigpUuXBiM/AAAAAAAAEJX8OtVUkjIyMlRVVaXCwkLFxcXJYrFoy5YtSkhIkCR1dHSos7PTPf6iiy5SbW2tFi9erNNOO01dXV1auHChbr311uAdBQAAAAAAABBl/G68SVJBQYEKCgq83ldTU9NrXV5envLy8gLZFQAAAAAAAGBKfp9qCgAAAAAAAKB/NN4AAAAAAACAEKDxBgAAAAAAAIQAjTcAAIAoVltbq7S0NGVmZiorK0u7d+/2OdYwDK1evVrTp09XVlaW0tLS9Jvf/CaMaQEAAHCygC6uAAAAgNBraGhQcXGxHA6HbDabNmzYoLy8PO3Zs8d9RfmT/fa3v9VPfvITvfnmmxo/frz+9a9/6cILL9T48eM1d+7cCBwBAADA8MaMNwAAgChVWVmp/Px82Ww2SVJRUZG6urpUVVXldfxf/vIXTZkyRePHj5ckTZw4UTabTVu3bg1bZgAAAHyKxhsAAECUqq+vV3p6unvZYrEoNTVVdXV1XsdfeeWV2rNnj3bt2iVJev311/XXv/5VZ511VljyAgAAwBOnmgIAAEShtrY2tbe3Kzk52WN9cnKyGhsbvW6Tm5urRx55RDk5OTrjjDP01ltvKTMzU9/73vd87sflcsnlcrmXDcMIzgEAAACAGW8AAADR6OjRo5Ikq9Xqsd5qtbrvO9WmTZu0cOFCPf/883rzzTe1d+9effWrX1V8fLzP/VRUVCgxMdF9279/f/AOAgAAYJij8QYAABCFepplJ89G61n21Uhbvny5vvGNbyg1NVWSlJKSor1792rJkiU+91NWVqb29nb3bdy4cUE6AgDDHVdlBgBONQUAAIhKY8eOVWJiopxOp8d6p9OplJQUr9vs3btX11xzjce6c889Vz//+c99/gFrtVo9ZtXFxMQMMjkAcFVmAOjBjDcAAIAolZOTI4fD4V42DENNTU3Kzc31On78+PFqbW31WNfa2qq4uLiQ5gSAU3FVZgA4gcYbAABAlLLb7dq8ebOam5slSdXV1YqNjdWCBQskSSUlJZo/f757/PXXX6/HHntM//znPyVJ//jHP/Too4/2mgUHAKHGVZkB4ARONQUwbNTW1mrNmjWKi4uTxWLRunXrNG3aNK9j169fr7Vr1/a6muBzzz3HzBEAYZORkaGqqioVFha6a9eWLVvcp2l1dHSos7PTPf72229XTEyMvv71rys+Pl6HDx/WokWLtGLFikgdAoBhiKsyA8CnaLwBGBb8/Z4R6cRMk+uuuy68QQHgFAUFBSooKPB6X01NjcfyiBEjZLfbZbfbwxENALwazFWZt27dqtTUVLW0tOjxxx/v96rMq1atCl5wAAgBTjUFMCz4+z0jAAAACAxXZQaAT9F4AzAs+Ps9IwAAAAhMoFdlPuecczzWnXvuufrDH/7gcz9Wq1Vjxoxx37gqM4BoROMNwJDX1/eMtLS0+Nxu06ZNysnJ0cyZM3XNNddo586dfe7H5XLp8OHD7hvfMwIAAIYrrsoMACfQeAMw5AXyPSNnnXWWzj//fD333HN65ZVXNGfOHF188cV9Nt8qKiqUmJjovu3fvz94BwEAAGAiXJUZAE7g4goAhrxAvmdkzpw5mjNnjnu5pKRE69at009/+lNVV1d73aasrEy33HKLe3nq1Kk03wAAwLDEVZkB4AQabwCGvEC+Z8Sb8847T++8847P+61Wq8esOr5nBAAADGdclRkAONUUwDDh7/eMlJWV9ToNdd++fZo4cWJIcwIAAAAAhg4abwCGBX+/Z2THjh16+OGH3csvvPCCXn31Vd14443hDQ4AAAAAMC1ONQUwLPj7PSPLli3TvffeqyeeeELHjx9Xd3e3nnrqKV1++eWROgQAAAAAgMnQeAMwbPjzPSOnXlwBAAAAAAB/BdR4q62t1Zo1a9yzRtatW6dp06Z5Hbt+/XqtXbtWycnJHuufe+45xcXFBbJ7AAAARJHsbeWRjhCg8kgHAAAAQ5zfjbeGhgYVFxfL4XDIZrNpw4YNysvL0549e9ynbJ3KbrfruuuuG2xWAAAAAAAAwDT8vrhCZWWl8vPzZbPZJElFRUXq6upSVVVV0MMBAAAAAAAAZuV3462+vl7p6emf/gCLRampqaqrqwtqMAAAAAAAAMDM/Gq8tbW1qb29vdf3tSUnJ6ulpcXndps2bVJOTo5mzpypa665Rjt37uxzPy6XS4cPH3bfDMPwJyYAAAAAAAAQcX413o4ePSpJslqtHuutVqv7vlOdddZZOv/88/Xcc8/plVde0Zw5c3TxxRf32XyrqKhQYmKi+7Z//35/YgIAAAAAAAAR51fjLT4+XtKJGWknc7lc7vtONWfOHFVUVLibdSUlJZo+fbp++tOf+txPWVmZ2tvb3bdx48b5ExMAAAAAAACIOL+uajp27FglJibK6XR6rHc6nUpJSRnwzznvvPP0zjvv+LzfarV6zKqLiYnxJyYAmFL2tvJIRwhQeaQDAAAAAEBU8vviCjk5OXI4HO5lwzDU1NSk3Nxcr+PLysp6nYa6b98+TZw40d9dAwAAAAAAAKbhd+PNbrdr8+bNam5uliRVV1crNjZWCxYskHTiVNL58+e7x+/YsUMPP/ywe/mFF17Qq6++qhtvvHGw2QEAAAAAAICo5depppKUkZGhqqoqFRYWKi4uThaLRVu2bFFCQoIkqaOjQ52dne7xy5Yt07333qsnnnhCx48fV3d3t5566ildfvnlwTsKAAAAAAAAIMr43XiTpIKCAhUUFHi9r6amxmN5zpw5mjNnTiC7AQAAAAAAAEzL71NNAQAAAAAAAPSPxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNAAAAAAAACAEabwAAAAAAAEAI0HgDAAAAAAAAQmBEpAMAAAAAABBu2dvKIx0hQOWRDgDAD8x4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNwLBRW1urtLQ0ZWZmKisrS7t37x7Qdps2bVJMTIzWr18f2oAAAAAAgCFlRKQDAEA4NDQ0qLi4WA6HQzabTRs2bFBeXp727NmjhIQEn9t9/PHHWrFiRRiTAgAAAACGCma8ARgWKisrlZ+fL5vNJkkqKipSV1eXqqqq+tzurrvu0qJFi8IREQC88ne27qFDh3TDDTcoOztbaWlpuvDCC/XYY4+FKS0AAABORuMNwLBQX1+v9PR097LFYlFqaqrq6up8brNz5041NDRo4cKF4YgIAL30zNatrq7W9u3bVVpaqry8PB05csTr+GPHjik3N1eXXXaZtm3bJofDoTlz5qixsTHMyQEAACDReAMwDLS1tam9vV3Jycke65OTk9XS0uJ1m+7ubi1evFj33XefYmJiBrQfl8ulw4cPu2+GYQw6O4Dhzd/Zug899JBGjRql4uJi97ply5aptLQ0LHkBAADgaUh+x1v2tvJIRwhQeaQDAEPS0aNHJUlWq9VjvdVqdd93qnvvvVczZ87URRddNOD9VFRUaNWqVYEHBYBT1NfXe3zP5MmzdZcsWdJr/JNPPqmsrCyPdUlJSUpKSgp5VgAAAPTGjDcAQ158fLykEzPSTuZyudz3nWzfvn166KGHtHLlSr/2U1ZWpvb2dvdt3LhxgYcGMOwFMlt3165diouL06JFi3TppZfqK1/5ih544IE+Z+AyWxcAACB0aLwBGPLGjh2rxMREOZ1Oj/VOp1MpKSm9xm/dulWSNHfuXGVnZys7O1uStHbtWmVnZ+uVV17xuh+r1aoxY8a4bwM9RRUAvAlktu6HH36oiooKXXHFFfrjH/+o3/zmNyovL9ePf/xjn/upqKhQYmKi+7Z///7gHQSAYY2LwwAAjTcAw0ROTo4cDod72TAMNTU1KTc3t9fYkpISvfHGG9q2bZv7Jkl2u13btm3TzJkzwxUbwDDm72xd6cSpqBkZGZozZ44k6fzzz9f111+vu+++2+d+mK0LIBS4OAwAnEDjDcCwYLfbtXnzZjU3N0uSqqurFRsbqwULFkg60WybP39+JCMCgAd/Z+tK0sSJEzVhwgSPdZMmTdKBAwf0ySefeN2G2boAQoGLwwDACQE13vydMtxj06ZNiomJ0fr16wPZLQAELCMjQ1VVVSosLFRmZqYefPBBbdmyRQkJCZKkjo4Or3+U9pxeeuq/ASAc/JmtK0mZmZlqbW31WHfgwAElJSUpLi4upFkB4GT19fVKT093L598cRhvfF0cZurUqSHNCQCh5vdVTXumDDscDtlsNm3YsEF5eXnas2eP+w9Ybz7++GOPq3IBQLgVFBSooKDA6301NTVe19vtdtnt9lDGAgCf7Ha7cnNz1dzcrMmTJ3udrdvV1aWNGzdKkm6++WbNmDFDjY2NSk9P1wcffKANGzZo6dKlkTwMAMNMXxeH8XXq6K5du5SZmalFixbpjTfe0MiRI/Wtb31L3/3ud33OxHW5XB6n43NxGADRyO8Zb/5OGe5x1113adGiRYGlBAAAGIb8na170UUXqba2VosXL9all16qOXPmaOHChfr+978fqUMAMAxxcRgA+JTfjTd/pwxL0s6dO9XQ0KCFCxcGlhIAAGCYKigokMPh0Pbt2/XSSy9p2rRp7vtqamr0hz/8wWN8Xl6eGhoa9Mc//lGvvfaa7rjjDsXGxoY7NoBhjIvDAMCn/DrVNJApw93d3Vq8eLEeeOCBAX9ZL1OGAQAAAMCcQnFxGG/fU2m1Wj1m1XFxGADRyK8Zb4FMGb733ns1c+ZMXXTRRQPeD1OGAQAAAMC8uDgMAJzgV+PN3ynD+/bt00MPPaSVK1f6FYopwwAAAABgXna7XZs3b1Zzc7Mkeb04zPz5893jb775ZjU0NLjPpOLiMACGCr9ONfV3yvDWrVslSXPnzvVYv3btWq1fv16rV6/WzJkze23HlGEAAAAAMK+TLw4TFxcni8XS6+IwnZ2d7vEnXxzmtNNOU1dXlxYuXKhbb701UocAAEHhV+NN8j1lePny5b3GlpSUqKSkxGNdTEyM7Ha7rrvuOv/TAgAAAABMoaCgQAUFBV7vq6mp6bUuLy9PeXl5oY4FAGHl91VN/Z0yDAAAAAAAAAxHfs9483fKcI+1a9fq+eefd/97/fr12rZt2+DSAwAAAAAAAFHK78ab5P+UYenETDm73R7I7gAAAAAAAADT8ftUUwAAAAAAAAD9o/EGAAAAAAAAhACNNwAAAAAAACAEaLwBAAAAAAAAIUDjDQAAAAAAAAgBGm8AAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNAAAAAAAACAEabwAAAAAAAEAI0HgDAAAAAAAAQoDGGwAAAAAAABACNN4AAAAAAACAEKDxBmDYqK2tVVpamjIzM5WVlaXdu3f7HPvyyy/r6quvVk5Oji677DJNnz5d9913XxjTAgAAAADMbkSkAwBAODQ0NKi4uFgOh0M2m00bNmxQXl6e9uzZo4SEhF7jf//73+vzn/+87rrrLknS66+/ri996Us655xzNHfu3HDHBwAAAACYEDPeAAwLlZWVys/Pl81mkyQVFRWpq6tLVVVVXscvXbpUN998s3t5+vTpOv3009Xc3ByWvAAAAAAA86PxBmBYqK+vV3p6unvZYrEoNTVVdXV1XsdfcMEF7plw3d3devDBB2W1WnX11VeHJS8AAAAAwPw41RTAkNfW1qb29nYlJyd7rE9OTlZjY2Of265evVr33HOPkpKStHXrVk2YMMHnWJfLJZfL5V42DGNwwQEAAAAApsaMNwBD3tGjRyVJVqvVY73VanXf58uKFSvkdDp10003KSsrS7t27fI5tqKiQomJie7b/v37Bx8eAAAAAGBaNN4ADHnx8fGS5DEbrWe5576+xMTE6Dvf+Y6mTp2qH/zgBz7HlZWVqb293X0bN27c4IIDAAAAAEyNU00BDHljx45VYmKinE6nx3qn06mUlBSv2xw7dkwjR470WGez2fSnP/3J536sVqvHrLqYmJhBpAYAAAAAmF1AM95qa2uVlpamzMxMZWVlaffu3T7Hvvzyy7r66quVk5Ojyy67TNOnT9d9990XcGAACEROTo4cDod72TAMNTU1KTc31+v41NTUXutaW1uZxQYAAAAAGDC/G28NDQ0qLi5WdXW1tm/frtLSUuXl5enIkSNex//+97/X5z//eb344ot6+eWXtWHDBi1dulTPPvvsoMMDwEDZ7XZt3rxZzc3NkqTq6mrFxsZqwYIFkqSSkhLNnz/fPf7IkSNat26de/mll17S1q1bdf3114c3OAAAAADAtPw+1bSyslL5+fmy2WySpKKiIt1xxx2qqqrSkiVLeo1funSpJk6c6F6ePn26Tj/9dDU3N2vu3LmDiA4AA5eRkaGqqioVFhYqLi5OFotFW7ZsUUJCgiSpo6NDnZ2d7vE/+tGP9OCDD+p3v/udLBaLXC6XHn74YV177bWROgQAAAAAgMn43Xirr6/XihUr3MsWi0Wpqamqq6vz2ni74IIL3P/u7u7Www8/LKvVqquvvjrAyAAQmIKCAhUUFHi9r6amxmO5sLBQhYWF4YgFAAAAABii/Gq8tbW1qb29XcnJyR7rk5OT1djY2Oe2q1ev1j333KOkpCRt3bpVEyZM8DnW5XJ5XH3QMAx/YgIAAAAAAAAR59d3vB09elSSPK7a17Pcc58vK1askNPp1E033aSsrCzt2rXL59iKigolJia6b/v37/cnJgAAAAAAABBxfjXe4uPjJcljNlrPcs99fYmJidF3vvMdTZ06VT/4wQ98jisrK1N7e7v7xlUEAQAAAAAAYDZ+Nd7Gjh2rxMREOZ1Oj/VOp1MpKSletzl27FivdTabTW+++abP/VitVo0ZM8Z9i4mJ8ScmAADAkFFbW6u0tDRlZmYqKytLu3fvHtB2mzZtUkxMjNavXx/agAAAAPDJr8abJOXk5MjhcLiXDcNQU1OTcnNzvY5PTU3tta61tZVZbAAAAP1oaGhQcXGxqqurtX37dpWWliovL09Hjhzpc7uPP/7Y42JYAAAAiAy/G292u12bN29Wc3OzJKm6ulqxsbFasGCBJKmkpETz5893jz9y5IjWrVvnXn7ppZe0detWXX/99YPNDgAAMKRVVlYqPz9fNptNklRUVKSuri5VVVX1ud1dd92lRYsWhSMiAAAA+uB34y0jI0NVVVUqLCxUZmamHnzwQW3ZskUJCQmSpI6ODn3yySfu8T/60Y/0xBNP6Mtf/rJmzpyp2267TQ8//LCuvfba4B0FAADAEFRfX6/09HT3ssViUWpqqurq6nxus3PnTjU0NGjhwoXhiAgAPnGqPABIIwLZqKCgQAUFBV7vq6mp8VguLCxUYWFhILsBAAAYttra2tTe3q7k5GSP9cnJyWpsbPS6TXd3txYvXqwHHnhgwN+R63K5PC6cZRhG4KEB4P/0nCrvcDhks9m0YcMG5eXlac+ePe5JG95wqjyAocbvGW8AAAAIvaNHj0o6cdGpk1mtVvd9p7r33ns1c+ZMXXTRRQPeT0VFhRITE923/fv3Bx4aAP4Pp8oDwAk03gAAAKJQfHy8JHnMRutZ7rnvZPv27dNDDz2klStX+rWfsrIytbe3u29cAAtAMHCqPACcENCppgAAAAitsWPHKjExUU6n02O90+lUSkpKr/Fbt26VJM2dO9dj/dq1a7V+/XqtXr1aM2fO7LWd1Wr1mFU30FNUAcAXTpUHgE/ReAMAAIhSOTk5cjgc7mXDMNTU1KTly5f3GltSUqKSkhKPdTExMbLb7bruuutCHRUA3MJ5qvyqVasCDwoAYcCppgAAAFHKbrdr8+bNam5uliRVV1crNjZWCxYskHSi2TZ//vxIRgSAXjhVHgA+xYw3AACAKJWRkaGqqioVFhYqLi5OFotFW7ZscV8RsKOjQ52dnb22W7t2rZ5//nn3v9evX69t27aFMzqAYYxT5QHgUzTeAAAAolhBQYEKCgq83ldTU+N1vd1ul91uD2UsAOgTp8oDwAmcagoAAAAACCpOlQeAE5jxBgAAAAAIKk6VB4ATaLwBAAAAAIKOU+UBgFNNAQAAAAAAgJCg8QYAAAAAAACEAI03AAAAAAAAIARovAEAAAAAAAAhQOMNAAAAAAAACAEabwAAAAAAAEAIjIh0AACAuZWXRzpBYMyaGwAAAIB5MOMNAAAAAAAACAEabwAAAAAAAEAI0HgDAAAAAAAAQoDGG4Bho7a2VmlpacrMzFRWVpZ2797tc2xdXZ2uuOIK5eTk6JJLLtHs2bO1c+fOMKYFAAAAAJgdjTcAw0JDQ4OKi4tVXV2t7du3q7S0VHl5eTpy5IjX8TfeeKO+9rWv6cUXX9SOHTs0Y8YMzZo1S++//36YkwMAAAAAzIrGG4BhobKyUvn5+bLZbJKkoqIidXV1qaqqyuv4tLQ0lZaWupeXLl2qtrY21dXVhSUvAAAAAMD8aLwBGBbq6+uVnp7uXrZYLEpNTfXZSHv00UdlsXxaIkeNGiVJOnbsWGiDAgAAAACGjBGRDgAAodbW1qb29nYlJyd7rE9OTlZjY+OAfsaOHTsUFxenefPm+Rzjcrnkcrncy4ZhBBYYAAAAADAkBDTjjS8oB2AmR48elSRZrVaP9Var1X1fXwzD0OrVq/XDH/5QSUlJPsdVVFQoMTHRfdu/f//gggMAAAAATM3vxhtfUA7AbOLj4yXJYzZaz3LPfX0pLy/X+PHjdeutt/Y5rqysTO3t7e7buHHjAg8NAAAAADA9vxtvfEE5ALMZO3asEhMT5XQ6PdY7nU6lpKT0ue2vf/1rNTY2av369f3ux2q1asyYMe5bTEzMYGIDAAAAAEzO78YbX1AOwIxycnLkcDjcy4ZhqKmpSbm5uT63qamp0WOPPaYnn3xSI0eOVEtLCx8aAAAAAAAGzK+LK/AF5QDMym63Kzc3V83NzZo8ebKqq6sVGxurBQsWSJJKSkrU1dWljRs3SpI2bdoku92u9evXu7/H8s9//rNaW1v7bNYBAAAAANDDr8ZbOL+gfNWqVf5EA4A+ZWRkqKqqSoWFhYqLi5PFYtGWLVuUkJAgSero6FBnZ6d7fElJiQ4dOqScnByPn7Ny5cqw5gYAAAAAmJdfjbdwfkH5Lbfc4l6eOnUqVwcEMGgFBQUqKCjwel9NTY3H8sGDB8MRaUjI3lYe6QgBKo90AAAAAABDnF+Nt2B8QfnTTz/d736sVqvHrDq+oBwAAAAAAABm41fjTfL9BeXLly/3uU3PF5Q/++yz7i8ob2lp4XuSTlFeHukEgTFrbgAAAAAAgFDyu/HGF5QDAAAAAAAA/fO78cYXlAMAAAAAAAD987vxJvEF5QAAAAAAAEB/LJEOAAAAAAAAAAxFAc14AxBBZr2ahVlzAwAAAAAQIGa8AQAAAAAAACFA4w0AAAAAAAAIARpvAAAAAAAAQAjQeAMAAAAAAABCgIsrAACGJ7Ne8MOsuQEAAIBhiBlvAAAAAAAAQAjQeAMAAAAAAABCgMYbAAAAAAAAEAI03gAAAKJYbW2t0tLSlJmZqaysLO3evdvn2Lq6Ol1xxRXKycnRJZdcotmzZ2vnzp1hTAsAAICT0XgDAACIUg0NDSouLlZ1dbW2b9+u0tJS5eXl6ciRI17H33jjjfra176mF198UTt27NCMGTM0a9Ysvf/++2FODgB8cAAAEo03AACAqFVZWan8/HzZbDZJUlFRkbq6ulRVVeV1fFpamkpLS93LS5cuVVtbm+rq6sKSFwB68MEBAJxA4w0AACBK1dfXKz093b1ssViUmprqs5H26KOPymL59O3dqFGjJEnHjh0LbVAAOAUfHADACTTeAAAAolBbW5va29uVnJzssT45OVktLS0D+hk7duxQXFyc5s2b53OMy+XS4cOH3TfDMAaVGwAkPjgAgB4jIh0An8reVh7pCAEqj3QAAACGnKNHj0qSrFarx3qr1eq+ry+GYWj16tX64Q9/qKSkJJ/jKioqtGrVqsGFBYCT9PXBQWNj44B+xkA/OHC5XO5lPjgAEI1ovAEAhqVt2yKdIDDZkQ6AsImPj5ckjz8qe5Z77utLeXm5xo8fr1tvvbXPcWVlZbrlllvcy1OnTtX+/fsDSAwAJ/DBAQB8isYbBq+8PNIJAmPW3ACAYWHs2LFKTEyU0+n0WO90OpWSktLntr/+9a/V2Niop59+ut/9WK1Wjz+OY2JiAsoLAD344AAAPsV3vAEAAESpnJwcORwO97JhGGpqalJubq7PbWpqavTYY4/pySef1MiRI9XS0sKXkwMIq2B8cLB+/fp+92O1WjVmzBj3jQ8OAEQjGm8AAABRym63a/PmzWpubpYkVVdXKzY2VgsWLJAklZSUaP78+e7xmzZtkt1u15133qndu3fL4XDohRde0CuvvBKR/ACGLz44AIATONUUAAAgSmVkZKiqqkqFhYWKi4uTxWLRli1blJCQIEnq6OhQZ2ene3xJSYkOHTqknJwcj5+zcuXKsOYGALvdrtzcXDU3N2vy5MlePzjo6urSxo0bJX36wcH69eu1e/duSdKf//xntba29tmsA4BoR+MNAAAzMev3U5o1dxQoKChQQUGB1/tqamo8lg8ePBiOSADQLz44AIATaLwBGDZqa2u1Zs0a95u/devWadq0aT7Hd3d36xe/+IWWL1+u5557TtnZ2eELCwAAYHJ8cAAANN4wnDH7YlhpaGhQcXGxHA6HbDabNmzYoLy8PO3Zs8f9yevJPvzwQ33zm9/Ueeedp46OjggkBgAAAACYHRdXADAsVFZWKj8/XzabTZJUVFSkrq4uVVVVeR3/8ccfq7KyUt///vfDGRMAAAAAMIQE1Hirra1VWlqaMjMzlZWV5f7yS1+6u7v185//XHFxcdq2bVsguwSAQamvr1d6erp72WKxKDU11eeVsiZMmKC0tLRwxQMAAAAADEF+n2rK6VoAzKatrU3t7e1KTk72WJ+cnKzGxsag7cflcsnlcrmXDcMI2s8GAAAAgAEz61crmTV3H/ye8cbpWgDM5ujRo5Ikq9Xqsd5qtbrvC4aKigolJia6b/v37w/azwYAAAAAmI/fjTdO1wJgNvHx8ZLkMRutZ7nnvmAoKytTe3u7+zZu3Lig/WwAAAAAgPn4daopp2vBG7N+bV92dqQTIFzGjh2rxMREOZ1Oj/VOp1MpKSlB24/VavWYVRcTExO0nw0AAAAAMB+/ZrxxuhYAs8rJyZHD4XAvG4ahpqYm5ebmRjAVAAAAAGAo86vxxulaAMzKbrdr8+bNam5uliRVV1crNjZWCxYskCSVlJRo/vz5kYwIAAAAABhi/DrVlNO1AJhVRkaGqqqqVFhYqLi4OFksFm3ZssV9NeaOjg51dnZ6bPONb3zDPeP2pptu0umnn676+nrFxsaGPT8AAAAAwHz8arxJvk/XWr58eVCDAUCwFRQUqKCgwOt9NTU1vdY99dRToY4EAAAAABjC/L6qKadrAQAAAAAAAP3ze8Ybp2sBAAC/lZdHOkFgzJobAAAAUcHvxpvE6VoAAAAAAABAfwJqvAEAAADAsGDGma9mzAwAQ5Tf3/EGAAAAAAAAoH803gAAAAAAAIAQoPEGAAAAAAAAhACNNwAAAAAAACAEaLwBAAAAAAAAIUDjDQAAAAAAAAgBGm8AAAAAAABACNB4AwAAAAAAAEKAxhsAAAAAAAAQAjTeAAAAAAAAgBCg8QYAAAAAAACEwIhIBwAiZdu2SCcITHZ2pBMAAAAAwDBRXh7pBDA5ZrwBAAAAAAAAIUDjDQAAAAAAAAgBTjUFAMBEOE0eCCKznj5k1twAAAxDzHgDAAAAAAAAQoAZbwAAAABCj5l6wPBGDcAwxYw3AAAAAAAAIASY8QYAAACYCbNGgOGNGgCYCjPeAAAAAAAAgBCg8QYAAAAAAACEAKeaAgAAYFjati3SCQKTnR3pBAAAYKBovAEAAAAAYBJ8aACYC6eaAgAAAAAAACEQ0Iy32tparVmzRnFxcbJYLFq3bp2mTZvmc/wrr7yi2267TVarVS6XSz/5yU+UmZkZcGhgODPtJ1yRDiBqFwBzonbhVKZ9L5Ad6QSBMePjnR3pAP+H+gUAATTeGhoaVFxcLIfDIZvNpg0bNigvL0979uxRQkJCr/H/+Mc/NHfuXD3zzDPKzs7WSy+9pHnz5umNN97QpEmTgnIQANAfahcAM6J2ATAr6hcAnOB3462yslL5+fmy2WySpKKiIt1xxx2qqqrSkiVLeo3/1a9+pSlTpij7/z7iysrKks1m0z333KOf/vSng0sPAANE7QIiy4wzRqTIzxqhdmEoMWsdQGCoX0BkmbXmZkc6QAj43Xirr6/XihUr3MsWi0Wpqamqq6vzWkDr6up6TQ9OT09XXV1dAHEBIDDULgBmRO0CYFbUL5zKtI2g7EgngNn51Xhra2tTe3u7kpOTPdYnJyersbHR6zYtLS26+uqre41vaWnxuR+XyyWXy+Vefv/99yVJra2tmjBhQr85XQcP9zsGQHhZJzw0oHGtra2SPv29DwZqF4BADbR2ScGvX2apXRL1C4g2kaxdknnqF7ULA7Ij0gGGl0j+3RgqfjXejh49KkmyWq0e661Wq/s+b9v4M16SKioqtGrVql7ru7u7tW/fPn8iA4gW+474Nfz48eNB2zW1C0DA/KxdUvDqF7ULQMAiWLsk6heAQYjg342h4lfjLT4+XpI8PlXoWe65z9s2/oyXpLKyMt1yyy3u5eTkZLlcLsXGxurMM8/0J3LUOnDggM4666xIxwiqoXRMZj8Ws+Y/cOCAYmJidPz4cY0aNSpoP3eo1C6zPq++DIXjMfMxmC27GfK+//77Qa1fQ6V2hYoZXhO+mC27mfJGe9ZozBfs2iVRv6LxefaHWfObLbcZ8kZzxvfff1+dnZ1BrV2h4lfjbezYsUpMTJTT6fRY73Q6lZKS4nWblJQUv8ZLJz7ZOPnTjr4+5TCrCy64QG+++WakYwTVUDomsx+LWfOHKvdQqV1mfV59GQrHY+ZjMFt2s+UNhqFSu0LFzK8Js2U3U95ozxrt+YJluNcvsz/PZs1vttxmyBvtGaM9Xw+Lvxvk5OTI4XC4lw3DUFNTk3Jzc72Ov/zyyz3GS5LD4fA5frhYvHhxpCME3VA6JrMfi1nzhzL3UKhdZn1efRkKx2PmYzBbdrPlDZahULtCxcyvCbNlN1PeaM8a7fmCaTjXL7M/z2bNb7bcZsgb7RmjPZ+b4afXXnvNSEhIMN566y3DMAxj48aNxvjx443Dhw8bhmEY1113nVFUVOQe/+677xpjxowxXnrpJcMwDOPll182EhISjHfffdffXQNAwKhdAMyI2gXArKhfAHCCX6eaSlJGRoaqqqpUWFiouLg4WSwWbdmyRQkJCZKkjo4OdXZ2usdPmjRJmzZt0u23366RI0fK5XLp2Wef1aRJk4LXPQSAflC7AJgRtQuAWVG/AOCEGMMwjEiHAAAAAAAAAIYav7/jDQAAAAAAAED//D7VFOby7LPP6vnnn9fo0aN1zjnn6MYbb4x0pAEzc/aBMvsxmjW/WXOHy1B6fMx0LGbKOlBmPCYzZkZomfk1Ea3ZozXXQJkhvxkyYvDM/jxHe/5ozzdQZjiOaM846HyR/pI5hM6RI0eMyZMnG52dnYZhGMaMGTOMd955J8KpBsbM2QfK7Mdo1vxmzR0uQ+nxMdOxmCnrQJnxmMyYGaFl5tdEtGaP1lwDZYb8ZsiIwTP78xzt+aM930CZ4TiiPWMw8nGqqaRjx46prKxMI0aM0Lvvvtvv+M7OTq1evVozZszQJZdcohkzZujll1+OSLba2lqlpaUpMzNTWVlZ2r17t/u+P/3pT/rc5z6nESNOTGxMT09XRUWFZs+ercsvv1zp6em66qqr1NLSEvbcA83+1FNPafbs2XrvvfeUk5Mz4Lz+PqeB6m8/t912m8aMGaPTTz9dCQkJmjVrljt/JJ+f/rI//vjjmj17ti666CKNHj1aSUlJysjI6PP1NXbsWF155ZWaNWuWZs6cqdTUVD3++ONhzy75/3uxdevWoGc4mcvl0s0336wvfOELysrK0sUXX6za2lq/9xmsXKc+Pn/5y1/c2zz99NP9Pj5mqoGjR49Wfn6+Lr/8crW0tERdzev5XcvMzNSBAwf0rW99Sy0tLf2+LiNZ43oyX3755frc5z6nz3zmM0pPTw96HQ9W3lOFoz4MR2auiY8//rjOPvtsxcTE6OKLL+73NfHcc89FZQ284IIL9OUvf9n9HmLOnDkaN25c0F/PA3lM/X3/05MrnLXt61//umJiYnTppZf2es8VzHodaD5qWfCYuT7V1tbqS1/6kiZOnKiYmBiP59HX8xzt79PC9bvkz+/RlClT9JnPfEYzZsxw14Pa2tp+f4+oWQPL5yvjM888o3nz5ikrK0sHDx7UxRdfrMcffzwidTXUNXXYN97effddZWVlaf/+/Tp+/PiAtlm+fLlqamq0ZcsW7dixQ+Xl5ZozZ47eeeedsGZraGhQcXGxqqurtX37dpWWliovL09HjhyRJB08eNB91SBJGjNmjB555BHddtttqq+v12uvvaaEhAR99atfVUdHR9hy+5O9qKhIt912mxYsWKCSkpIB5Q3kOQ3EQJ6fn/3sZ7r77rv173//W/fcc49effVVzZ49Wx0dHRF7fgaSvaioSPPmzdPf//53ORwOzZs3T//85z81e/Zsn6+vPXv2aPLkyXrhhRf0yiuvqLy8XNdee6127doV1uyB/F4cPHgwqBlOtXr1aj3zzDPavn27XnrpJT3wwAP69re/rddff92v/QYj16mPzxVXXKGMjAz94x//0PHjx/XBBx/0+/iYqQa+/vrr7t+pG264QZKiqub11Lg77rhDeXl57t/5+Ph4n6/LSNe4nswVFRVyOp3Kzs5We3u75s+fH7Q6Hsy8JwtHfRiOzFwTn376aV177bW6+OKLJUnXXHNNv6+JmpqaqKyBzc3Nam5u1tNPP63XXntNFotFDofD/bsWjNfzQH/P/H3/c/DgwbDXtv/93/+VJFVXV3u859q+fXvQ6vVg8lHLgsPM9amhoUFFRUXq7u5WTk6OJKm4uLjf5zna36eF43fJ39+jlpYW3XDDDXrvvfdUV1enhIQE3XHHHYqPj3ePP/X3iJo1sHwdHR0+Xwv33HOPrrnmGt12222aPXu2++/Hnv8rgp0xkn8/DvvG20cffaSNGzeqpKRkQOO7u7t13333qbS0VImJiZJO/BF3zjnn6Fe/+lWv8YZh6IYbbtBbb73V675f/vKXeuyxxwLOVllZqfz8fNlsNkkn/hjq6upSVVWVJOmMM85wv1gk6fDhw5o+fbpmz54tSbJYLFqyZIn27t2rpqamoGUfyGM60OxXXnmlZs+ercOHD+uss87qM68/+x/sMQ5kP5WVlZowYYJKS0slnfiPctSoUXrnnXfU1NQ06OdnMPn7y37llVfqpZdeUn5+vqZOnaolS5bowIED+uSTT3y+vtLT091vCiQpOztb3d3devvtt4OWeyDZA/m9OOOMM3zuL5AMp/rLX/6i9PR0d8H+4he/qMTERL344otex4fzd2/WrFkaM2aM+zH4z//8zz4fH7PVwDPPPNP9O3XkyBF99atfjaqaN23aNM2ePVtnnHGGPvroI/fv/DvvvOPzdRnpGtdTlysrKzV37lwtX75ce/fu1ZQpU4JSx6Ph/54egdSH4cjMNfHee+9VXl6ebrrpJknS/9/e/UdHUd/7H39lE1ySECISJCZYMAhBoSAkwSiEUAwgaKsp1HsvjRGKFSm0va1YwqEWPJcezK0/a3tQ6lUD5lIURa9oRcHSxkohC2oFkR9Fi0Ci/NAEhQRCPt8/+O6aJbvJzmZnf8Dzcc6ew87O7Lz2MzNvdt+Z2S0uLm5zn6irq1N1dXVU1sCbb75ZCQkJqqiokMPh0C233KIvv/zSc6y13J/tPs6svv9x18Bw1raCggLPtJbvue65556Q1GtqWXSI5fpUXl6ugoICrVq1yjPf6dOn29zO3bt3j/r3aVaOpXAdRzfddJPKy8vV1NSk5cuXa/bs2aqtrdWBAwc88599HFGzAsu3detWv/vCsGHDNGXKFM++7P78eODAgZDtCy0zRvLz43nfeBs0aJAuv/zygOc/fPiwjh8/rp49e3pNz8jI8HkKb1xcnNLS0jR27Fh98sknnunLli3TPffcoz59+gSdbf369crLy/PcdzgcysnJ0bp16yRJ+fn52rNnj5qamiRJLpdLzz33nNdzdO7cWdKZUy9DlT2QMQ00+4oVKzzZx40b12ZeK+vv6GsMZD3r16/XT3/6U6/XOHDgQE/+jm6fjuRvL/tzzz3ntY3cOfr37+93/9q3b58mTJgg6cyliL/5zW905ZVXauzYsSHLHUj2YI4Ld2MmUFbrxqRJk1RVVaX9+/dLktauXatDhw61qiNu4Tz2Bg8erKuvvlpbtmyRdOYNZlvjE2s1sGvXrl6vZdSoUZKip+b17t3bk3XPnj2eU9g/+OADv/tlpGucu065X5O7PjQ1NYWkjkfD/z0dqQ/no1iuiS6Xy+uPRu3tE3//+9918uTJqKyBq1at8srufuzEiROe1+ren+0+zqy+/xk3blzYa9vChQu9prtrk8vlCkm9ppZFh1iuT+vXr1dRUZHXfIMGDWpzO+fm5kb9+zQrx1K4jqPnnnvO6zhy14NPPvnE73FEzQos38mTJ/3uC7t27VJCQoLy8/O1e/dulZeX68orr9Thw4dDti+0zOhPOGoqv2pqUY8ePZScnKx9+/Z5Td+/f78OHz7sc5n77rtPR44c0bhx4/TWW2/pb3/7m2bOnKmXXnrJc2mDVUeOHFFdXZ3S09O9pqenp6u6ulqS1KVLFz3wwAOaM2eOkpOTVVpaqqysLK/5N27cqIyMDI0YMSImsv/hD39oM28wwvkam5ublZCQoBEjRqhTp04d3j7hyu/OccUVV3gaNP620axZs1RZWamBAwdq7dq16tKlS8Ryu1k9LkJt6tSp+vLLLzVo0CBdcskl2rlzpyZNmqTvfe97fpcJ9/js2LFDkpScnNzm+MR6Ddy/f3/U17wuXbroRz/6Ucj2S7tfU8s6tXz58pDU8WjYDuGqD+ejWKiJ/vaJH/zgB9qxY0dM1MD33ntPF110kV588UW9+eabrfbnaH7/E4hQ59+4caPS09NVW1sbsnodLfsttSxw0V6fevTooQ8//FCS7+2cl5cXk+/T2jqWIpHP/d7m0UcfDdlxdL7VLPcYDhgwoN19Ye7cuTp48KCefPJJTZo0SQMHDoyafUEKUU3t4A88nDP+/Oc/G0nmo48+anfesrIy07t3b7Nv3z5jjDHPPPOM6dSpk+nWrZvfZZqamsx3v/tdM3DgQJOcnGxWrVrVoWz79u0zksyzzz7rNe/MmTNN3759A3rehoYG069fv3azBJvd35gGmz3QvO2t35dwbJ+GhgaTmppqevbsGdDzWnm9dm6jljkC3b+amprMPffcY77xjW+YgwcPhjy3v+yhOC6sCHQfe+yxx8yll15q9uzZY4wx5t133zUPPfSQaW5ubnO5cB57GRkZ53wNjPaaF4s1rrKy0itzKOt4tPzfg8DFck1sOU97+0Qs1MBoqndW3/9Eora5x+uxxx4Leb2mlkWHWK5PLef7/ve/3+52joUa5RZNtcqdLysrK+rfj0V7zWq5vkD3hXP98+N5f6lpMBYtWqQf//jHmjJligoKCvTOO+9o5syZ6tatm99l4uPjNWfOHG3fvl19+/bVjTfe2KEM7i96bGxs9Jre2Njo9SWQbZkxY4YmT56sSZMmtTlftGQPNG8wwvEaZ8yYod69e+viiy8O6DmsvF4787fMEej+FR8fr4ULF8oYowcffDAiuVuyclyEmjFGZWVlmjFjhvr27StJGjJkiF5++WUtXry4zWXDOT5OpzPg54nVGhjtNS8Wa9ySJUu8MoeyjkfDPoPQi5Wa2NY+EQs1MJrqndX3P1aEKr97vCZPniwptPU6WvZbtC8W6tPJkyfb3c6xUKPcoqlWue8fPXo06t+PRXvNarm+QPeFc/3zI423IMTHx+uuu+5SVVWVqqqqdP/996uurk7f/OY3/S6ze/du3XzzzVq4cKGcTqemTJnSoV8/6d69u1JTU1VbW+s1vba2NqDTHsvKypSQkKBf//rX7c4bDdmt5A2G3a/RnT8zMzPk28fO/E8++aRXjra20dnf1+RwONSvXz998MEHYc8d7HFhh0OHDumLL75o9b0Dl112mVatWtXmsuEcn4yMjICfJxZrYLTXvFiscU6nUydPnvTKHMo6Hul9BvaIlZrY1j4R7TXwxIkTUVPvrL7/sSoU+cvLyz3jZUe9jpb9Fu2Lhfp06NChdrdztNeoYI6lcOXbsGGDunbtGtXvx6K9ZrXMJ7W9L5x9rJ3Lnx9pvAXhH//4h44ePeq5b4xRVVWVp+N8tgMHDmjs2LGaOnWqFixYoD/96U/68MMPNWPGjA7lGDNmjFwul1eOrVu3qqioqM3lysvL9fHHH2vp0qWKi4vTli1bPN/bFY3ZreQNht2v0Z3/8ccf1zvvvKP+/fu3md/q67Urf69evbRnzx5PDpfLpU2bNvndv4YNG9ZqWk1Njd+GTjTsW+GQlpYmp9Opmpoar+k1NTVKTEz0u1y4xycnJyfg54i1GhjtNS8Wa1x5ebnS0tLUp08fT2aXyxWyOh7pfQb2iZWa2NY+Ec018K9//au6du0aFfXO6vsfq0KVf//+/V7jNXTo0JDV62jab9G+aK9PkrRt27Z2t3M016hgjqVw5bvvvvt08OBB3XXXXVH9fizaa9bZ+bZs2eJ3Xzh7/5bO4c+PIbtoNcb5uzZ7wYIFpqCgwGvarFmzzIIFCzz3H3nkEZOfn2+amppaPW9zc7MZOnSomT59utf0/fv3mz59+pj7778/6GybNm0yKSkpZufOncYYY5YvX24yMzNNfX293+dasmSJGThwoHn77bdNdXW1qa6uNgsWLDBPPfVUyLO3db17oNnby+tr+wSy/lC9xrbWs2nTJuN0Ok2/fv3M22+/be69917To0cPU1ZW5je/le0Tivz+si9ZssRkZWWZpKQks2rVKlNdXW2Ki4vNhRdeaOrr632Oe1xcnFmzZo3n/vLly43D4TBVVVUhz91W9mCOi2AFWjfuuOMOk52dbY4ePWqMMWbLli2mU6dO5uGHH/b5vJE49l555ZVzsgZGe8176KGHYq7Gucf0iSee8NSIBQsWmB/+8IcmMzPTlJWVRWVdMya89eF8FMs1cdmyZUaSefDBB732iViqgVOnTjUJCQnmjTfeiHi9s/r+J9DnDlV+Y4z52c9+ZiSZ559/3mu87rnnHpOSkmJmz55tCgoKAq7Xoc5ILQutWK5PO3fu9MzXs2dPr+0cSzUqmGMpXOO7ZMkSk5mZadLS0syGDRui8v1YtNcsf/meeuops2nTJnPBBReY3NxcY8zX+8L59PnxvG+8NTY2msLCQjNkyBAjyVx99dVm8uTJnsfnzp1rcnJyvJZZtmyZGTBggMnLyzMjR440M2bMMJ9//rnfdWzcuNFnoduzZ4+pqakJOpsxxrzwwgsmJyfHjBw50owaNcps27bN7/PV19cbh8NhJLW6+TsAg8keSO5AsgeS19f2CXT9HXmNgazHav5gtk+w+dvK3laORYsWtcrt9tvf/tZcc801ZuTIkeaaa64x1157rVchDUXu9rK7WTkugmG1bnz11Vfm7rvvNkOHDjUjRowwgwcPNg888ECbX9QbrmNv5MiRJjc395ysgdFe8zZt2hRzNe6mm27ym7l///5m27ZtUVnXWrK7PpyPYrkmNjY2moEDB5rExEQjyaSkpJhx48b5zW5MdNbA/Px8n8dZJOpdLLx/GzFiRJvj9cILL5iePXuaLl26BFyvQ5WRWhZasVyfjDFm5cqVpkuXLiY5OdlIMoMHD47p92nBHEt2H0dXXXVVu/WTmtX2WLeXzxhjiouLTVJSklfNOp8+P8YZY4wAAAAAAAAAhBTf8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANEiIdIBDJyclqaGhQfHy8Lr744kjHAWCjzz77TKdPn1bnzp311VdfRTpOh1C7gPPLuVS/AAAAEBpxxhgT6RDtiY+PV3Nzc6RjAAgjh8Oh06dPRzpGh1C7gPPTuVC/AAAAEBoxccab+8Orw+HQJZdcEuk4AGxUU1Oj5uZmxcfHRzpKh1G7gPPLuVS/AAAAEBox0Xi7+OKLdeDAAV1yySXav39/pOMAsFGvXr104MCBc+LSTGoXcH45l+oXAAAAQoMfVwAAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbJAQ6QB2WLgw0gmCE6u5AYRGrNaAWM0NAAAAAHbjjDcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABkE13lavXq3c3FwVFBSosLBQ27dv9zuvMUaLFi3SkCFDVFhYqNzcXC1dujTowAAAAAAAAEAsSLC6wObNm1VaWiqXy6Xs7GwtW7ZM48eP144dO5SSktJq/ieffFK/+c1v9MEHHygzM1OffPKJBg0apMzMTN1www0heREAAAAAAABAtLF8xlt5ebkmTpyo7OxsSVJJSYmamppUUVHhc/53331XAwYMUGZmpiTp0ksvVXZ2tl5//fUOxAYAAAAAAACim+XG2/r165WXl/f1EzgcysnJ0bp163zOf9NNN2nHjh16//33JUnvvfeetm3bpp49ewYZGQCCw2XyAAAAAIBwsnSp6ZEjR1RXV6f09HSv6enp6aqurva5TFFRkZ566imNGTNGPXr00M6dO1VQUKAf/ehHftfT2NioxsZGz31jjJWYANAKl8kDAAAAAMLN0hlvx48flyQ5nU6v6U6n0/PY2dasWaM77rhDr732mj744APt3r1b119/vZKSkvyuZ/HixUpNTfXcDh48aCUmALTCZfIAAAAAgHCz1HhzN8tano3mvu+vkTZ//nx997vfVU5OjiQpKytLu3fv1uzZs/2uZ968eaqrq/PcMjIyrMQEgFa4TB4AAAAAEG6WGm/du3dXamqqamtrvabX1tYqKyvL5zK7d+9Wnz59vKZddtllWrVqld/1OJ1Ode3a1XOLi4uzEhMAvLR1mfzevXt9LtPyMvkrr7xSw4YN0/Dhw9u9TL6+vt5z4zJ5AAAAADi/Wf5xhTFjxsjlcnnuG2O0detWFRUV+Zw/MzNTNTU1XtNqamqUmJhoddUAEBQukwcAAAAARILlxltZWZleffVV7dq1S5JUWVmp+Ph43XbbbZKkadOm6dZbb/XM/4Mf/EArV67Uvn37JEn/+te/9Mc//lG33HJLKPIDQLu4TB4AAAAAEAmWftVUkoYPH66KigpNmTJFiYmJcjgcWrt2redXARsaGnTq1CnP/Hfffbfi4uJ08803KykpSfX19Zo5c6Z++ctfhu5VAEAbgr1M/uw/EFx22WV68MEHtXTpUp/LOJ1Or7PquEweAAAAAM5vlhtvklRcXKzi4mKfj61YscJ7BQkJKisrU1lZWTCrAoCQ8HeZ/Pz5833Oz2XyAAAAAICOsnypKQDEIi6TBwAAAACEW1BnvAFArOEyeQAAAABAuNF4A3De4DJ5AAAAAEA4cakpAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2SIh0ADuM3rAw0hGCtDDSAQAAAAAAABAinPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYIOESAcAAJwxesPCSEcI0sJIBwAAAACAqMQZbwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADYIqvG2evVq5ebmqqCgQIWFhdq+fXub8x8+fFi33367Ro8erdzcXA0aNEgrV64MKjAAAAAAAAAQCyw33jZv3qzS0lJVVlaqqqpK06dP1/jx43Xs2DGf8588eVJFRUUaNWqUNmzYIJfLpQkTJqi6urrD4QEAAAAAAIBoZbnxVl5erokTJyo7O1uSVFJSoqamJlVUVPic/4knnlDnzp1VWlrqmTZ37lxNnz49yMgAEBzO1gUAAAAAhJPlxtv69euVl5f39RM4HMrJydG6det8zv/888+rsLDQa1paWpquuOIKq6sGgKBxti4AAAAAINwsNd6OHDmiuro6paene01PT0/X3r17fS7z/vvvKzExUTNnztSIESP0rW99S4899piMMX7X09jYqPr6es+trXkBIBCcrQsAAAAACDdLjbfjx49LkpxOp9d0p9Ppeexsn3/+uRYvXqzvfOc7+tvf/qalS5dq4cKF+u///m+/61m8eLFSU1M9t4MHD1qJCQCtcLYuAAAAACDcLDXekpKSJJ05I62lxsZGz2OtVuBwaPjw4ZowYYIkqV+/fvrBD36ghx56yO965s2bp7q6Os8tIyPDSkwA8MLZugAAAACASEiwMnP37t2Vmpqq2tpar+m1tbXKysryucyll16qXr16eU3r3bu3Pv30U504cUKJiYmtlnE6nV5n1cXFxVmJCQBeOnK27osvvqglS5Zo9+7dKigoUF1dnebOnetzmcWLF+vee+8NbXgAAAAAQMyy/OMKY8aMkcvl8tw3xmjr1q0qKiryOX9BQYFqamq8pn366adKS0vz2XQDgFDjbF0AAAAAQCRYbryVlZXp1Vdf1a5duyRJlZWVio+P12233SZJmjZtmm699VbP/D/72c+0efNmzy8BHj16VMuWLdNPfvKTUOQHgHbZcbauL06nU127dvXcOFsXAAAAAM5vli41laThw4eroqJCU6ZMUWJiohwOh9auXauUlBRJUkNDg06dOuWZf/DgwVq9erVmzZqlTp06qampSXfccYfuuuuu0L0KAGiHv7N158+f73P+goICffTRR17TOFsXAAAAAGCF5cabJBUXF6u4uNjnYytWrGg1bfz48Ro/fnwwqwKAkCgrK1NRUZF27dql/v37+zxbt6mpScuXL5d05mzd/Px8VVdXKy8vj7N1AQAAAACWBdV4A4BYw9m6AAAAAIBwo/EG4LzB2boAAAAAgHCy/OMKAAAAAAAAANpH4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwQVCNt9WrVys3N1cFBQUqLCzU9u3bA1puzZo1iouL09NPPx3MagEAAAAAAICYkWB1gc2bN6u0tFQul0vZ2dlatmyZxo8frx07diglJcXvcl999ZV++ctfdigsAAAAAAAAECssn/FWXl6uiRMnKjs7W5JUUlKipqYmVVRUtLncr371K82cOTO4lAAQApytCwAAAAAIJ8uNt/Xr1ysvL+/rJ3A4lJOTo3Xr1vld5p133tHmzZt1xx13BJcSADrIfbZuZWWlqqqqNH36dI0fP17Hjh1rcznO1gUAAAAABMtS4+3IkSOqq6tTenq61/T09HTt3bvX5zLNzc2aNWuWfv/73ysuLi6g9TQ2Nqq+vt5zM8ZYiQkArXC2LgAAAAAg3Cw13o4fPy5JcjqdXtOdTqfnsbP97ne/08iRIzV48OCA17N48WKlpqZ6bgcPHrQSEwBa4WxdAAAAAEC4WWq8JSUlSTpzRlpLjY2NnsdaOnDggJ544gktWLDAUqh58+aprq7Oc8vIyLC0PAC0xNm6AAAAAIBIsNR46969u1JTU1VbW+s1vba2VllZWa3mf/311yVJN9xwg0aPHq3Ro0dLku677z6NHj1ab731ls/1OJ1Ode3a1XML9EMvAPjC2boAAAAAgEhIsLrAmDFj5HK5PPeNMdq6davmz5/fat5p06Zp2rRpXtPi4uJUVlamqVOnWk8LAEEI9mzdjRs3WlrPvHnz9POf/9xz/4orrqD5BgAAAADnMcu/alpWVqZXX31Vu3btkiRVVlYqPj5et912m6QzzbZbb701tCkBoAM4WxcAAAAAEAmWz3gbPny4KioqNGXKFCUmJsrhcGjt2rVKSUmRJDU0NOjUqVOtlrvvvvv02muvef799NNPa8OGDR1LDwAB4mxdAAAAAEC4WW68SVJxcbGKi4t9PrZixQqf08vKylRWVhbM6gCgw8rKylRUVKRdu3apf//+Ps/WbWpq0vLlyyOcFAAAAABwrgiq8QYAsYazdQEAAAAA4UbjDcB5g7N1AQAAAADhZPnHFQAAAAAAAAC0j8YbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANqDxBgAAAAAAANiAxhsAAAAAAABgAxpvAAAAAAAAgA1ovAEAAAAAAAA2oPEGAAAAAAAA2IDGGwAAAAAAAGADGm8AAAAAAACADWi8AQAAAAAAADag8QYAAAAAAADYgMYbAAAAAAAAYIOgGm+rV69Wbm6uCgoKVFhYqO3bt/udd926dfrOd76jMWPG6JprrtG4ceP0zjvvBB0YAAAAAAAAiAWWG2+bN29WaWmpKisrVVVVpenTp2v8+PE6duyYz/nvvPNOffvb39abb76pjRs3Kj8/X2PHjtVnn33W4fAAYAV/NAAAAAAAhJPlxlt5ebkmTpyo7OxsSVJJSYmamppUUVHhc/7c3FxNnz7dc/8nP/mJjhw5onXr1gUZGQCs448GAAAAAIBws9x4W79+vfLy8r5+AodDOTk5fhtpf/zjH+VwfL2azp07S5JOnjxpddUAEDT+aAAAAAAACDdLjbcjR46orq5O6enpXtPT09O1d+/egJ5j48aNSkxM1I033uh3nsbGRtXX13tuxhgrMQGgFf5oAAAAAAAIN0uNt+PHj0uSnE6n13Sn0+l5rC3GGC1atEj/9V//pbS0NL/zLV68WKmpqZ7bwYMHrcQEAC/80QAAAAAAEAmWGm9JSUmSzny4bKmxsdHzWFsWLlyozMxM3XXXXW3ON2/ePNXV1XluGRkZVmICgBf+aAAAAAAAiARLjbfu3bsrNTVVtbW1XtNra2uVlZXV5rKPP/64qqur9fTTT7e7HqfTqa5du3pucXFxVmICgBf+aAAAAAAAiATLP64wZswYuVwuz31jjLZu3aqioiK/y6xYsUIrV67U888/rwsuuEB79+7lC8oBhA1/NAAAAAAARILlxltZWZleffVV7dq1S5JUWVmp+Ph43XbbbZKkadOm6dZbb/XMv2bNGpWVlemee+7R9u3b5XK59MYbb+itt94K0UsAgPbxRwMAAAAAQLglWF1g+PDhqqio0JQpU5SYmCiHw6G1a9cqJSVFktTQ0KBTp0555p82bZoOHz6sMWPGeD3PggULOhgdAAJXVlamoqIi7dq1S/379/f5R4OmpiYtX75c0td/NHj66ae1fft2SdKWLVtUU1PTZrMOAAAAAAA3y403SSouLlZxcbHPx1asWOF1/9ChQ8GsAgBCij8aAAAAAADCLajGGwDEIv5oAAAAAAAIJ8vf8QYAAAAAAACgfTTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsEFCpAMAsGjhwkgnCE6s5gYAAAAAIEic8QYAAAAAAADYgMYbAAAAAAAAYAMabwAAAAAAAIANaLwBAAAAAAAANuDHFQAAiCWx+kMlsZobAAAA6ADOeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGfMcbAACwX6x+x1us5gYAAEBUoPGG8xcfpgAAAAAAgI241BQAAAAAAACwAWe8AQA6JlbPHo3V3AAAAABiBme8AQAAAAAAADag8QYAAAAAAADYgEtNAQDnJy41BQAAAGAzGm9RZMPohZGOEJTRGxZGOgIAAAAAAEDU4VJTAAAAAAAAwAY03gAAAAAAAAAbcKkpOo7vSQIAAAAAAGiFxhsAoEM2bIh0guCMHh3pBAAAAADOdVxqCgAAAAAAANiAxhsAAAAAAABgg6Aab6tXr1Zubq4KCgpUWFio7du3tzn/W2+9pfz8fBUWFio/P19VVVVBhQWAjqB2AQAAAADCyfJ3vG3evFmlpaVyuVzKzs7WsmXLNH78eO3YsUMpKSmt5v/Xv/6lG264QS+99JJGjx6tv/zlL7rxxhv1j3/8Q7179w7JiwDOJzH7fVoRXj+1C2eL2WNpdKQTAAAAAAiU5cZbeXm5Jk6cqOzsbElSSUmJfvGLX6iiokKzZ89uNf9vf/tbDRgwQKP//yeFwsJCZWdn69FHH9X999/fsfSICnx4RSygdgGRFbP/V0Q6AAAAAGKa5cbb+vXr9ctf/tJz3+FwKCcnR+vWrfP54XXdunUqKCjwmpaXl6d169YFERcAgkPtwrkiVhtYAAAAwPnIUuPtyJEjqqurU3p6utf09PR0VVdX+1xm7969+t73vtdq/r179/pdT2NjoxobGz33P/vsM0lSTU2NevXq1W7OxkP17c4DaGOkA5xfnL2eCGi+mpoaSV8f96FA7QIQrEBrl2RP/QIAAEBss9R4O378uCTJ6XR6TXc6nZ7HfC1jZX5JWrx4se69995W05ubm3XgwAErkQFEiwPHLM1++vTpkK2a2gUgaBZrlxTa+gUAAIDYZqnxlpSUJEleZ3S477sf87WMlfklad68efr5z3/uuZ+enq7GxkY1NzcrMzPTSuSo8emnn6pnz56RjhGwWMkb7TmjOV+0ZXPn+eyzz3T69Gl17tw5ZM8d6doVHx+viy++ONj4ERNt+0igYil3rGSN9pzRks+O+gUAAIDYZqnx1r17d6Wmpqq2ttZrem1trbKysnwuk5WVZWl+6cxZJS3PNHGfYXLllVfqgw8+sBI5asRa9ljJG+05ozlftGWzM0+ka1esirZ9JFCxlDtWskZ7zmjPBwAAgPOXw+oCY8aMkcvl8tw3xmjr1q0qKiryOf91113nNb8kuVwuv/O3ZdasWZaXiRaxlj1W8kZ7zmjOF23Z7M4TydoVq6JtHwlULOWOlazRnjPa8wEAAOD8FWeMMVYW2Lx5s4qKiuRyudS/f38988wzKisr044dO5SSkqJp06apqalJy5cvlyT961//0uDBg/Xyyy9r1KhRqqqq0g033KD3339fvXv3tuVFAcDZqF0AAAAAgHCzdKmpJA0fPlwVFRWaMmWKEhMT5XA4tHbtWqWkpEiSGhoadOrUKc/8vXv31po1a3T33XfrggsuUGNjo1555RU+uAIIK2oXAAAAACDcLJ/xBgAAAAAAAKB9lr/jDQAAAAAAAED7LF9qeq575ZVX9Nprryk5OVl9+vTRnXfeGelIlsRa/mjLG215AhXNuaMtW7TlQexuk2jMHY2ZrIj2/NGeDwAAAFHIwOPYsWOmf//+5tSpU8YYY/Lz880///nPCKcKXKzlj7a80ZYnUNGcO9qyRVsexO42icbc0ZjJimjPH+35AAAAEJ2CutT05MmTmjdvnhISEvTxxx+3O/+AAQM0evRor9vll1+uUaNGBbP6DmVbvXq1hg0bpksvvVRxcXF6/fXXPY/9/e9/1+WXX66EhDMnAubl5emVV17Rz372M1111VUqLCzU1VdfrdWrV4c8d1vZW05//PHHlZubq4KCAhUWFmr79u2t8htjtGjRIn300UcqKipSfn6+/vrXv4Y1s9vq1auVlZWlbt26qVu3bho0aJAmTZqkvXv3+hzvltsjlFmeffZZjRs3ToMHD1ZycrLS0tI0fPhwn+PXMs+rr75qaV8PxrPPPquioiL16dNHcXFxmjBhgvbu3es1z+rVqz3b/corr9S1116r6667Tnl5eZowYYIyMjJCNo7usbruuuuUm5ur7OxsxcfH+339q1ev1uWXX66LLrpI3bp107Bhw5STk6Nnn3025NtYCmyfa+8YCWWecLFSdxsbGyNet1o+PmnSJMXFxSkvL6/dbTJs2DD953/+p/Lz83XNNddEvH7l5uZqwIAB6tatm/Lz85WXl6dJkyZ59vtw7tuSNGfOHHXt2lUXXnihUlJSNHbsWE+9aGv/tvr/drD+93//V5dddpni4uI0ZMgQT713O/v4fOCBBzz1JicnR3V1ddq3b1+r/KEQyBicq/UDAAAAkWO58fbxxx+rsLBQBw8e1OnTpwNaJj09XRs2bPC6XXXVVfq3f/s3y4E7km3z5s0qKSlRc3OzxowZI0kqLS3VsWPHJEmHDh3y/MKhJHXt2lXPP/+8XnrpJVVVVekvf/mLHnvsMf37v/+73nvvvbBkP3v6z3/+c1VWVqqqqkrTp0/X+PHjW+WfP3++VqxYoZKSEk2bNk0LFy7UhAkT9M9//jMsmd02b96s0tJS7d+/XytXrtQjjzyizz//XJ07d9b111+vAwcOtBrvQ4cO2ZKlpKREN954oz766CO5XC7deOON2rdvn8aNG+d3+zc3N6u8vNzSvh6MkpISHThwQIWFhZKk5ORkXX/99WpoaJD09Ti6t/uuXbu0a9cuvfjii9q0aZMcDodcLpdn/o6MozvPnDlz9D//8z9KSEjQ6dOn1dzcrMbGxlbzurNdcsklevjhh/XII4/os88+09y5c/Uf//Ef2rJlS8i2sRT4PtfeMRKqPOFite4uWrQoonWr5eM5OTn6v//7P0nSc8891+422bp1q/7+979r7dq12rhxY8TrV2Vlpfbu3avbb79d+/fv17p165SSkqJf/OIXSkpK8sxv977tzvTAAw/ooYce0hdffKFHH31Ub7/9tsaNG6eGhga/+3cw/28H+xpKSkp0+eWXSzrTxEpJSfHUM1/H55w5c/SjH/1I69ev169+9SslJiZ65g/l8Rno+J6L9QMAAACRZbnx9uWXX2r58uWaNm1awMs89dRTXvePHj2qN954Q1OmTGk1rzFGt99+u3bu3NnqsUceeUQrV64MOlt5ebkKCgq0atUqzzynT59WRUWFJKlHjx6eN9iSVF9fr6NHjyovL8/zZnvo0KFKTU3Vm2++6XMdweb3l/3s6d/61reUnZ0t6UyDpKmpySt/fX29fv/732v69Ok6efKkevTooeuvv159+vTRb3/725DlbSuzW3l5uSZOnKibbrpJ48aNU0lJiU6fPq3MzEzt3r1bX3zxRavxTktLC+n4ud100036y1/+ookTJ+qKK67Q7Nmz9emnn+rEiRN+t//nn3+uH/7whwHt6x0Zx9GjR+vll1/2rKe0tFS7d+/W1q1bJX09ju7tfvPNNyshIUEVFRVyOBy65ZZb9OWXX3rmr6+vV48ePYLO5t5eX375pZ555hndfffdkqRt27a1Wt6d7eGHH9aUKVM8++S+ffvU3NysEydOhGwbS4Hvc20dI2fnaTlW0cpq3X333XcjWrdaPp6RkaGRI0d6prW1TZqbm+VyuTRq1CilpqZKUsTrV3Z2tm666SaVl5erqalJy5cv1+zZs1VbW6sDBw545nfvS3aNpTtTr169NH36dElnakXnzp31z3/+U1u3bvW7f1vZfzo6puPHj9f8+fMlSQ6HQ7Nnz/bUM1/Hp9Pp1P79+yVJPXv2VEZGhmd+X8en3eN7LtYPAAAARJblxtugQYM8f80O1GWXXeZ1f8WKFZowYYK6devWat64uDilpaVp7Nix+uSTTzzTly1bpnvuuUd9+vQJOtv69etVVFTkNc+gQYO0bt06SVJ+fr727NmjpqYmSZLL5VJJSYmqqqo8HwzWrl2rQ4cOqWfPnj7XEWx+f9nPnj548GDPvx0Oh3Jycrzy79y5U8ePH1fPnj3lcrk0btw4SVJGRobPy7XsHu+8vDw999xzXnldLpckqX///q3Ge/z48SEdP7fnnnvOk0eSOnfu7Mngb/t/9NFHKikp8fucLXVkHF9//XWv7E6nU9KZy6IkeeWWpFWrVnltd/djJ06ckCSv7R5MNvf2co/pBRdcIEk6depUq+zubDk5OUpISJDD4dDQoUP1hz/8QVdeeaXuvPPOkG3jlpn8OXusfB0jZ+dpOVbRymrdnTRpUkTrVsvHN23apAEDBnimtbVNDh8+rKamJo0YMcLreSJZv6Qzx0TL3O768cknn7Tal+waS3emn/70p577DodDAwcOlHSmXvjbv63sPx0d0z/96U9e09xjdfLkSZ/H53XXXee1L7ibmSdPnvR5fNo9vudi/QAAAEBkReRXTZ9++mktWrTI7+P33Xefjhw5onHjxumtt97S3/72N82cOVMvvfSSrr766qDWeeTIEdXV1Sk9Pd1reo8ePfThhx9Kkrp06aIHHnhAc+bMUXJyskpLS3XnnXcqKSlJgwYN0iWXXKKdO3dq0qRJ+t73vhfW/HV1dZ68LaWnp6u6utqT/+GHH9akSZP0xBNPqLS0VFlZWZKk/fv36/Dhw2HL62+809PTtXbtWmVkZKioqKjVeGdlZYUlz8aNG5WRkaErrrhCW7ZskeR7+2dlZXm+b6g9ocq9detWZWRkaMSIEW2Oo3u7v/fee7rooov04osv6s033/Ta7qHI5v6Oo5ycHK/pvrLNmjVLb775phISErRz506lp6dHbBv7Git/2/hcM3XqVH355ZcRr1vubXLRRRd5Tfe3TZKSkuR0Oj2XTbtFU/2qrq721I9HH33U574UzkzNzc1KSEjQiBEj1KlTp5Ds36HM7x6rAQMGBHR8fvvb39ZTTz2l1157zW9+6gcAAABiSrC/yvDnP//ZSDIfffSRpeW2b99uevXqZU6fPt3mfE1NTea73/2uGThwoElOTjarVq3qULZ9+/YZSebZZ5/1muf73/++6du3r9/neuyxx8yll15q9uzZY4wx5t133zUPPfSQaW5utiW/v3FduXKlkWR+97vfeU2fOXNmq/xlZWWmd+/eZt++fcYYY5555hnTqVMn061bt5Dn9Zf57PF2u+OOO0ynTp3aff5Qj1/LPA0NDaZfv35m1apVPscv0OcMZe6W6+nTp49nOX/j6M7d8rXYka2hocH06tWr3TFtacaMGaZbt27mG9/4hjl48GBI87hZ2ecC2caxItB9MVrqlnubLFiwwOvxtrZJNNevmTNnmqysrICOOTtrmFtDQ4NJTU01PXv27NBzhzJ/y/V8+OGHnrEK5Pi0u561zBZoPTuX6gcAAAAiI6hfNe2Ip59+WqWlpXI42l51fHy85syZo+3bt6tv37668cYbO7Re95dgn/0F8SdPnvT6guyWjDEqKyvTjBkz1LdvX0nSkCFD9PLLL2vx4sVhzX/25YdujY2NrfIvWrRIP/7xjzVlyhQVFBTonXfe0cyZM31e2mtXXn/j/eabb+rCCy/UpEmT2lzezjwzZszQ5MmTNWnSJJ/j1xGhyD1x4kTP+PgbR3fulq/FjmwzZszw++vD/rKdOnVKmZmZMsbowQcfDGmetrQ3VueLaKpb7nE/+zLltrZJNNevxsZGHT16NKBjLhyZZsyYod69e+viiy/u0HP7Eor88+fP94xVIMen3fWsLdQPAAAA2CWsjbfTp0+rsrIyoC943r17t26++WYtXLhQTqdTU6ZM6dCvsXXv3l2pqamqra31mn7o0CG/l4ocOnRIX3zxRavvjLnsssu0atWqsOZ3f9H42b+gVltb2yp/fHy87rrrLlVVVamqqkr333+/6urq9M1vfjNseX2Nd1lZmRoaGnTNNde0u7xdeZ588kklJCTo17/+tSTf49cRHcm9dOlSSdKcOXNa5T57v62trdWJEye8Xkuos5WVlSkhIUG33367z8dbZmvZEK6trVXfvn3Vr18/ffDBByHL0562xup8uhwsmuqWe5scPXrUa3pb2yRa65ckbdiwQV27dg3omLM7k/v4zMzMtGX/DkX++Ph4z1i1d3y6X49d9aw91A8AAADYJayNt9dff119+/Zt9wuODxw4oLFjx2rq1KlasGCB/vSnP+nDDz/UjBkzOrT+MWPGeL7Y323btm0qKiryOX9aWpqcTqdqamq8ptfU1CgxMTHs+SXp/fff9/zbGKOtW7e2yv+Pf/zD64OuMUZVVVWaPHlyWPO2HO/y8nJ9/PHHOnXqlMaOHastW7Z4vlstXHl69eqlPXv2aOnSpYqLi5PL5dKmTZv8bn+rOpK7vLzc84EvLi7Oa3zO3m+NMfrrX/+qrl27el5LW+MZTDb39nI/v+S977m5sw0bNsyTzb1P1tTUKCMjI+Rj1RZfY+XrGDmXRVvdGjNmjNcvULa3TaKxfklnvlfs4MGDuuuuu9o95uzO5D4+H3/8cb3zzjvq379/m8e/VR3Nv2LFCknS4sWLvcbK3/FpjPGqN6GuZ4GifgAAAMAWwV6j6u97UhYsWGAKCgp8LnPLLbeYJ598ss3nbW5uNkOHDjXTp0/3mr5//37Tp08fc//99wedbdOmTSYlJcXs3LnTM0/Pnj1NfX293/x33HGHyc7ONkePHjXGGLNlyxbTqVMn8/DDD9uS31929/Tk5GSzc+dOY4wxy5cvNykpKebaa6/1mnfWrFlmwYIFnvuPPPKIyc/PN01NTSHP21Zm93gvXLjQDBw40PzqV78yPXr0MBs2bDALFiwwTz31VMjz+MuyZMkSk5WVZZKSksyqVatMdXW1KS4uNhdeeKGpr69vc78N5HuROpJ7yZIlZuDAgeZ3v/udkWReeuklr/G5/fbbTXx8vGe7T5061SQkJJg33njDVFdXm+rqar/jGUw2d563337bVFdXm8cee8xIMr/5zW+MMd7HiHsbx8XFmTVr1pjly5ebzMxMs3TpUuNwOExVVVVIx8otkGPcGOPJ0/IYj2WB1t1oqVvGnNkmiYmJnsdbbhNfx1201a+dO3eaJUuWmMzMTJOWlmY2bNjQ5jFn91g6nU7Tr18/8/bbb5t7773X9OjRw5SVlXmyRLKWGXOmfvTp08dTy1qO1aZNm8wFF1xgcnNzjTFnjs8LL7zQXHHFFZ56E+p6ZmUMzvX6AQAAgMiw3HhrbGw0hYWFZsiQIUaSufrqq83kyZM9j8+dO9fk5OS0Wu7zzz833bt3N8eOHWt3HRs3bvT5IWvPnj2mpqYm6GzGnPmRgi5dupjk5GQjyQwePLjN/F999ZW5++67zdChQ82IESPM4MGDzQMPPNDml5QHk99f9rOn9+vXz1x44YVm5MiRZtSoUWb69OmtxnvZsmVmwIABJi8vz4wcOdLMmDHDfP755yHN21bmliorK40knzd/H6xCOX7GGFNfX28cDofPDIsWLTLG+N5vA3l9Hc3dVjb3+MydO9f07dvX5OTkmPz8fMvjaSVbW3mysrLM5MmTW43VCy+8YC699FKTkpJiunbtaoYMGWKuvfZas2bNmpCOlTGBbZMXXnjB5OTkeI6Rbdu2+X2+WGG17ka6bp39uLsRk5KSYtLS0jzbxNdxF03164UXXjBXXXVVxGuYW6D1IhK1zBhjDh8+3O5YFRcXm6SkJDNy5Ehz7bXXtvt6QpUv0DE4F+sHAAAAIivOGGOsnSMHAAAAAAAAoD1h/1VTAAAAAAAA4HxA4w0AAAAAAACwAY03AAAAAAAAwAY03gAAAAAAAAAb0HgDAAAAAAAAbEDjDQAAAAAAALABjTcAAAAAAADABjTeAAAAAAAAABvQeAMAAAAAAABsQOMNAAAAAAAAsAGNNwAAAAAAAMAGNN4AAAAAAAAAG9B4AwAAAAAAAGzw/wBc5s1HK8qdqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from privacy_risk_score_utils import *\n",
    "\n",
    "risk_score = calculate_risk_score(MIA.s_tr_m_entr, MIA.s_te_m_entr, MIA.s_tr_labels, MIA.s_te_labels, MIA.t_tr_m_entr, MIA.t_tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
