{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from hop_skip_jump_attack import hop_skip_jump_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# torch.cuda.set_device(2)\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "global best_acc\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_fc(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_fc, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "#         x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_fc(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_fc(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_conv(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_conv, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.q, alpha = self.alpha)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.q, alpha = self.alpha)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.q, alpha = self.alpha)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.q, alpha = self.alpha)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "#         h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "#         h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value = np.percentile(nonzero, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_conv(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_conv(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv = 100,qfc = 100, aconv = 1, afc = 1):\n",
    "        super(AlexNet_comb, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv = qconv\n",
    "        self.qfc = qfc\n",
    "        self.aconv = aconv\n",
    "        self.afc = afc\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.qconv, alpha = self.aconv)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.qconv, alpha = self.aconv)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.qconv, alpha = self.aconv)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.qconv, alpha = self.aconv)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.qconv, alpha = self.aconv)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.qfc, alpha = self.afc)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.qfc, alpha = self.afc)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "#     print('q: ', q, 'percentile_value: ', percentile_value)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_comb(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_comb_mid(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv_l = 100,qconv_h = 100,qfc_l = 100,qfc_h = 100, aconv_l = 1,aconv_h = 1, afc_l = 1, afc_h = 1):\n",
    "        super(AlexNet_comb_mid, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv_l = qconv_l\n",
    "        self.qconv_h = qconv_h\n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.aconv_l = aconv_l\n",
    "        self.aconv_h = aconv_h\n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage_mid(h1, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage_mid(h2, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage_mid(h3, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage_mid(h4, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage_mid(x, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage_mid(h6, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage_mid(h7, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "#     p_value_low = np.percentile(flattened_weights, q_l)\n",
    "#     p_value_high = np.percentile(flattened_weights, q_h)\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    p_value_low = np.percentile(nonzero, q_l)\n",
    "    p_value_high = np.percentile(nonzero, q_h)\n",
    "#     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "        \n",
    "def alexnet_comb_mid(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb_mid(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_mod(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_mod, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.lin4 = nn.Linear(256*1*1, 10)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "        out_h5 = self.lin4(torch.flatten(h5, 1))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7, out_h5\n",
    "\n",
    "def alexnet_mod(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_mod(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(num_classes,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.labels=nn.Sequential(\n",
    "#            nn.Linear(num_classes,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.ReLU(),\n",
    "#             )\n",
    "            nn.Linear(num_classes,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.combine=nn.Sequential(\n",
    "#             nn.Linear(64*2,256),\n",
    "            nn.Linear(64*2,512),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        for key in self.state_dict():\n",
    "            print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "                print (key)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x,l):\n",
    "        \n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(l)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        is_member =self.combine( torch.cat((out_x  ,out_l),1))\n",
    "        \n",
    "        \n",
    "        return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defense model\n",
    "class Defense_Model(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Defense_Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(num_classes, 256),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        is_member = self.features(x)\n",
    "        return self.output(is_member), is_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "        diff_top = torch.mean(sort_diff_top) * alpha\n",
    "    #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "    #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_top.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_one(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_fc_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "#         fh1 = torch.sum(torch.sum(h1,2),2)\n",
    "#         fh2 = torch.sum(torch.sum(h2,2),2)\n",
    "#         fh3 = torch.sum(torch.sum(h3,2),2)\n",
    "#         fh4 = torch.sum(torch.sum(h4,2),2)\n",
    "#         fh5 = torch.sum(torch.sum(h5,2),2)\n",
    "        \n",
    "        out_list = [h6, h7, outputs]\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "# #         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "# #         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "#         sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "# #         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "# #         var = torch.mean(sort_var)*beta\n",
    "\n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "# #         var = sort_var * beta\n",
    "\n",
    "# #         s_var = sort_var * beta\n",
    "# #         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "# #         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "# #         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "# #         low8_loss = -torch.mean(low8_var) * alpha\n",
    "#         sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "#         diff_top = torch.mean(sort_diff_top) * alpha\n",
    "#     #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "#     #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# h2n = torch.norm(h2, dim = (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs ,_,_,_,_,_,_,_,_ = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            if epoch == 9:\n",
    "                print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "                print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train_softmax(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs) #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        \n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test_softmax(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs)#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "#             if epoch == 9:\n",
    "#                 print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#                 print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_sort_softmax(trainloader, model, attack_model, criterion,\n",
    "                              attack_criterion, optimizer, attack_optimizer, epoch, use_cuda, num_batchs=100000,\n",
    "                              skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // att_batch_size), (len(train_data) // att_batch_size)) + 1\n",
    "\n",
    "    # print (skip_batch, len_t)\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_  = model(inputs_attack)\n",
    "\n",
    "        # classifier_input = torch.cat((inputs, inputs_attack))\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        #         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "\n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        # print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_sort_softmax(testloader, model, attack_model, criterion, attack_criterion,\n",
    "                             optimizer, attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    sum_correct = 0.0\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // batch_size), (len(train_data) // batch_size)) + 1\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in testloader:\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_ = model(inputs_attack)\n",
    "\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        # attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        correct = np.sum(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        sum_correct += correct\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "\n",
    "    #         break\n",
    "    #     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "\n",
    "    return (losses.avg, top1.avg, sum_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset='cifar10'\n",
    "checkpoint_path='./checkpoints_cifar10'\n",
    "train_batch=200\n",
    "test_batch=100\n",
    "lr=0.001\n",
    "epochs=60\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))\n",
    "        \n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    mkdir_p(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# global best_acc\n",
    "start_epoch = 0  # start_ from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing dataset %s' % dataset)\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "\n",
    "# prepare test data parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mean = [0]\n",
    "# std = [1]\n",
    "# transform_test = transforms.Compose(\n",
    "#     [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    dataloader = datasets.CIFAR10\n",
    "    num_classes = 10\n",
    "else:\n",
    "    dataloader = datasets.CIFAR100\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> creating model \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"==> creating model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AlexNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = model.cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 20.13M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_attack = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_admm = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Resume\n",
    "title = 'cifar-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainset_private = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "trainloader_private = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_privacy=200\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(5000):\n",
    "#     private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "# for i in range(15000,30000):\n",
    "#     private_trainset_intest.append(trainset[r[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "r = np.arange(50000)\n",
    "for i in range(5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,30000):\n",
    "    private_testset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.shuffle(r)\n",
    "\n",
    "for i in range(5000):\n",
    "    private_trainset_intest.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# start train\n",
    "dataset='cifar10'\n",
    "checkpoint_path='./checkpoints_cifar10'\n",
    "# train_batch=400\n",
    "# test_batch=200\n",
    "lr=0.01\n",
    "epochs=100\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)\n",
    "model = AlexNet(num_classes)\n",
    "# model = AlexNet_comb_mid(num_classes, qconv_l = 60,qconv_h = 90,qfc_l = 80,qfc_h = 90,\n",
    "#                          aconv_l = 2.2,aconv_h = 1.2, afc_l = 1, afc_h = 1.2)\n",
    "# model = AlexNet_b(num_classes)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [60]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "#     elif epoch == 60:\n",
    "#         state['lr'] = 0.005\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  with random clip\n",
    "is_best=False\n",
    "best_acc=0.0\n",
    "start_epoch=0\n",
    "alpha = 0\n",
    "beta =250\n",
    "num_class = 10\n",
    "epochs=100\n",
    "mean_class = np.zeros((num_class,num_class))\n",
    "mean_class_h5 = np.zeros((num_class,num_class))\n",
    "var_n = np.zeros(num_class)\n",
    "\n",
    "test_acc_res = []\n",
    "test_loss_res = []\n",
    "train_acc_res = []\n",
    "train_loss_res = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "#     adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "#     train_enum = enumerate(trainloader)\n",
    "#     train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "    train_loss,train_var,train_svar, train_acc = train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=num_class, alpha = alpha, beta = beta)\n",
    "\n",
    "    train_acc_res.append(train_acc.item())\n",
    "    train_loss_res.append(train_loss.item())\n",
    "    \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "    test_acc_res.append(test_acc.item())\n",
    "    test_loss_res.append(test_loss.item())\n",
    "    \n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}, best_acc: {best_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc, best_acc=best_acc))\n",
    "    \n",
    "#     # save model\n",
    "#     if test_acc > 70:\n",
    "#         if (is_best) or epoch+1 == epochs:\n",
    "#             save_checkpoint({\n",
    "#                     'epoch': epoch + 1,\n",
    "#                     'state_dict': model.state_dict(),\n",
    "#                     'acc': test_acc,\n",
    "#                     'best_acc': best_acc,\n",
    "#                     'optimizer' : optimizer.state_dict(),\n",
    "#                 }, is_best, checkpoint=checkpoint_path,filename='cifar10_min_var_beta250_epoch%d'%(epoch+1))\n",
    "\n",
    "    \n",
    "print('Best acc:')\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss, test_acc = test(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc))\n",
    "\n",
    "test_loss, test_acc = test(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "for i in range(len(train_acc_res)):\n",
    "    plt.plot(train_acc_res[i])\n",
    "    plt.plot(test_acc_res[i])\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "# model = AlexNet(num_classes)\n",
    "# model = AlexNet_fc(num_classes, q = 70, alpha = 5)\n",
    "# model = AlexNet_conv(num_classes, q = 65, alpha = 1.5)\n",
    "# model = AlexNet_comb(num_classes, qconv = 100, qfc = 50, aconv = 1, afc = 20)\n",
    "model = AlexNet_comb_mid(num_classes, qconv_l = 100,qconv_h = 65,qfc_l = 100,qfc_h = 100,\n",
    "                         aconv_l = 1,aconv_h = 1.5, afc_l = 1, afc_h = 1)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training and testing the attack result from the checkpoint\n",
    "\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta200_conv60a2.2_90a1.2_fc90a1.2_defense_epoch69'\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta200_alpha2_conv90_defense_epoch95'\n",
    "resume='./checkpoints_cifar10/cifar10_min_var_beta200_conv90a1.5_nonz_defense_epoch68'\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta250_defense_epoch66'\n",
    "# resume='./checkpoints_cifar10/cifar10_base_epoch79'\n",
    "# resume='./checkpoints_cifar10/cifar10_alexnet_100_epoch14'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load(resume)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer']) \n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/100) Data: 0.064s | Batch: 0.125s | Loss: 2.0818 | top1:  71.0000 | top5:  95.0000\n",
      "(21/100) Data: 0.004s | Batch: 0.049s | Loss: 2.0638 | top1:  74.9524 | top5:  96.6667\n",
      "(41/100) Data: 0.003s | Batch: 0.046s | Loss: 2.0677 | top1:  74.0976 | top5:  96.3415\n",
      "(61/100) Data: 0.002s | Batch: 0.045s | Loss: 2.0654 | top1:  74.6721 | top5:  96.4754\n",
      "(81/100) Data: 0.002s | Batch: 0.045s | Loss: 2.0659 | top1:  74.7284 | top5:  96.4938\n",
      "Classification accuracy: 74.51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "train_loss, final_train_acc = test(trainloader, model, criterion, epoch, use_cuda)\n",
    "print ('Trainset Classification accuracy: %.2f'%(final_train_acc))\n",
    "\n",
    "# q:  75 percentile_value:  0.3447321504354477\n",
    "# q:  75 percentile_value:  0.18002179265022278\n",
    "# q:  75 percentile_value:  0.01930836820974946\n",
    "# q:  75 percentile_value:  0.01867781998589635\n",
    "# q:  75 percentile_value:  0.005449312971904874\n",
    "# q:  50 percentile_value:  0.0015738015063107014\n",
    "# q:  50 percentile_value:  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss( reduction = 'none')\n",
    "model.eval()\n",
    "test_loss_list = []\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "test_prob = []\n",
    "test_logits = []\n",
    "test_label = []\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    # measure data loading time\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "    # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "    outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss_value = loss.data.cpu().numpy()\n",
    "    outputs_value = outputs.data.cpu().numpy()\n",
    "    prob_outputs = softmax(outputs)\n",
    "    prob_outputs_value = prob_outputs.data.cpu().numpy()\n",
    "    targets_value = targets.data.cpu().numpy()\n",
    "    for i in range(len(loss_value)):\n",
    "        test_loss_list.append(loss_value[i])\n",
    "        test_logits.append(outputs_value[i])\n",
    "        test_prob.append(prob_outputs_value[i])\n",
    "        test_label.append(targets_value[i])\n",
    "#     break\n",
    "test_loss_array = np.asarray(test_loss_list)\n",
    "\n",
    "test_prob_array = np.asarray(test_prob)\n",
    "test_logits_array = np.asarray(test_logits)\n",
    "test_label = np.asarray(test_label)\n",
    "# test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(test_prob_array.shape, test_label.shape)\n",
    "\n",
    "sort_test_prob=np.sort(test_prob_array,axis=1)\n",
    "sort_test_logits=np.sort(test_logits_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_prob_array = np.asarray(test_prob)\n",
    "test_logits_array = np.asarray(test_logits)\n",
    "test_label = np.asarray(test_label)\n",
    "# test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(test_prob_array.shape, test_label.shape)\n",
    "\n",
    "sort_test_prob=np.sort(test_prob_array,axis=1)\n",
    "sort_test_logits=np.sort(test_logits_array,axis=1)\n",
    "# sort_test_hidden_layer_logits = np.sort(test_hidden_layer_logits_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "np.mean(np.var(test_prob_array,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss_array\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "# plt.hist(test_loss_array, range = (test_loss_array.min(), 0.05), bins=100, density=True, log=True)\n",
    "plt.hist(test_loss_array, bins=100, density=True, log=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss( reduction = 'none')\n",
    "softmax = nn.Softmax()\n",
    "model.eval()\n",
    "train_loss_list = []\n",
    "train_prob = []\n",
    "train_logits = []\n",
    "train_label = []\n",
    "for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "    # measure data loading time\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "    # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "    outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    outputs_value = outputs.data.cpu().numpy()\n",
    "    loss_value = loss.data.cpu().numpy()\n",
    "    prob_outputs = softmax(outputs)\n",
    "    prob_outputs_value = prob_outputs.data.cpu().numpy()\n",
    "    targets_value = targets.data.cpu().numpy()\n",
    "    \n",
    "    for i in range(len(loss_value)):\n",
    "        train_loss_list.append(loss_value[i])\n",
    "        train_logits.append(outputs_value[i])\n",
    "        train_prob.append(prob_outputs_value[i])\n",
    "        train_label.append(targets_value[i])\n",
    "#     break\n",
    "\n",
    "train_loss_array = np.asarray(train_loss_list)\n",
    "train_prob_array = np.asarray(train_prob)\n",
    "train_logits_array = np.asarray(train_logits)\n",
    "train_label = np.asarray(train_label)\n",
    "# test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(train_prob_array.shape, train_label.shape)\n",
    "\n",
    "sort_train_prob=np.sort(train_prob_array,axis=1)\n",
    "sort_train_logits=np.sort(train_logits_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_prob_array = np.asarray(train_prob)\n",
    "train_logits_array = np.asarray(train_logits)\n",
    "train_label = np.asarray(train_label)\n",
    "# test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(train_prob_array.shape, train_label.shape)\n",
    "\n",
    "sort_train_prob=np.sort(train_prob_array,axis=1)\n",
    "\n",
    "sort_train_logits=np.sort(train_logits_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.mean(np.var(train_prob_array,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "# plt.hist(train_loss_array, range = (train_loss_array.min(), 0.05), bins = 100, density=True)\n",
    "plt.hist(train_loss_array[:10000], bins=100, density=True, log=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = pyplot.figure(figsize=(15,10))\n",
    "pyplot.hist(train_loss_array[:10000], bins=100, density=True, log=True, alpha=0.5, label = 'trainset')\n",
    "pyplot.hist(test_loss_array, bins=100, density=True, log=True, alpha=0.5, label = 'testset')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt\n",
    "# pyplot.plot(predict_origin)\n",
    "# pyplot.plot(f_evaluate_origin[3])\n",
    "\n",
    "for i in range(1000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if test_label[i] == 0:\n",
    "        plt.plot(test_prob_array[i])\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(30,20))\n",
    "count1 = 0\n",
    "for i in range(10000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if test_label[i] == 0:\n",
    "        count1 += 1\n",
    "        axs[0,0].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 1:\n",
    "        axs[0,1].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 2:\n",
    "        axs[1,0].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 3:\n",
    "        axs[1,1].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 4:\n",
    "        axs[2,0].plot(test_prob_array[i], 'g')\n",
    "    if test_label[i] == 5:\n",
    "        axs[2,1].plot(test_prob_array[i], 'g')      \n",
    "    if test_label[i] == 6:\n",
    "        axs[3,0].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 7:\n",
    "        axs[3,1].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 8:\n",
    "        axs[4,0].plot(test_prob_array[i], 'm')\n",
    "    if test_label[i] == 9:\n",
    "        axs[4,1].plot(test_prob_array[i], 'm')  \n",
    "\n",
    "count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(30,20))\n",
    "count1 = 0\n",
    "for i in range(10000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if train_label[i] == 0:\n",
    "        count1 += 1\n",
    "        axs[0,0].plot(train_prob_array[i], 'r')\n",
    "    if train_label[i] == 1:\n",
    "        axs[0,1].plot(train_prob_array[i], 'r')\n",
    "    if train_label[i] == 2:\n",
    "        axs[1,0].plot(train_prob_array[i], 'y')\n",
    "    if train_label[i] == 3:\n",
    "        axs[1,1].plot(train_prob_array[i], 'y')\n",
    "    if train_label[i] == 4:\n",
    "        axs[2,0].plot(train_prob_array[i], 'g')\n",
    "    if train_label[i] == 5:\n",
    "        axs[2,1].plot(train_prob_array[i], 'g')      \n",
    "    if train_label[i] == 6:\n",
    "        axs[3,0].plot(train_prob_array[i], 'b')\n",
    "    if train_label[i] == 7:\n",
    "        axs[3,1].plot(train_prob_array[i], 'b')\n",
    "    if train_label[i] == 8:\n",
    "        axs[4,0].plot(train_prob_array[i], 'm')\n",
    "    if train_label[i] == 9:\n",
    "        axs[4,1].plot(train_prob_array[i], 'm')  \n",
    "\n",
    "count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(10000):\n",
    "    plt.plot(sort_train_prob[i],'r')\n",
    "    plt.plot(sort_test_prob[i],'y')\n",
    "    \n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(10000):\n",
    "#     plt.plot(sort_train_prob[i],'r')\n",
    "    plt.plot(sort_test_prob[i],'y')\n",
    "\n",
    "for i in range(10000):\n",
    "    plt.plot(sort_train_prob[i],'r')\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "\n",
    "for i in range(10000):\n",
    "    plt.plot(sort_train_prob[i],'r')\n",
    "    \n",
    "for i in range(10000):\n",
    "#     plt.plot(sort_train_prob[i],'r')\n",
    "    plt.plot(sort_test_prob[i],'y')\n",
    "\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load for c&w label-only attack\n",
    "\n",
    "def get_max_accuracy(y_true, probs, thresholds=None):\n",
    "    \n",
    "    \"\"\"Return the max accuracy possible given the correct labels and guesses. Will try all thresholds unless passed.\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        y_true: True label of `in' or `out' (member or non-member, 1/0)\n",
    "        probs: The scalar to threshold\n",
    "        thresholds: In a blackbox setup with a shadow/source model, the threshold obtained by the source model can be passed\n",
    "          here for attackin the target model. This threshold will then be used.\n",
    "\n",
    "      Returns: max accuracy possible, accuracy at the threshold passed (if one was passed), the max precision possible,\n",
    "       and the precision at the threshold passed.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    for thresh in thresholds:\n",
    "        accuracy_scores.append(accuracy_score(y_true,\n",
    "                                              [1 if m > thresh else 0 for m in probs]))\n",
    "        precision_scores.append(precision_score(y_true, [1 if m > thresh else 0 for m in probs]))\n",
    "\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    precisions = np.array(precision_scores)\n",
    "    max_accuracy = accuracies.max()\n",
    "    max_precision = precisions.max()\n",
    "    max_accuracy_threshold = thresholds[accuracies.argmax()]\n",
    "    max_precision_threshold = thresholds[precisions.argmax()]\n",
    "    return max_accuracy, max_accuracy_threshold, max_precision, max_precision_threshold\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold(source_m, source_stats, target_m, target_stats):\n",
    "    \"\"\" Train a threshold attack model and get teh accuracy on source and target models.\n",
    "\n",
    "  Args:\n",
    "    source_m: membership labels for source dataset (1 for member, 0 for non-member)\n",
    "    source_stats: scalar values to threshold (attack features) for source dataset\n",
    "    target_m: membership labels for target dataset (1 for member, 0 for non-member)\n",
    "    target_stats: scalar values to threshold (attack features) for target dataset\n",
    "\n",
    "  Returns: best acc from source thresh, precision @ same threshold, threshold for best acc,\n",
    "    precision at the best threshold for precision. all tuned on source model.\n",
    "\n",
    "    \"\"\"\n",
    "    # find best threshold on source data\n",
    "    acc_source, t, prec_source, tprec = get_max_accuracy(source_m, source_stats)\n",
    "\n",
    "    # find best accuracy on test data (just to check how much we overfit)\n",
    "    acc_test, _, prec_test, _ = get_max_accuracy(target_m, target_stats)\n",
    "\n",
    "    # get the test accuracy at the threshold selected on the source data\n",
    "    acc_test_t, _, _, _ = get_max_accuracy(target_m, target_stats, thresholds=[t])\n",
    "    _, _, prec_test_t, _ = get_max_accuracy(target_m, target_stats, thresholds=[tprec])\n",
    "    print(\"acc src: {}, acc test (best thresh): {}, acc test (src thresh): {}, thresh: {}\".format(acc_source, acc_test,\n",
    "                                                                                                acc_test_t, t))\n",
    "    print(\n",
    "    \"prec src: {}, prec test (best thresh): {}, prec test (src thresh): {}, thresh: {}\".format(prec_source, prec_test,\n",
    "                                                                                               prec_test_t, tprec))\n",
    "\n",
    "    return acc_test_t, prec_test_t, t, tprec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "INF = float(\"inf\")\n",
    "\n",
    "\n",
    "def carlini_wagner_l2(\n",
    "    model_fn,\n",
    "    x,\n",
    "    n_classes,\n",
    "    y=None,\n",
    "    targeted=False,\n",
    "    lr=5e-3,\n",
    "    confidence=0,\n",
    "    clip_min=0,\n",
    "    clip_max=1,\n",
    "    initial_const=1e-2,\n",
    "    binary_search_steps=5,\n",
    "    max_iterations=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    This attack was originally proposed by Carlini and Wagner. It is an\n",
    "    iterative attack that finds adversarial examples on many defenses that\n",
    "    are robust to other attacks.\n",
    "    Paper link: https://arxiv.org/abs/1608.04644\n",
    "\n",
    "    At a high level, this attack is an iterative attack using Adam and\n",
    "    a specially-chosen loss function to find adversarial examples with\n",
    "    lower distortion than other attacks. This comes at the cost of speed,\n",
    "    as this attack is often much slower than others.\n",
    "\n",
    "    :param model_fn: a callable that takes an input tensor and returns\n",
    "              the model logits. The logits should be a tensor of shape\n",
    "              (n_examples, n_classes).\n",
    "    :param x: input tensor of shape (n_examples, ...), where ... can\n",
    "              be any arbitrary dimension that is compatible with\n",
    "              model_fn.\n",
    "    :param n_classes: the number of classes.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true,\n",
    "              then provide the target label. Otherwise, only provide\n",
    "              this parameter if you'd like to use true labels when\n",
    "              crafting adversarial samples. Otherwise, model predictions\n",
    "              are used as labels to avoid the \"label leaking\" effect\n",
    "              (explained in this paper:\n",
    "              https://arxiv.org/abs/1611.01236). If provide y, it\n",
    "              should be a 1D tensor of shape (n_examples, ).\n",
    "              Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or\n",
    "              untargeted? Untargeted, the default, will try to make the\n",
    "              label incorrect. Targeted will instead try to move in the\n",
    "              direction of being more like y.\n",
    "    :param lr: (optional) float. The learning rate for the attack\n",
    "              algorithm. Default is 5e-3.\n",
    "    :param confidence: (optional) float. Confidence of adversarial\n",
    "              examples: higher produces examples with larger l2\n",
    "              distortion, but more strongly classified as adversarial.\n",
    "              Default is 0.\n",
    "    :param clip_min: (optional) float. Minimum float value for\n",
    "              adversarial example components. Default is 0.\n",
    "    :param clip_max: (optional) float. Maximum float value for\n",
    "              adversarial example components. Default is 1.\n",
    "    :param initial_const: The initial tradeoff-constant to use to tune the\n",
    "              relative importance of size of the perturbation and\n",
    "              confidence of classification. If binary_search_steps is\n",
    "              large, the initial constant is not important. A smaller\n",
    "              value of this constant gives lower distortion results.\n",
    "              Default is 1e-2.\n",
    "    :param binary_search_steps: (optional) int. The number of times we\n",
    "              perform binary search to find the optimal tradeoff-constant\n",
    "              between norm of the perturbation and confidence of the\n",
    "              classification. Default is 5.\n",
    "    :param max_iterations: (optional) int. The maximum number of\n",
    "              iterations. Setting this to a larger value will produce\n",
    "              lower distortion results. Using only a few iterations\n",
    "              requires a larger learning rate, and will produce larger\n",
    "              distortion results. Default is 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def compare(pred, label, is_logits=False):\n",
    "        \"\"\"\n",
    "        A helper function to compare prediction against a label.\n",
    "        Returns true if the attack is considered successful.\n",
    "\n",
    "        :param pred: can be either a 1D tensor of logits or a predicted\n",
    "                class (int).\n",
    "        :param label: int. A label to compare against.\n",
    "        :param is_logits: (optional) bool. If True, treat pred as an\n",
    "                array of logits. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert logits to predicted class if necessary\n",
    "        if is_logits:\n",
    "            pred_copy = pred.clone().detach()\n",
    "            pred_copy[label] += -confidence if targeted else confidence\n",
    "            pred = torch.argmax(pred_copy)\n",
    "\n",
    "        return pred == label if targeted else pred != label\n",
    "\n",
    "    if y is None:\n",
    "        # Using model predictions as ground truth to avoid label leaking\n",
    "        pred = model_fn(x)\n",
    "        y = torch.argmax(pred, 1)\n",
    "\n",
    "    # Initialize some values needed for binary search on const\n",
    "    lower_bound = [0.0] * len(x)\n",
    "    upper_bound = [1e10] * len(x)\n",
    "    const = x.new_ones(len(x), 1) * initial_const\n",
    "\n",
    "    o_bestl2 = [INF] * len(x)\n",
    "    o_bestscore = [-1.0] * len(x)\n",
    "    x = torch.clamp(x, clip_min, clip_max)\n",
    "    ox = x.clone().detach()  # save the original x\n",
    "    o_bestattack = x.clone().detach()\n",
    "\n",
    "    # Map images into the tanh-space\n",
    "    x = (x - clip_min) / (clip_max - clip_min)\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    x = x * 2 - 1\n",
    "    x = torch.arctanh(x * 0.999999)\n",
    "    # x = torch.atanh(x * 0.999999)\n",
    "\n",
    "    # Prepare some variables\n",
    "    modifier = torch.zeros_like(x, requires_grad=True)\n",
    "    y_onehot = torch.nn.functional.one_hot(y, n_classes).to(torch.float)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    f_fn = lambda real, other, targeted: torch.max(\n",
    "        ((other - real) if targeted else (real - other)) + confidence,\n",
    "        torch.tensor(0.0).to(real.device),\n",
    "    )\n",
    "    l2dist_fn = lambda x, y: torch.pow(x - y, 2).sum(list(range(len(x.size())))[1:])\n",
    "    optimizer = torch.optim.Adam([modifier], lr=lr)\n",
    "\n",
    "    # Outer loop performing binary search on const\n",
    "    for outer_step in range(binary_search_steps):\n",
    "        # Initialize some values needed for the inner loop\n",
    "        bestl2 = [INF] * len(x)\n",
    "        bestscore = [-1.0] * len(x)\n",
    "\n",
    "        # Inner loop performing attack iterations\n",
    "        for i in range(max_iterations):\n",
    "            # One attack step\n",
    "            new_x = (torch.tanh(modifier + x) + 1) / 2\n",
    "            new_x = new_x * (clip_max - clip_min) + clip_min\n",
    "            logits,_,_,_,_,_,_,_ = model_fn(new_x)\n",
    "#             print(logits.shape)\n",
    "#             print(logits[0])\n",
    "#             print(y_onehot.shape)\n",
    "#             print(y_onehot[0])\n",
    "            real = torch.sum(y_onehot * logits, 1)\n",
    "            other, _ = torch.max((1 - y_onehot) * logits - y_onehot * 1e4, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            f = f_fn(real, other, targeted)\n",
    "            l2 = l2dist_fn(new_x, ox)\n",
    "            loss = (const * f + l2).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update best results\n",
    "            for n, (l2_n, logits_n, new_x_n) in enumerate(zip(l2, logits, new_x)):\n",
    "                y_n = y[n]\n",
    "                succeeded = compare(logits_n, y_n, is_logits=True)\n",
    "                if l2_n < o_bestl2[n] and succeeded:\n",
    "                    pred_n = torch.argmax(logits_n)\n",
    "                    o_bestl2[n] = l2_n\n",
    "                    o_bestscore[n] = pred_n\n",
    "                    o_bestattack[n] = new_x_n\n",
    "                    # l2_n < o_bestl2[n] implies l2_n < bestl2[n] so we modify inner loop variables too\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = pred_n\n",
    "                elif l2_n < bestl2[n] and succeeded:\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = torch.argmax(logits_n)\n",
    "\n",
    "        # Binary search step\n",
    "        for n in range(len(x)):\n",
    "            y_n = y[n]\n",
    "\n",
    "            if compare(bestscore[n], y_n) and bestscore[n] != -1:\n",
    "                # Success, divide const by two\n",
    "                upper_bound[n] = min(upper_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "            else:\n",
    "                # Failure, either multiply by 10 if no solution found yet\n",
    "                # or do binary search with the known upper bound\n",
    "                lower_bound[n] = max(lower_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "                else:\n",
    "                    const[n] *= 10\n",
    "\n",
    "    return o_bestattack.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dists(model_fn, dataloader, attack=\"CW\", max_samples=100, input_dim=[None, 32, 32, 3], n_classes=10):\n",
    "    \"\"\"Calculate untargeted distance to decision boundary for Adv-x MI attack.\n",
    "      :param model: model to approximate distances on (attack).\n",
    "      :param ds: tf dataset should be either the training set or the test set.\n",
    "      :param attack: \"CW\" for carlini wagner or \"HSJ\" for hop skip jump\n",
    "      :param max_samples: maximum number of samples to take from the ds\n",
    "      :return: an array of the first samples from the ds, of len max_samples, with the untargeted distances. \n",
    "    \"\"\"\n",
    "#   # switch to TF1 style\n",
    "#   sess = K.get_session()\n",
    "#   x = tf.placeholder(dtype=tf.float32, shape=input_dim)\n",
    "#   y = tf.placeholder(dtype=tf.int32, shape=[None, n_classes])\n",
    "#   output = model_(x)\n",
    "#   model = CallableModelWrapper(lambda x: model_(x), \"logits\")\n",
    "    \n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "    if attack == \"CW\":\n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "                    x_adv_curr = carlini_wagner_l2(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "                                                   10, targeted=False, y=targets[i].reshape(1),clip_min=inputs[i].min(),\n",
    "                                                   clip_max = inputs[i].max())\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "            \n",
    "    elif attack == \"HSJ\":\n",
    "        \n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "#                     stime = time.time()\n",
    "#                     x_adv_curr = hop_skip_jump_attack(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "#                                                       verbose=False,clip_min=inputs[i].min(),clip_max = inputs[i].max())\n",
    "#                     print(i)\n",
    "                    x_adv_curr = hop_skip_jump_attack(model, inputs[i:i+1], \n",
    "                                                      verbose=False,clip_min=inputs[i:i+1].min(),clip_max = inputs[i:i+1].max())\n",
    "#                     print('generate one data takes: ', time.time()-stime)\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unknown attack {}\".format(attack))\n",
    "\n",
    "#   next_element = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return dist_adv[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load attack data\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "# np.random.seed(200)\n",
    "r = np.arange(50000)\n",
    "batch_privacy = 50\n",
    "\n",
    "np.random.shuffle(r)\n",
    "for i in range(1000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,26000): \n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.seed(300)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(1000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,6000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================attack==============================\n",
    "\n",
    "source_train_ds = private_trainloader_intrain \n",
    "target_train_ds = private_trainloader_intest \n",
    "source_test_ds = private_testloader_intrain \n",
    "target_test_ds = private_testloader_intest\n",
    "\n",
    "source_model = model\n",
    "target_model = model\n",
    "\n",
    "input_dim = [None, 32,32,3]\n",
    "max_samples = 1000\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/20) Data: 0.040s | Batch: 0.930s | Loss: 1.9965 | top1:  98.0000 | top5:  100.0000\n",
      "train_loss:  2.0112, train_acc:  93.1000\n",
      "(1/20) Data: 0.042s | Batch: 0.078s | Loss: 1.9976 | top1:  96.0000 | top5:  100.0000\n",
      "test_loss:  2.0072, test_acc:  94.0000\n",
      "(1/20) Data: 0.041s | Batch: 0.082s | Loss: 2.0428 | top1:  80.0000 | top5:  100.0000\n",
      "train_loss:  2.0645, train_acc:  74.8000\n",
      "(1/20) Data: 0.043s | Batch: 0.077s | Loss: 2.0615 | top1:  76.0000 | top5:  100.0000\n",
      "test_loss:  2.0648, test_acc:  74.5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, train_acc1 = test(source_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc1))\n",
    "\n",
    "test_loss, train_acc2 = test(target_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc2))\n",
    "\n",
    "test_loss, test_acc1 = test(source_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc1))\n",
    "\n",
    "test_loss, test_acc2 = test(target_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59.1500, device='cuda:0')\n",
      "tensor(59.7500, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print((train_acc1 - test_acc1)/2 + 50)\n",
    "\n",
    "print((train_acc2 - test_acc2)/2 + 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 1000\n",
    "n_classes = 10\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw1 = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw1 = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw1 = np.concatenate([dists_source_in_cw1, dists_source_out_cw1], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw1 = dists_source_in_cw1\n",
    "dists_target_out_cw1 = dists_source_out_cw1\n",
    "dists_target_cw1 = np.concatenate([dists_target_in_cw1, dists_target_out_cw1], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc11, prec11, _, _ = get_threshold(source_m, dists_source_cw1, target_m, dists_target_cw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 500\n",
    "n_classes = 10\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw = np.concatenate([dists_source_in_cw, dists_source_out_cw], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw = dists_source_in_cw\n",
    "dists_target_out_cw = dists_source_out_cw\n",
    "dists_target_cw = np.concatenate([dists_target_in_cw, dists_target_out_cw], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc1, prec1, _, _ = get_threshold(source_m, dists_source_cw, target_m, dists_target_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(dists_source_in_500, label = 'dists_source_in_500')\n",
    "# plt.plot(dists_source_out_500,  label = 'dists_source_out_500')\n",
    "\n",
    "plt.plot(np.sort(dists_target_in_cw), label = 'dists_target_in_cw')\n",
    "plt.plot(np.sort(dists_target_out_cw),  label = 'dists_target_out_cw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "np.save('dists_target_in_cw.npy', dists_target_in)\n",
    "dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "np.save('dists_target_out_cw.npy', dists_target_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dists_target_in = np.load('dists_target_in_cw.npy')\n",
    "dists_target_out = np.load('dists_target_out_cw.npy')\n",
    "\n",
    "dists_target_in.shape\n",
    "dists_source_cw = np.concatenate([dists_target_in, dists_target_out], axis=0)\n",
    "\n",
    "dists_target_cw = dists_source_cw\n",
    "print(\"threshold on C&W:\")\n",
    "acc1, prec1, _, _ = get_threshold(source_m, dists_source_cw, target_m, dists_target_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dists_source = np.zeros(500)\n",
    "dists_target = np.zeros(1000)\n",
    "\n",
    "for i in range(500):\n",
    "    dists_source[i] = dists_source_in[i].cpu().numpy()\n",
    "#     dists_target[i] = dists_source_out[i].cpu().numpy()\n",
    "    \n",
    "# for i in range(500,1000):\n",
    "#     dists_source[i] = d_target[i].cpu().numpy()\n",
    "#     dists_target[i-500] = d_source[i].cpu().numpy() \n",
    "    \n",
    "dists_source = np.concatenate([dists_source, dists_source_out], axis=0)\n",
    "dists_target = np.concatenate([dists_target_in, dists_target_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"threshold on C&W:\")\n",
    "acc1, prec1, _, _ = get_threshold(source_m, dists_source, target_m, dists_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "labels.0.weight\n",
      "labels.0.weight\n",
      "labels.0.bias\n",
      "labels.2.weight\n",
      "labels.2.weight\n",
      "labels.2.bias\n",
      "combine.0.weight\n",
      "combine.0.weight\n",
      "combine.0.bias\n",
      "combine.2.weight\n",
      "combine.2.weight\n",
      "combine.2.bias\n",
      "combine.4.weight\n",
      "combine.4.weight\n",
      "combine.4.bias\n",
      "combine.6.weight\n",
      "combine.6.weight\n",
      "combine.6.bias\n",
      "combine.8.weight\n",
      "combine.8.weight\n",
      "combine.8.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load membership inference attack\n",
    "\n",
    "inferenece_model = InferenceAttack_HZ(10).cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "optimizer_mem = optim.Adam(inferenece_model.parameters(), lr=at_lr )\n",
    "criterion_attack = nn.MSELoss()\n",
    "best_acc= 0.0\n",
    "batch_size=100\n",
    "epochs= 100\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_train)\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,25000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(25000,50000):\n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "# private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "# private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_tr_attack = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "train_te_attack = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "test_tr_attack = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "test_te_attack = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_nsh(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [30]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_defense='./checkpoints_cifar10/cifar10_NSH_attack_softmax_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "inferenece_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy_test--(0/100) Data: 0.004s | Batch: 0.109s | | Loss: 0.2496 | top1:  0.5000 \n",
      "privacy_test--(10/100) Data: 0.002s | Batch: 0.089s | | Loss: 0.2500 | top1:  0.4964 \n",
      "privacy_test--(20/100) Data: 0.002s | Batch: 0.088s | | Loss: 0.2501 | top1:  0.4926 \n",
      "privacy_test--(30/100) Data: 0.002s | Batch: 0.088s | | Loss: 0.2501 | top1:  0.4918 \n",
      "privacy_test--(40/100) Data: 0.002s | Batch: 0.088s | | Loss: 0.2501 | top1:  0.4956 \n",
      "test acc 0.4954 test_loss:  tensor(0.2501, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "best_acc = 0\n",
    "epochs= 200\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_nsh(optimizer_mem, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_attack_enum = enumerate(zip(train_tr_attack,test_tr_attack))\n",
    "    \n",
    "    train_loss, train_acc = privacy_train_softmax(train_attack_enum,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda, 10000)\n",
    "    \n",
    "    test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "    \n",
    "    print ('train acc', train_acc)\n",
    "    test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "    \n",
    "    is_best = test_acc>best_acc or (1-test_acc)>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    best_acc = max(1-test_acc, best_acc)\n",
    "\n",
    "    \n",
    "    print ('test acc', test_acc, best_acc)\n",
    "\n",
    "    # save model\n",
    "    if is_best:\n",
    "        best_epoch = epoch+1;\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': inferenece_model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer_mem.state_dict(),\n",
    "            }, False, checkpoint=checkpoint_path,filename='cifar10_NSH_attack_softmax_best')\n",
    "        \n",
    "print('model train acc: ', final_train_acc, '  model test acc: ', final_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Defense_Model(\n",
      "    (features): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load NN attack model\n",
    "\n",
    "defense_model = Defense_Model(10)\n",
    "defense_model = torch.nn.DataParallel(defense_model).cuda()\n",
    "defense_criterion = nn.MSELoss()\n",
    "# defense_criterion = nn.CrossEntropyLoss()\n",
    "att_batch_size = 100\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "defense_optimizer = torch.optim.Adam(defense_model.parameters(), lr=at_lr)\n",
    "print(defense_model)\n",
    "best_acc = 0.0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_attack(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [40, 80]:\n",
    "        #     if (epoch+1)%100 == 0:\n",
    "        state['lr'] *= 0.1\n",
    "        #         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/25) Data: 0.003s | Batch: 0.103s | | Loss: 0.2434 | top1:  0.5600 \n",
      "(11/25) Data: 0.002s | Batch: 0.090s | | Loss: 0.2466 | top1:  0.5423 \n",
      "(21/25) Data: 0.002s | Batch: 0.089s | | Loss: 0.2452 | top1:  0.5500 \n",
      "(31/25) Data: 0.002s | Batch: 0.089s | | Loss: 0.2459 | top1:  0.5468 \n",
      "(41/25) Data: 0.002s | Batch: 0.089s | | Loss: 0.2452 | top1:  0.5515 \n",
      "test acc 0.5507 test_loss:  tensor(0.2455, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resume_defense='./checkpoints_cifar10/cifar10_softmax_sort_NN_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "defense_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_attack(defense_optimizer, epoch)\n",
    "\n",
    "    print('\\nEpoch: [%d | %d] , lr : %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_attack_enum = enumerate(zip(train_tr_attack, test_tr_attack))\n",
    "    train_loss, train_acc = train_attack_sort_softmax(train_attack_enum, model,\n",
    "                                                      defense_model, criterion, defense_criterion, optimizer,\n",
    "                                                      defense_optimizer, epoch, use_cuda)\n",
    "\n",
    "    print('train acc:', train_acc)\n",
    "    test_attack_enum = enumerate(zip(train_te_attack, test_te_attack))\n",
    "    test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "# save model\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    if is_best or epoch + 1 == epochs:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': defense_model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': defense_optimizer.state_dict(),\n",
    "        }, False, checkpoint=checkpoint_path, filename='cifar10_softmax_sort_NN_best')\n",
    "\n",
    "    print('test acc', test_acc, best_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load for metric base attack\n",
    "\n",
    "def test_by_class(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    class_count = np.zeros(10)\n",
    "    class_correct = np.zeros(10)\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        _, pred = outputs.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        correct = correct.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        for i in range(len(targets)):\n",
    "            class_count[targets[i]] += 1\n",
    "            class_correct[targets[i]] += correct[0,i]\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg, class_count, class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class black_box_benchmarks(object):\n",
    "    \n",
    "    def __init__(self, shadow_train_performance, shadow_test_performance, \n",
    "                 target_train_performance, target_test_performance, num_classes):\n",
    "        '''\n",
    "        each input contains both model predictions (shape: num_data*num_classes) and ground-truth labels. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.s_tr_outputs, self.s_tr_labels = shadow_train_performance\n",
    "        self.s_te_outputs, self.s_te_labels = shadow_test_performance\n",
    "        self.t_tr_outputs, self.t_tr_labels = target_train_performance\n",
    "        self.t_te_outputs, self.t_te_labels = target_test_performance\n",
    "        \n",
    "        self.s_tr_corr = (np.argmax(self.s_tr_outputs, axis=1)==self.s_tr_labels).astype(int)\n",
    "        self.s_te_corr = (np.argmax(self.s_te_outputs, axis=1)==self.s_te_labels).astype(int)\n",
    "        self.t_tr_corr = (np.argmax(self.t_tr_outputs, axis=1)==self.t_tr_labels).astype(int)\n",
    "        self.t_te_corr = (np.argmax(self.t_te_outputs, axis=1)==self.t_te_labels).astype(int)\n",
    "        \n",
    "        self.s_tr_conf = np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "        self.s_te_conf = np.array([self.s_te_outputs[i, self.s_te_labels[i]] for i in range(len(self.s_te_labels))])\n",
    "        self.t_tr_conf = np.array([self.t_tr_outputs[i, self.t_tr_labels[i]] for i in range(len(self.t_tr_labels))])\n",
    "        self.t_te_conf = np.array([self.t_te_outputs[i, self.t_te_labels[i]] for i in range(len(self.t_te_labels))])\n",
    "        \n",
    "        self.s_tr_entr = self._entr_comp(self.s_tr_outputs)\n",
    "        self.s_te_entr = self._entr_comp(self.s_te_outputs)\n",
    "        self.t_tr_entr = self._entr_comp(self.t_tr_outputs)\n",
    "        self.t_te_entr = self._entr_comp(self.t_te_outputs)\n",
    "        \n",
    "        self.s_tr_m_entr = self._m_entr_comp(self.s_tr_outputs, self.s_tr_labels)\n",
    "        self.s_te_m_entr = self._m_entr_comp(self.s_te_outputs, self.s_te_labels)\n",
    "        self.t_tr_m_entr = self._m_entr_comp(self.t_tr_outputs, self.t_tr_labels)\n",
    "        self.t_te_m_entr = self._m_entr_comp(self.t_te_outputs, self.t_te_labels)\n",
    "        \n",
    "    \n",
    "    def _log_value(self, probs, small_value=1e-30):\n",
    "        return -np.log(np.maximum(probs, small_value))\n",
    "    \n",
    "    def _entr_comp(self, probs):\n",
    "        return np.sum(np.multiply(probs, self._log_value(probs)),axis=1)\n",
    "    \n",
    "    def _m_entr_comp(self, probs, true_labels):\n",
    "        log_probs = self._log_value(probs)\n",
    "        reverse_probs = 1-probs\n",
    "        log_reverse_probs = self._log_value(reverse_probs)\n",
    "        modified_probs = np.copy(probs)\n",
    "        modified_probs[range(true_labels.size), true_labels] = reverse_probs[range(true_labels.size), true_labels]\n",
    "        modified_log_probs = np.copy(log_reverse_probs)\n",
    "        modified_log_probs[range(true_labels.size), true_labels] = log_probs[range(true_labels.size), true_labels]\n",
    "        return np.sum(np.multiply(modified_probs, modified_log_probs),axis=1)\n",
    "    \n",
    "    def _thre_setting(self, tr_values, te_values):\n",
    "        value_list = np.concatenate((tr_values, te_values))\n",
    "        thre, max_acc = 0, 0\n",
    "        for value in value_list:\n",
    "            tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "            te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "            acc = 0.5*(tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        return thre\n",
    "    \n",
    "    def _mem_inf_via_corr(self):\n",
    "        # perform membership inference attack based on whether the input is correctly classified or not\n",
    "        t_tr_acc = np.sum(self.t_tr_corr)/(len(self.t_tr_corr)+0.0)\n",
    "        t_te_acc = np.sum(self.t_te_corr)/(len(self.t_te_corr)+0.0)\n",
    "        mem_inf_acc = 0.5*(t_tr_acc + 1 - t_te_acc)\n",
    "        print('With train acc {acc2:.3f} and test acc {acc3:.3f}\\nFor membership inference attack via correctness, the attack acc is {acc1:.3f} '.format(acc1=mem_inf_acc, acc2=t_tr_acc, acc3=t_te_acc) )\n",
    "        return\n",
    "    \n",
    "    def _mem_inf_thre(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        # perform membership inference attack by thresholding feature values: the feature can be prediction confidence,\n",
    "        # (negative) prediction entropy, and (negative) modified entropy\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        for num in range(self.num_classes):\n",
    "            thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "            t_tr_mem += np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "            t_te_non_mem += np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "        mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "        print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    def _mem_inf_benchmarks(self, all_methods=True, benchmark_methods=[]):\n",
    "        if (all_methods) or ('correctness' in benchmark_methods):\n",
    "            self._mem_inf_via_corr()\n",
    "        if (all_methods) or ('confidence' in benchmark_methods):\n",
    "            self._mem_inf_thre('confidence', self.s_tr_conf, self.s_te_conf, self.t_tr_conf, self.t_te_conf)\n",
    "        if (all_methods) or ('entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('entropy', -self.s_tr_entr, -self.s_te_entr, -self.t_tr_entr, -self.t_te_entr)\n",
    "        if (all_methods) or ('modified entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr, -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load me evaluation\n",
    "\n",
    "# np.random.seed(100)\n",
    "r = np.arange(25000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_trainset_intest.append(trainset[r[i]+25000])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nux219/.conda/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/50) Data: 0.065s | Batch: 0.142s | Loss: 2.0089 | top1:  94.0000 | top5:  100.0000\n",
      "(21/50) Data: 0.004s | Batch: 0.049s | Loss: 2.0088 | top1:  93.1905 | top5:  99.6190\n",
      "(41/50) Data: 0.003s | Batch: 0.046s | Loss: 2.0091 | top1:  93.2439 | top5:  99.5610\n",
      "Classification accuracy: 93.32\n",
      "(1/50) Data: 0.061s | Batch: 0.129s | Loss: 2.0241 | top1:  90.0000 | top5:  100.0000\n",
      "(21/50) Data: 0.004s | Batch: 0.051s | Loss: 2.0094 | top1:  92.5714 | top5:  99.7143\n",
      "(41/50) Data: 0.003s | Batch: 0.049s | Loss: 2.0086 | top1:  92.5122 | top5:  99.5854\n",
      "Classification accuracy: 92.30\n",
      "(1/50) Data: 0.075s | Batch: 0.131s | Loss: 2.0496 | top1:  81.0000 | top5:  97.0000\n",
      "(21/50) Data: 0.005s | Batch: 0.049s | Loss: 2.0660 | top1:  74.0000 | top5:  96.8571\n",
      "(41/50) Data: 0.003s | Batch: 0.047s | Loss: 2.0665 | top1:  74.0732 | top5:  96.6829\n",
      "Classification accuracy: 74.14\n",
      "(1/50) Data: 0.071s | Batch: 0.126s | Loss: 2.0780 | top1:  70.0000 | top5:  94.0000\n",
      "(21/50) Data: 0.005s | Batch: 0.049s | Loss: 2.0657 | top1:  74.3810 | top5:  96.0000\n",
      "(41/50) Data: 0.003s | Batch: 0.047s | Loss: 2.0660 | top1:  74.5854 | top5:  95.9268\n",
      "Classification accuracy: 74.68\n",
      "[1014. 1018.  997.  973. 1000.  955.  986. 1006. 1008. 1043.]\n",
      "[492. 488. 444. 448. 437. 404. 501. 504. 467. 481.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc, s_tr_class, s_tr_correct = test_by_class(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_tr_class, t_tr_correct = test_by_class(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, s_te_class, s_te_correct = test_by_class(private_testloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_te_class, t_te_correct = test_by_class(private_testloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "print( s_tr_class + t_tr_class)\n",
    "print(s_tr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97633136 0.99410609 0.87562688 0.8705036  0.88       0.8408377\n",
      " 0.96855984 0.97614314 0.9375     0.95302013]\n",
      "[0.81  0.903 0.608 0.626 0.633 0.662 0.83  0.778 0.788 0.803]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_acc = (s_tr_correct + t_tr_correct)/(s_tr_class + t_tr_class)\n",
    "print(train_acc)\n",
    "test_acc = (s_te_correct + t_te_correct)/(s_te_class+t_te_class)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "\n",
    "\n",
    "def _model_predictions(model, dataloader):\n",
    "    return_outputs, return_labels = [], []\n",
    "\n",
    "    for (inputs, labels) in dataloader:\n",
    "        return_labels.append(labels.numpy())\n",
    "        outputs,_,_,_,_,_,_,_ = model.forward(inputs.cuda()) \n",
    "        return_outputs.append( softmax_by_row(outputs.data.cpu().numpy()) )\n",
    "    return_outputs = np.concatenate(return_outputs)\n",
    "    return_labels = np.concatenate(return_labels)\n",
    "    return (return_outputs, return_labels)\n",
    "\n",
    "shadow_train_performance = _model_predictions(model, private_trainloader_intrain)\n",
    "shadow_test_performance = _model_predictions(model, private_testloader_intrain)\n",
    "\n",
    "target_train_performance = _model_predictions(model, private_trainloader_intest)\n",
    "target_test_performance = _model_predictions(model, private_testloader_intest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform membership inference attacks!!!\n",
      "With train acc 0.922 and test acc 0.746\n",
      "For membership inference attack via correctness, the attack acc is 0.588 \n",
      "For membership inference attack via confidence, the attack acc is 0.600\n",
      "For membership inference attack via entropy, the attack acc is 0.553\n",
      "For membership inference attack via modified entropy, the attack acc is 0.600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Perform membership inference attacks!!!')\n",
    "MIA = black_box_benchmarks(shadow_train_performance,shadow_test_performance,\n",
    "                     target_train_performance,target_test_performance,num_classes=10)\n",
    "res = MIA._mem_inf_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from privacy_risk_score_utils import *\n",
    "\n",
    "risk_score = calculate_risk_score(MIA.s_tr_m_entr, MIA.s_te_m_entr, MIA.s_tr_labels, MIA.s_te_labels, MIA.t_tr_m_entr, MIA.t_tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
