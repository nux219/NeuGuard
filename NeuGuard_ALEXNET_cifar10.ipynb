{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from hop_skip_jump_attack import hop_skip_jump_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# torch.cuda.set_device(2)\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "global best_acc\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_fc(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_fc, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "#         x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_fc(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_fc(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_conv(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_conv, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.q, alpha = self.alpha)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.q, alpha = self.alpha)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.q, alpha = self.alpha)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.q, alpha = self.alpha)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "#         h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "#         h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value = np.percentile(nonzero, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_conv(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_conv(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv = 100,qfc = 100, aconv = 1, afc = 1):\n",
    "        super(AlexNet_comb, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv = qconv\n",
    "        self.qfc = qfc\n",
    "        self.aconv = aconv\n",
    "        self.afc = afc\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.qconv, alpha = self.aconv)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.qconv, alpha = self.aconv)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.qconv, alpha = self.aconv)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.qconv, alpha = self.aconv)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.qconv, alpha = self.aconv)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.qfc, alpha = self.afc)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.qfc, alpha = self.afc)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "#     print('q: ', q, 'percentile_value: ', percentile_value)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_comb(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_comb_mid(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv_l = 100,qconv_h = 100,qfc_l = 100,qfc_h = 100, aconv_l = 1,aconv_h = 1, afc_l = 1, afc_h = 1):\n",
    "        super(AlexNet_comb_mid, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv_l = qconv_l\n",
    "        self.qconv_h = qconv_h\n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.aconv_l = aconv_l\n",
    "        self.aconv_h = aconv_h\n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage_mid(h1, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage_mid(h2, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage_mid(h3, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage_mid(h4, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage_mid(x, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage_mid(h6, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage_mid(h7, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "#     p_value_low = np.percentile(flattened_weights, q_l)\n",
    "#     p_value_high = np.percentile(flattened_weights, q_h)\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    p_value_low = np.percentile(nonzero, q_l)\n",
    "    p_value_high = np.percentile(nonzero, q_h)\n",
    "#     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "        \n",
    "def alexnet_comb_mid(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb_mid(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_mod(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_mod, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.lin4 = nn.Linear(256*1*1, 10)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "        out_h5 = self.lin4(torch.flatten(h5, 1))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7, out_h5\n",
    "\n",
    "def alexnet_mod(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_mod(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(num_classes,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.labels=nn.Sequential(\n",
    "#            nn.Linear(num_classes,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.ReLU(),\n",
    "#             )\n",
    "            nn.Linear(num_classes,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.combine=nn.Sequential(\n",
    "#             nn.Linear(64*2,256),\n",
    "            nn.Linear(64*2,512),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        for key in self.state_dict():\n",
    "            print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "                print (key)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x,l):\n",
    "        \n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(l)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        is_member =self.combine( torch.cat((out_x  ,out_l),1))\n",
    "        \n",
    "        \n",
    "        return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defense model\n",
    "class Defense_Model(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Defense_Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(num_classes, 256),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        is_member = self.features(x)\n",
    "        return self.output(is_member), is_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "        diff_top = torch.mean(sort_diff_top) * alpha\n",
    "    #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "    #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_top.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_one(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_fc_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "#         fh1 = torch.sum(torch.sum(h1,2),2)\n",
    "#         fh2 = torch.sum(torch.sum(h2,2),2)\n",
    "#         fh3 = torch.sum(torch.sum(h3,2),2)\n",
    "#         fh4 = torch.sum(torch.sum(h4,2),2)\n",
    "#         fh5 = torch.sum(torch.sum(h5,2),2)\n",
    "        \n",
    "        out_list = [h6, h7, outputs]\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "# #         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "# #         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "#         sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "# #         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "# #         var = torch.mean(sort_var)*beta\n",
    "\n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "# #         var = sort_var * beta\n",
    "\n",
    "# #         s_var = sort_var * beta\n",
    "# #         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "# #         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "# #         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "# #         low8_loss = -torch.mean(low8_var) * alpha\n",
    "#         sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "#         diff_top = torch.mean(sort_diff_top) * alpha\n",
    "#     #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "#     #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# h2n = torch.norm(h2, dim = (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs ,_,_,_,_,_,_,_,_ = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            if epoch == 9:\n",
    "                print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "                print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train_softmax(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs) #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        \n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test_softmax(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs)#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "#             if epoch == 9:\n",
    "#                 print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#                 print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_sort_softmax(trainloader, model, attack_model, criterion,\n",
    "                              attack_criterion, optimizer, attack_optimizer, epoch, use_cuda, num_batchs=100000,\n",
    "                              skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // att_batch_size), (len(train_data) // att_batch_size)) + 1\n",
    "\n",
    "    # print (skip_batch, len_t)\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_  = model(inputs_attack)\n",
    "\n",
    "        # classifier_input = torch.cat((inputs, inputs_attack))\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        #         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "\n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        # print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_sort_softmax(testloader, model, attack_model, criterion, attack_criterion,\n",
    "                             optimizer, attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    sum_correct = 0.0\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // batch_size), (len(train_data) // batch_size)) + 1\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in testloader:\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_ = model(inputs_attack)\n",
    "\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        # attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        correct = np.sum(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        sum_correct += correct\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "\n",
    "    #         break\n",
    "    #     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "\n",
    "    return (losses.avg, top1.avg, sum_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset='cifar10'\n",
    "checkpoint_path='./checkpoints_cifar10'\n",
    "train_batch=200\n",
    "test_batch=100\n",
    "lr=0.001\n",
    "epochs=60\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))\n",
    "        \n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    mkdir_p(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# global best_acc\n",
    "start_epoch = 0  # start_ from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing dataset %s' % dataset)\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "\n",
    "# prepare test data parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mean = [0]\n",
    "# std = [1]\n",
    "# transform_test = transforms.Compose(\n",
    "#     [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    dataloader = datasets.CIFAR10\n",
    "    num_classes = 10\n",
    "else:\n",
    "    dataloader = datasets.CIFAR100\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> creating model \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"==> creating model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AlexNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = model.cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 20.13M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_attack = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_admm = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Resume\n",
    "title = 'cifar-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainset_private = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "trainloader_private = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_privacy=200\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_test)\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(5000):\n",
    "#     private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "# for i in range(15000,30000):\n",
    "#     private_trainset_intest.append(trainset[r[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "r = np.arange(50000)\n",
    "for i in range(5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,30000):\n",
    "    private_testset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.shuffle(r)\n",
    "\n",
    "for i in range(5000):\n",
    "    private_trainset_intest.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_cifar10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# start train\n",
    "dataset='cifar10'\n",
    "checkpoint_path='./checkpoints_cifar10'\n",
    "# train_batch=400\n",
    "# test_batch=200\n",
    "lr=0.01\n",
    "epochs=100\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)\n",
    "model = AlexNet(num_classes)\n",
    "# model = AlexNet_comb_mid(num_classes, qconv_l = 60,qconv_h = 90,qfc_l = 80,qfc_h = 90,\n",
    "#                          aconv_l = 2.2,aconv_h = 1.2, afc_l = 1, afc_h = 1.2)\n",
    "# model = AlexNet_b(num_classes)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (re1): ReLU(inplace=True)\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re2): ReLU(inplace=True)\n",
      "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (re5): ReLU(inplace=True)\n",
      "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (d1): Dropout(p=0.5, inplace=False)\n",
      "  (lin1): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (re6): ReLU(inplace=True)\n",
      "  (d2): Dropout(p=0.5, inplace=False)\n",
      "  (lin2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (re7): ReLU(inplace=True)\n",
      "  (lin3): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [60]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "#     elif epoch == 60:\n",
    "#         state['lr'] = 0.005\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# code for training defenseive NeuGuard model\n",
    "\n",
    "\n",
    "#  with random clip\n",
    "is_best=False\n",
    "best_acc=0.0\n",
    "start_epoch=0\n",
    "alpha = 0\n",
    "beta =250\n",
    "num_class = 10\n",
    "epochs=100\n",
    "mean_class = np.zeros((num_class,num_class))\n",
    "mean_class_h5 = np.zeros((num_class,num_class))\n",
    "var_n = np.zeros(num_class)\n",
    "\n",
    "test_acc_res = []\n",
    "test_loss_res = []\n",
    "train_acc_res = []\n",
    "train_loss_res = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "#     adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "#     train_enum = enumerate(trainloader)\n",
    "#     train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "    train_loss,train_var,train_svar, train_acc = train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=num_class, alpha = alpha, beta = beta)\n",
    "\n",
    "    train_acc_res.append(train_acc.item())\n",
    "    train_loss_res.append(train_loss.item())\n",
    "    \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "    test_acc_res.append(test_acc.item())\n",
    "    test_loss_res.append(test_loss.item())\n",
    "    \n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}, best_acc: {best_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc, best_acc=best_acc))\n",
    "    \n",
    "#     # save model\n",
    "#     if test_acc > 70:\n",
    "#         if (is_best) or epoch+1 == epochs:\n",
    "#             save_checkpoint({\n",
    "#                     'epoch': epoch + 1,\n",
    "#                     'state_dict': model.state_dict(),\n",
    "#                     'acc': test_acc,\n",
    "#                     'best_acc': best_acc,\n",
    "#                     'optimizer' : optimizer.state_dict(),\n",
    "#                 }, is_best, checkpoint=checkpoint_path,filename='cifar10_min_var_beta250_epoch%d'%(epoch+1))\n",
    "\n",
    "    \n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n",
    "test_loss, test_acc = test(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc))\n",
    "\n",
    "test_loss, test_acc = test(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from matplotlib import pyplot\n",
    "# from matplotlib.pylab import plt\n",
    "\n",
    "# for i in range(len(train_acc_res)):\n",
    "#     plt.plot(train_acc_res[i])\n",
    "#     plt.plot(test_acc_res[i])\n",
    "    \n",
    "# # plt.ylim(0, 0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "# model = AlexNet(num_classes)\n",
    "# model = AlexNet_fc(num_classes, q = 70, alpha = 5)\n",
    "# model = AlexNet_conv(num_classes, q = 65, alpha = 1.5)\n",
    "# model = AlexNet_comb(num_classes, qconv = 100, qfc = 50, aconv = 1, afc = 20)\n",
    "model = AlexNet_comb_mid(num_classes, qconv_l = 100,qconv_h = 65,qfc_l = 100,qfc_h = 100,\n",
    "                         aconv_l = 1,aconv_h = 1.5, afc_l = 1, afc_h = 1)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training and testing the attack result from the checkpoint\n",
    "\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta200_conv60a2.2_90a1.2_fc90a1.2_defense_epoch69'\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta200_alpha2_conv90_defense_epoch95'\n",
    "resume='./checkpoints_cifar10/cifar10_min_var_beta200_conv90a1.5_nonz_defense_epoch68'\n",
    "# resume='./checkpoints_cifar10/cifar10_min_var_beta250_defense_epoch66'\n",
    "# resume='./checkpoints_cifar10/cifar10_base_epoch79'\n",
    "# resume='./checkpoints_cifar10/cifar10_alexnet_100_epoch14'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load(resume)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer']) \n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/298495222.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/100) Data: 0.097s | Batch: 0.185s | Loss: 2.0511 | top1:  83.0000 | top5:  99.0000\n",
      "(21/100) Data: 0.006s | Batch: 0.062s | Loss: 2.0633 | top1:  76.3333 | top5:  96.0952\n",
      "(41/100) Data: 0.004s | Batch: 0.059s | Loss: 2.0661 | top1:  74.8537 | top5:  96.2439\n",
      "(61/100) Data: 0.003s | Batch: 0.059s | Loss: 2.0652 | top1:  74.9672 | top5:  96.4098\n",
      "(81/100) Data: 0.003s | Batch: 0.058s | Loss: 2.0656 | top1:  74.5556 | top5:  96.5432\n",
      "Classification accuracy: 74.31\n",
      "(1/500) Data: 0.105s | Batch: 0.267s | Loss: 1.9931 | top1:  97.0000 | top5:  100.0000\n",
      "(21/500) Data: 0.006s | Batch: 0.066s | Loss: 2.0111 | top1:  92.9048 | top5:  99.5238\n",
      "(41/500) Data: 0.004s | Batch: 0.063s | Loss: 2.0100 | top1:  92.9756 | top5:  99.5366\n",
      "(61/500) Data: 0.003s | Batch: 0.060s | Loss: 2.0099 | top1:  92.7377 | top5:  99.5246\n",
      "(81/500) Data: 0.003s | Batch: 0.059s | Loss: 2.0096 | top1:  92.8148 | top5:  99.5556\n",
      "(101/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0091 | top1:  92.9010 | top5:  99.5743\n",
      "(121/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0091 | top1:  93.0578 | top5:  99.6033\n",
      "(141/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0091 | top1:  92.9716 | top5:  99.5816\n",
      "(161/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9938 | top5:  99.5590\n",
      "(181/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0092 | top1:  92.9779 | top5:  99.5414\n",
      "(201/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0091 | top1:  93.0249 | top5:  99.5522\n",
      "(221/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0087 | top1:  93.0679 | top5:  99.5566\n",
      "(241/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0089 | top1:  93.0249 | top5:  99.5560\n",
      "(261/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0090 | top1:  93.0153 | top5:  99.5556\n",
      "(281/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0090 | top1:  92.9573 | top5:  99.5445\n",
      "(301/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0092 | top1:  92.9036 | top5:  99.5515\n",
      "(321/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0091 | top1:  92.9439 | top5:  99.5608\n",
      "(341/500) Data: 0.002s | Batch: 0.057s | Loss: 2.0093 | top1:  92.9150 | top5:  99.5543\n",
      "(361/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9224 | top5:  99.5679\n",
      "(381/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0094 | top1:  92.8950 | top5:  99.5617\n",
      "(401/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9152 | top5:  99.5636\n",
      "(421/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.9026 | top5:  99.5653\n",
      "(441/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.8798 | top5:  99.5669\n",
      "(461/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0093 | top1:  92.8655 | top5:  99.5683\n",
      "(481/500) Data: 0.002s | Batch: 0.058s | Loss: 2.0092 | top1:  92.8836 | top5:  99.5613\n",
      "Trainset Classification accuracy: 92.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "train_loss, final_train_acc = test(trainloader, model, criterion, epoch, use_cuda)\n",
    "print ('Trainset Classification accuracy: %.2f'%(final_train_acc))\n",
    "\n",
    "# q:  75 percentile_value:  0.3447321504354477\n",
    "# q:  75 percentile_value:  0.18002179265022278\n",
    "# q:  75 percentile_value:  0.01930836820974946\n",
    "# q:  75 percentile_value:  0.01867781998589635\n",
    "# q:  75 percentile_value:  0.005449312971904874\n",
    "# q:  50 percentile_value:  0.0015738015063107014\n",
    "# q:  50 percentile_value:  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load for c&w label-only attack\n",
    "\n",
    "def get_max_accuracy(y_true, probs, thresholds=None):\n",
    "    \n",
    "    \"\"\"Return the max accuracy possible given the correct labels and guesses. Will try all thresholds unless passed.\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        y_true: True label of `in' or `out' (member or non-member, 1/0)\n",
    "        probs: The scalar to threshold\n",
    "        thresholds: In a blackbox setup with a shadow/source model, the threshold obtained by the source model can be passed\n",
    "          here for attackin the target model. This threshold will then be used.\n",
    "\n",
    "      Returns: max accuracy possible, accuracy at the threshold passed (if one was passed), the max precision possible,\n",
    "       and the precision at the threshold passed.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    for thresh in thresholds:\n",
    "        accuracy_scores.append(accuracy_score(y_true,\n",
    "                                              [1 if m > thresh else 0 for m in probs]))\n",
    "        precision_scores.append(precision_score(y_true, [1 if m > thresh else 0 for m in probs]))\n",
    "\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    precisions = np.array(precision_scores)\n",
    "    max_accuracy = accuracies.max()\n",
    "    max_precision = precisions.max()\n",
    "    max_accuracy_threshold = thresholds[accuracies.argmax()]\n",
    "    max_precision_threshold = thresholds[precisions.argmax()]\n",
    "    return max_accuracy, max_accuracy_threshold, max_precision, max_precision_threshold\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold(source_m, source_stats, target_m, target_stats):\n",
    "    \"\"\" Train a threshold attack model and get teh accuracy on source and target models.\n",
    "\n",
    "  Args:\n",
    "    source_m: membership labels for source dataset (1 for member, 0 for non-member)\n",
    "    source_stats: scalar values to threshold (attack features) for source dataset\n",
    "    target_m: membership labels for target dataset (1 for member, 0 for non-member)\n",
    "    target_stats: scalar values to threshold (attack features) for target dataset\n",
    "\n",
    "  Returns: best acc from source thresh, precision @ same threshold, threshold for best acc,\n",
    "    precision at the best threshold for precision. all tuned on source model.\n",
    "\n",
    "    \"\"\"\n",
    "    # find best threshold on source data\n",
    "    acc_source, t, prec_source, tprec = get_max_accuracy(source_m, source_stats)\n",
    "\n",
    "    # find best accuracy on test data (just to check how much we overfit)\n",
    "    acc_test, _, prec_test, _ = get_max_accuracy(target_m, target_stats)\n",
    "\n",
    "    # get the test accuracy at the threshold selected on the source data\n",
    "    acc_test_t, _, _, _ = get_max_accuracy(target_m, target_stats, thresholds=[t])\n",
    "    _, _, prec_test_t, _ = get_max_accuracy(target_m, target_stats, thresholds=[tprec])\n",
    "    print(\"acc src: {}, acc test (best thresh): {}, acc test (src thresh): {}, thresh: {}\".format(acc_source, acc_test,\n",
    "                                                                                                acc_test_t, t))\n",
    "    print(\n",
    "    \"prec src: {}, prec test (best thresh): {}, prec test (src thresh): {}, thresh: {}\".format(prec_source, prec_test,\n",
    "                                                                                               prec_test_t, tprec))\n",
    "\n",
    "    return acc_test_t, prec_test_t, t, tprec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#  max_iterations is set to be 100 to make the attack faster, the orginal results are done with max_iterations=1000\n",
    "\n",
    "INF = float(\"inf\")\n",
    "\n",
    "\n",
    "def carlini_wagner_l2(\n",
    "    model_fn,\n",
    "    x,\n",
    "    n_classes,\n",
    "    y=None,\n",
    "    targeted=False,\n",
    "    lr=5e-3,\n",
    "    confidence=0,\n",
    "    clip_min=0,\n",
    "    clip_max=1,\n",
    "    initial_const=1e-2,\n",
    "    binary_search_steps=5,\n",
    "    max_iterations=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    This attack was originally proposed by Carlini and Wagner. It is an\n",
    "    iterative attack that finds adversarial examples on many defenses that\n",
    "    are robust to other attacks.\n",
    "    Paper link: https://arxiv.org/abs/1608.04644\n",
    "\n",
    "    At a high level, this attack is an iterative attack using Adam and\n",
    "    a specially-chosen loss function to find adversarial examples with\n",
    "    lower distortion than other attacks. This comes at the cost of speed,\n",
    "    as this attack is often much slower than others.\n",
    "\n",
    "    :param model_fn: a callable that takes an input tensor and returns\n",
    "              the model logits. The logits should be a tensor of shape\n",
    "              (n_examples, n_classes).\n",
    "    :param x: input tensor of shape (n_examples, ...), where ... can\n",
    "              be any arbitrary dimension that is compatible with\n",
    "              model_fn.\n",
    "    :param n_classes: the number of classes.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true,\n",
    "              then provide the target label. Otherwise, only provide\n",
    "              this parameter if you'd like to use true labels when\n",
    "              crafting adversarial samples. Otherwise, model predictions\n",
    "              are used as labels to avoid the \"label leaking\" effect\n",
    "              (explained in this paper:\n",
    "              https://arxiv.org/abs/1611.01236). If provide y, it\n",
    "              should be a 1D tensor of shape (n_examples, ).\n",
    "              Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or\n",
    "              untargeted? Untargeted, the default, will try to make the\n",
    "              label incorrect. Targeted will instead try to move in the\n",
    "              direction of being more like y.\n",
    "    :param lr: (optional) float. The learning rate for the attack\n",
    "              algorithm. Default is 5e-3.\n",
    "    :param confidence: (optional) float. Confidence of adversarial\n",
    "              examples: higher produces examples with larger l2\n",
    "              distortion, but more strongly classified as adversarial.\n",
    "              Default is 0.\n",
    "    :param clip_min: (optional) float. Minimum float value for\n",
    "              adversarial example components. Default is 0.\n",
    "    :param clip_max: (optional) float. Maximum float value for\n",
    "              adversarial example components. Default is 1.\n",
    "    :param initial_const: The initial tradeoff-constant to use to tune the\n",
    "              relative importance of size of the perturbation and\n",
    "              confidence of classification. If binary_search_steps is\n",
    "              large, the initial constant is not important. A smaller\n",
    "              value of this constant gives lower distortion results.\n",
    "              Default is 1e-2.\n",
    "    :param binary_search_steps: (optional) int. The number of times we\n",
    "              perform binary search to find the optimal tradeoff-constant\n",
    "              between norm of the perturbation and confidence of the\n",
    "              classification. Default is 5.\n",
    "    :param max_iterations: (optional) int. The maximum number of\n",
    "              iterations. Setting this to a larger value will produce\n",
    "              lower distortion results. Using only a few iterations\n",
    "              requires a larger learning rate, and will produce larger\n",
    "              distortion results. Default is 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def compare(pred, label, is_logits=False):\n",
    "        \"\"\"\n",
    "        A helper function to compare prediction against a label.\n",
    "        Returns true if the attack is considered successful.\n",
    "\n",
    "        :param pred: can be either a 1D tensor of logits or a predicted\n",
    "                class (int).\n",
    "        :param label: int. A label to compare against.\n",
    "        :param is_logits: (optional) bool. If True, treat pred as an\n",
    "                array of logits. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert logits to predicted class if necessary\n",
    "        if is_logits:\n",
    "            pred_copy = pred.clone().detach()\n",
    "            pred_copy[label] += -confidence if targeted else confidence\n",
    "            pred = torch.argmax(pred_copy)\n",
    "\n",
    "        return pred == label if targeted else pred != label\n",
    "\n",
    "    if y is None:\n",
    "        # Using model predictions as ground truth to avoid label leaking\n",
    "        pred = model_fn(x)\n",
    "        y = torch.argmax(pred, 1)\n",
    "\n",
    "    # Initialize some values needed for binary search on const\n",
    "    lower_bound = [0.0] * len(x)\n",
    "    upper_bound = [1e10] * len(x)\n",
    "    const = x.new_ones(len(x), 1) * initial_const\n",
    "\n",
    "    o_bestl2 = [INF] * len(x)\n",
    "    o_bestscore = [-1.0] * len(x)\n",
    "    x = torch.clamp(x, clip_min, clip_max)\n",
    "    ox = x.clone().detach()  # save the original x\n",
    "    o_bestattack = x.clone().detach()\n",
    "\n",
    "    # Map images into the tanh-space\n",
    "    x = (x - clip_min) / (clip_max - clip_min)\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    x = x * 2 - 1\n",
    "    x = torch.arctanh(x * 0.999999)\n",
    "    # x = torch.atanh(x * 0.999999)\n",
    "\n",
    "    # Prepare some variables\n",
    "    modifier = torch.zeros_like(x, requires_grad=True)\n",
    "    y_onehot = torch.nn.functional.one_hot(y, n_classes).to(torch.float)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    f_fn = lambda real, other, targeted: torch.max(\n",
    "        ((other - real) if targeted else (real - other)) + confidence,\n",
    "        torch.tensor(0.0).to(real.device),\n",
    "    )\n",
    "    l2dist_fn = lambda x, y: torch.pow(x - y, 2).sum(list(range(len(x.size())))[1:])\n",
    "    optimizer = torch.optim.Adam([modifier], lr=lr)\n",
    "\n",
    "    # Outer loop performing binary search on const\n",
    "    for outer_step in range(binary_search_steps):\n",
    "        # Initialize some values needed for the inner loop\n",
    "        bestl2 = [INF] * len(x)\n",
    "        bestscore = [-1.0] * len(x)\n",
    "\n",
    "        # Inner loop performing attack iterations\n",
    "        for i in range(max_iterations):\n",
    "            # One attack step\n",
    "            new_x = (torch.tanh(modifier + x) + 1) / 2\n",
    "            new_x = new_x * (clip_max - clip_min) + clip_min\n",
    "            logits,_,_,_,_,_,_,_ = model_fn(new_x)\n",
    "#             print(logits.shape)\n",
    "#             print(logits[0])\n",
    "#             print(y_onehot.shape)\n",
    "#             print(y_onehot[0])\n",
    "            real = torch.sum(y_onehot * logits, 1)\n",
    "            other, _ = torch.max((1 - y_onehot) * logits - y_onehot * 1e4, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            f = f_fn(real, other, targeted)\n",
    "            l2 = l2dist_fn(new_x, ox)\n",
    "            loss = (const * f + l2).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update best results\n",
    "            for n, (l2_n, logits_n, new_x_n) in enumerate(zip(l2, logits, new_x)):\n",
    "                y_n = y[n]\n",
    "                succeeded = compare(logits_n, y_n, is_logits=True)\n",
    "                if l2_n < o_bestl2[n] and succeeded:\n",
    "                    pred_n = torch.argmax(logits_n)\n",
    "                    o_bestl2[n] = l2_n\n",
    "                    o_bestscore[n] = pred_n\n",
    "                    o_bestattack[n] = new_x_n\n",
    "                    # l2_n < o_bestl2[n] implies l2_n < bestl2[n] so we modify inner loop variables too\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = pred_n\n",
    "                elif l2_n < bestl2[n] and succeeded:\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = torch.argmax(logits_n)\n",
    "\n",
    "        # Binary search step\n",
    "        for n in range(len(x)):\n",
    "            y_n = y[n]\n",
    "\n",
    "            if compare(bestscore[n], y_n) and bestscore[n] != -1:\n",
    "                # Success, divide const by two\n",
    "                upper_bound[n] = min(upper_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "            else:\n",
    "                # Failure, either multiply by 10 if no solution found yet\n",
    "                # or do binary search with the known upper bound\n",
    "                lower_bound[n] = max(lower_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "                else:\n",
    "                    const[n] *= 10\n",
    "\n",
    "    return o_bestattack.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dists(model_fn, dataloader, attack=\"CW\", max_samples=100, input_dim=[None, 32, 32, 3], n_classes=10):\n",
    "    \"\"\"Calculate untargeted distance to decision boundary for Adv-x MI attack.\n",
    "      :param model: model to approximate distances on (attack).\n",
    "      :param ds: tf dataset should be either the training set or the test set.\n",
    "      :param attack: \"CW\" for carlini wagner or \"HSJ\" for hop skip jump\n",
    "      :param max_samples: maximum number of samples to take from the ds\n",
    "      :return: an array of the first samples from the ds, of len max_samples, with the untargeted distances. \n",
    "    \"\"\"\n",
    "#   # switch to TF1 style\n",
    "#   sess = K.get_session()\n",
    "#   x = tf.placeholder(dtype=tf.float32, shape=input_dim)\n",
    "#   y = tf.placeholder(dtype=tf.int32, shape=[None, n_classes])\n",
    "#   output = model_(x)\n",
    "#   model = CallableModelWrapper(lambda x: model_(x), \"logits\")\n",
    "    \n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "    if attack == \"CW\":\n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "                    x_adv_curr = carlini_wagner_l2(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "                                                   10, targeted=False, y=targets[i].reshape(1),clip_min=inputs[i].min(),\n",
    "                                                   clip_max = inputs[i].max())\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "            if num_samples > max_samples:\n",
    "                return dist_adv[:max_samples]\n",
    "            \n",
    "    elif attack == \"HSJ\":\n",
    "        \n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "#                     stime = time.time()\n",
    "#                     x_adv_curr = hop_skip_jump_attack(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "#                                                       verbose=False,clip_min=inputs[i].min(),clip_max = inputs[i].max())\n",
    "#                     print(i)\n",
    "                    x_adv_curr = hop_skip_jump_attack(model, inputs[i:i+1], \n",
    "                                                      verbose=False,clip_min=inputs[i:i+1].min(),clip_max = inputs[i:i+1].max())\n",
    "#                     print('generate one data takes: ', time.time()-stime)\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unknown attack {}\".format(attack))\n",
    "\n",
    "#   next_element = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return dist_adv[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load attack data\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "# np.random.seed(200)\n",
    "r = np.arange(50000)\n",
    "batch_privacy = 50\n",
    "\n",
    "np.random.shuffle(r)\n",
    "for i in range(1000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,26000): \n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.seed(300)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(1000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,6000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================attack==============================\n",
    "\n",
    "source_train_ds = private_trainloader_intrain \n",
    "target_train_ds = private_trainloader_intest \n",
    "source_test_ds = private_testloader_intrain \n",
    "target_test_ds = private_testloader_intest\n",
    "\n",
    "source_model = model\n",
    "target_model = model\n",
    "\n",
    "input_dim = [None, 32,32,3]\n",
    "max_samples = 1000\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/298495222.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/20) Data: 0.050s | Batch: 0.094s | Loss: 2.0233 | top1:  90.0000 | top5:  100.0000\n",
      "train_loss:  2.0098, train_acc:  93.7000\n",
      "(1/20) Data: 0.045s | Batch: 0.084s | Loss: 2.0137 | top1:  92.0000 | top5:  100.0000\n",
      "test_loss:  2.0096, test_acc:  91.9000\n",
      "(1/20) Data: 0.046s | Batch: 0.088s | Loss: 2.0869 | top1:  74.0000 | top5:  98.0000\n",
      "train_loss:  2.0711, train_acc:  72.5000\n",
      "(1/20) Data: 0.055s | Batch: 0.092s | Loss: 2.0993 | top1:  66.0000 | top5:  92.0000\n",
      "test_loss:  2.0613, test_acc:  76.2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, train_acc1 = test(source_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc1))\n",
    "\n",
    "test_loss, train_acc2 = test(target_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc2))\n",
    "\n",
    "test_loss, test_acc1 = test(source_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc1))\n",
    "\n",
    "test_loss, test_acc2 = test(target_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60.6000, device='cuda:0')\n",
      "tensor(57.8500, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print((train_acc1 - test_acc1)/2 + 50)\n",
    "\n",
    "print((train_acc2 - test_acc2)/2 + 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/2863644652.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 50 examples\n",
      "processed 100 examples\n",
      "processed 50 examples\n",
      "processed 100 examples\n",
      "threshold on C&W:\n",
      "acc src: 0.51, acc test (best thresh): 0.51, acc test (src thresh): 0.51, thresh: 0.19540216962873633\n",
      "prec src: 0.5714285714285714, prec test (best thresh): 0.5714285714285714, prec test (src thresh): 0.5714285714285714, thresh: 0.19540216962873633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTICE:  max_samples is set to be 50 to do a quick attack, launch attack on 1000 samples will take a long time\n",
    "# max_samples=1000 \n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 50\n",
    "n_classes = 10\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw1 = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw1 = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw1 = np.concatenate([dists_source_in_cw1, dists_source_out_cw1], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw1 = dists_source_in_cw1\n",
    "dists_target_out_cw1 = dists_source_out_cw1\n",
    "dists_target_cw1 = np.concatenate([dists_target_in_cw1, dists_target_out_cw1], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc11, prec11, _, _ = get_threshold(source_m, dists_source_cw1, target_m, dists_target_cw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed 50 examples\n",
    "# processed 100 examples\n",
    "# processed 50 examples\n",
    "# processed 100 examples\n",
    "# threshold on C&W:\n",
    "# acc src: 0.53, acc test (best thresh): 0.53, acc test (src thresh): 0.53, thresh: 0.20805074739664173\n",
    "# prec src: 0.7142857142857143, prec test (best thresh): 0.7142857142857143, prec test (src thresh): 0.7142857142857143, thresh: 0.20805074739664173\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "labels.0.weight\n",
      "labels.0.weight\n",
      "labels.0.bias\n",
      "labels.2.weight\n",
      "labels.2.weight\n",
      "labels.2.bias\n",
      "combine.0.weight\n",
      "combine.0.weight\n",
      "combine.0.bias\n",
      "combine.2.weight\n",
      "combine.2.weight\n",
      "combine.2.bias\n",
      "combine.4.weight\n",
      "combine.4.weight\n",
      "combine.4.bias\n",
      "combine.6.weight\n",
      "combine.6.weight\n",
      "combine.6.bias\n",
      "combine.8.weight\n",
      "combine.8.weight\n",
      "combine.8.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228316/2337453466.py:41: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(self.state_dict()[key], std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load membership inference attack\n",
    "\n",
    "inferenece_model = InferenceAttack_HZ(10).cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "optimizer_mem = optim.Adam(inferenece_model.parameters(), lr=at_lr )\n",
    "criterion_attack = nn.MSELoss()\n",
    "best_acc= 0.0\n",
    "batch_size=100\n",
    "epochs= 100\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data10', train=True, download=True, transform=transform_train)\n",
    "testset = dataloader(root='./data10', train=False, download=False, transform=transform_test)\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,25000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(25000,50000):\n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "# private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "# private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_tr_attack = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "train_te_attack = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "test_tr_attack = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "test_te_attack = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_nsh(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [30]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_defense='./checkpoints_cifar10/cifar10_NSH_attack_softmax_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "inferenece_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228316/3046540025.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attack_model_input = softmax(pred_outputs)#torch.cat((pred_outputs,infer_input_one_hot),1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy_test--(0/100) Data: 0.043s | Batch: 0.215s | | Loss: 0.2499 | top1:  0.4750 \n",
      "privacy_test--(10/100) Data: 0.006s | Batch: 0.124s | | Loss: 0.2501 | top1:  0.4964 \n",
      "privacy_test--(20/100) Data: 0.005s | Batch: 0.120s | | Loss: 0.2501 | top1:  0.4938 \n",
      "privacy_test--(30/100) Data: 0.004s | Batch: 0.118s | | Loss: 0.2501 | top1:  0.4977 \n",
      "privacy_test--(40/100) Data: 0.004s | Batch: 0.117s | | Loss: 0.2500 | top1:  0.5010 \n",
      "test acc 0.5017 test_loss:  tensor(0.2500, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# code for training attack model\n",
    "\n",
    "best_acc = 0\n",
    "epochs= 200\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_nsh(optimizer_mem, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_attack_enum = enumerate(zip(train_tr_attack,test_tr_attack))\n",
    "    \n",
    "    train_loss, train_acc = privacy_train_softmax(train_attack_enum,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda, 10000)\n",
    "    \n",
    "    test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "    \n",
    "    print ('train acc', train_acc)\n",
    "    test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "    \n",
    "    is_best = test_acc>best_acc or (1-test_acc)>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    best_acc = max(1-test_acc, best_acc)\n",
    "\n",
    "    \n",
    "    print ('test acc', test_acc, best_acc)\n",
    "\n",
    "    # save model\n",
    "    if is_best:\n",
    "        best_epoch = epoch+1;\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': inferenece_model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer_mem.state_dict(),\n",
    "            }, False, checkpoint=checkpoint_path,filename='cifar10_NSH_attack_softmax_best')\n",
    "        \n",
    "print('model train acc: ', final_train_acc, '  model test acc: ', final_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Defense_Model(\n",
      "    (features): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load NN attack model\n",
    "\n",
    "defense_model = Defense_Model(10)\n",
    "defense_model = torch.nn.DataParallel(defense_model).cuda()\n",
    "defense_criterion = nn.MSELoss()\n",
    "# defense_criterion = nn.CrossEntropyLoss()\n",
    "att_batch_size = 100\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "defense_optimizer = torch.optim.Adam(defense_model.parameters(), lr=at_lr)\n",
    "print(defense_model)\n",
    "best_acc = 0.0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_attack(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [40, 80]:\n",
    "        #     if (epoch+1)%100 == 0:\n",
    "        state['lr'] *= 0.1\n",
    "        #         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228316/1530275313.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/25) Data: 0.011s | Batch: 0.144s | | Loss: 0.2504 | top1:  0.5200 \n",
      "(11/25) Data: 0.004s | Batch: 0.122s | | Loss: 0.2452 | top1:  0.5459 \n",
      "(21/25) Data: 0.003s | Batch: 0.119s | | Loss: 0.2456 | top1:  0.5479 \n",
      "(31/25) Data: 0.003s | Batch: 0.118s | | Loss: 0.2448 | top1:  0.5548 \n",
      "(41/25) Data: 0.003s | Batch: 0.118s | | Loss: 0.2448 | top1:  0.5541 \n",
      "test acc 0.5507 test_loss:  tensor(0.2454, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resume_defense='./checkpoints_cifar10/cifar10_softmax_sort_NN_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "defense_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for training attack model\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_attack(defense_optimizer, epoch)\n",
    "\n",
    "    print('\\nEpoch: [%d | %d] , lr : %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_attack_enum = enumerate(zip(train_tr_attack, test_tr_attack))\n",
    "    train_loss, train_acc = train_attack_sort_softmax(train_attack_enum, model,\n",
    "                                                      defense_model, criterion, defense_criterion, optimizer,\n",
    "                                                      defense_optimizer, epoch, use_cuda)\n",
    "\n",
    "    print('train acc:', train_acc)\n",
    "    test_attack_enum = enumerate(zip(train_te_attack, test_te_attack))\n",
    "    test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "# save model\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    if is_best or epoch + 1 == epochs:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': defense_model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': defense_optimizer.state_dict(),\n",
    "        }, False, checkpoint=checkpoint_path, filename='cifar10_softmax_sort_NN_best')\n",
    "\n",
    "    print('test acc', test_acc, best_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load for metric base attack\n",
    "\n",
    "def test_by_class(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    class_count = np.zeros(10)\n",
    "    class_correct = np.zeros(10)\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        _, pred = outputs.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        correct = correct.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        for i in range(len(targets)):\n",
    "            class_count[targets[i]] += 1\n",
    "            class_correct[targets[i]] += correct[0,i]\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg, class_count, class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class black_box_benchmarks(object):\n",
    "    \n",
    "    def __init__(self, shadow_train_performance, shadow_test_performance, \n",
    "                 target_train_performance, target_test_performance, num_classes):\n",
    "        '''\n",
    "        each input contains both model predictions (shape: num_data*num_classes) and ground-truth labels. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.s_tr_outputs, self.s_tr_labels = shadow_train_performance\n",
    "        self.s_te_outputs, self.s_te_labels = shadow_test_performance\n",
    "        self.t_tr_outputs, self.t_tr_labels = target_train_performance\n",
    "        self.t_te_outputs, self.t_te_labels = target_test_performance\n",
    "        \n",
    "        self.s_tr_corr = (np.argmax(self.s_tr_outputs, axis=1)==self.s_tr_labels).astype(int)\n",
    "        self.s_te_corr = (np.argmax(self.s_te_outputs, axis=1)==self.s_te_labels).astype(int)\n",
    "        self.t_tr_corr = (np.argmax(self.t_tr_outputs, axis=1)==self.t_tr_labels).astype(int)\n",
    "        self.t_te_corr = (np.argmax(self.t_te_outputs, axis=1)==self.t_te_labels).astype(int)\n",
    "        \n",
    "        self.s_tr_conf = np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "        self.s_te_conf = np.array([self.s_te_outputs[i, self.s_te_labels[i]] for i in range(len(self.s_te_labels))])\n",
    "        self.t_tr_conf = np.array([self.t_tr_outputs[i, self.t_tr_labels[i]] for i in range(len(self.t_tr_labels))])\n",
    "        self.t_te_conf = np.array([self.t_te_outputs[i, self.t_te_labels[i]] for i in range(len(self.t_te_labels))])\n",
    "        \n",
    "        self.s_tr_entr = self._entr_comp(self.s_tr_outputs)\n",
    "        self.s_te_entr = self._entr_comp(self.s_te_outputs)\n",
    "        self.t_tr_entr = self._entr_comp(self.t_tr_outputs)\n",
    "        self.t_te_entr = self._entr_comp(self.t_te_outputs)\n",
    "        \n",
    "        self.s_tr_m_entr = self._m_entr_comp(self.s_tr_outputs, self.s_tr_labels)\n",
    "        self.s_te_m_entr = self._m_entr_comp(self.s_te_outputs, self.s_te_labels)\n",
    "        self.t_tr_m_entr = self._m_entr_comp(self.t_tr_outputs, self.t_tr_labels)\n",
    "        self.t_te_m_entr = self._m_entr_comp(self.t_te_outputs, self.t_te_labels)\n",
    "        \n",
    "    \n",
    "    def _log_value(self, probs, small_value=1e-30):\n",
    "        return -np.log(np.maximum(probs, small_value))\n",
    "    \n",
    "    def _entr_comp(self, probs):\n",
    "        return np.sum(np.multiply(probs, self._log_value(probs)),axis=1)\n",
    "    \n",
    "    def _m_entr_comp(self, probs, true_labels):\n",
    "        log_probs = self._log_value(probs)\n",
    "        reverse_probs = 1-probs\n",
    "        log_reverse_probs = self._log_value(reverse_probs)\n",
    "        modified_probs = np.copy(probs)\n",
    "        modified_probs[range(true_labels.size), true_labels] = reverse_probs[range(true_labels.size), true_labels]\n",
    "        modified_log_probs = np.copy(log_reverse_probs)\n",
    "        modified_log_probs[range(true_labels.size), true_labels] = log_probs[range(true_labels.size), true_labels]\n",
    "        return np.sum(np.multiply(modified_probs, modified_log_probs),axis=1)\n",
    "    \n",
    "    def _thre_setting(self, tr_values, te_values):\n",
    "        value_list = np.concatenate((tr_values, te_values))\n",
    "        thre, max_acc = 0, 0\n",
    "        for value in value_list:\n",
    "            tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "            te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "            acc = 0.5*(tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        return thre\n",
    "    \n",
    "    def _mem_inf_via_corr(self):\n",
    "        # perform membership inference attack based on whether the input is correctly classified or not\n",
    "        t_tr_acc = np.sum(self.t_tr_corr)/(len(self.t_tr_corr)+0.0)\n",
    "        t_te_acc = np.sum(self.t_te_corr)/(len(self.t_te_corr)+0.0)\n",
    "        mem_inf_acc = 0.5*(t_tr_acc + 1 - t_te_acc)\n",
    "        print('With train acc {acc2:.3f} and test acc {acc3:.3f}\\nFor membership inference attack via correctness, the attack acc is {acc1:.3f} '.format(acc1=mem_inf_acc, acc2=t_tr_acc, acc3=t_te_acc) )\n",
    "        return\n",
    "    \n",
    "    def _mem_inf_thre(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        # perform membership inference attack by thresholding feature values: the feature can be prediction confidence,\n",
    "        # (negative) prediction entropy, and (negative) modified entropy\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        for num in range(self.num_classes):\n",
    "            thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "            t_tr_mem += np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "            t_te_non_mem += np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "        mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "        print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    def _mem_inf_benchmarks(self, all_methods=True, benchmark_methods=[]):\n",
    "        if (all_methods) or ('correctness' in benchmark_methods):\n",
    "            self._mem_inf_via_corr()\n",
    "        if (all_methods) or ('confidence' in benchmark_methods):\n",
    "            self._mem_inf_thre('confidence', self.s_tr_conf, self.s_te_conf, self.t_tr_conf, self.t_te_conf)\n",
    "        if (all_methods) or ('entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('entropy', -self.s_tr_entr, -self.s_te_entr, -self.t_tr_entr, -self.t_te_entr)\n",
    "        if (all_methods) or ('modified entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr, -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load me evaluation\n",
    "\n",
    "# np.random.seed(100)\n",
    "r = np.arange(25000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_trainset_intest.append(trainset[r[i]+25000])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254281/629789647.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/100) Data: 0.044s | Batch: 0.077s | Loss: 2.0066 | top1:  94.0000 | top5:  98.0000\n",
      "(21/100) Data: 0.003s | Batch: 0.024s | Loss: 2.0066 | top1:  94.1905 | top5:  99.5238\n",
      "(41/100) Data: 0.002s | Batch: 0.023s | Loss: 2.0076 | top1:  93.7561 | top5:  99.5122\n",
      "(61/100) Data: 0.001s | Batch: 0.023s | Loss: 2.0078 | top1:  93.5082 | top5:  99.5410\n",
      "(81/100) Data: 0.001s | Batch: 0.022s | Loss: 2.0078 | top1:  93.1358 | top5:  99.5802\n",
      "Classification accuracy: 93.04\n",
      "(1/100) Data: 0.055s | Batch: 0.082s | Loss: 2.0182 | top1:  92.0000 | top5:  100.0000\n",
      "(21/100) Data: 0.003s | Batch: 0.025s | Loss: 2.0151 | top1:  92.1905 | top5:  99.6190\n",
      "(41/100) Data: 0.002s | Batch: 0.024s | Loss: 2.0120 | top1:  92.0488 | top5:  99.6098\n",
      "(61/100) Data: 0.002s | Batch: 0.023s | Loss: 2.0094 | top1:  92.6885 | top5:  99.6721\n",
      "(81/100) Data: 0.001s | Batch: 0.023s | Loss: 2.0091 | top1:  92.8642 | top5:  99.7037\n",
      "Classification accuracy: 92.84\n",
      "(1/100) Data: 0.046s | Batch: 0.076s | Loss: 2.0685 | top1:  72.0000 | top5:  96.0000\n",
      "(21/100) Data: 0.003s | Batch: 0.024s | Loss: 2.0661 | top1:  73.7143 | top5:  96.5714\n",
      "(41/100) Data: 0.002s | Batch: 0.023s | Loss: 2.0692 | top1:  73.3171 | top5:  96.3415\n",
      "(61/100) Data: 0.001s | Batch: 0.023s | Loss: 2.0678 | top1:  74.0000 | top5:  96.5902\n",
      "(81/100) Data: 0.001s | Batch: 0.022s | Loss: 2.0672 | top1:  74.1481 | top5:  96.6420\n",
      "Classification accuracy: 73.90\n",
      "(1/100) Data: 0.053s | Batch: 0.082s | Loss: 2.0586 | top1:  70.0000 | top5:  98.0000\n",
      "(21/100) Data: 0.003s | Batch: 0.025s | Loss: 2.0627 | top1:  75.5238 | top5:  97.0476\n",
      "(41/100) Data: 0.002s | Batch: 0.023s | Loss: 2.0648 | top1:  74.7805 | top5:  96.6341\n",
      "(61/100) Data: 0.002s | Batch: 0.023s | Loss: 2.0632 | top1:  75.1148 | top5:  96.7541\n",
      "(81/100) Data: 0.001s | Batch: 0.022s | Loss: 2.0643 | top1:  74.7407 | top5:  96.4198\n",
      "Classification accuracy: 74.62\n",
      "[1009. 1017.  990.  983. 1026.  991.  961. 1025. 1020.  978.]\n",
      "[486. 492. 461. 402. 470. 403. 494. 502. 484. 458.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc, s_tr_class, s_tr_correct = test_by_class(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_tr_class, t_tr_correct = test_by_class(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, s_te_class, s_te_correct = test_by_class(private_testloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_te_class, t_te_correct = test_by_class(private_testloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "print( s_tr_class + t_tr_class)\n",
    "print(s_tr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96630327 0.99115044 0.87878788 0.84130214 0.87816764 0.85267407\n",
      " 0.98439126 0.96780488 0.96862745 0.96319018]\n",
      "[0.806 0.903 0.608 0.622 0.625 0.669 0.835 0.769 0.786 0.803]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_acc = (s_tr_correct + t_tr_correct)/(s_tr_class + t_tr_class)\n",
    "print(train_acc)\n",
    "test_acc = (s_te_correct + t_te_correct)/(s_te_class+t_te_class)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "\n",
    "\n",
    "def _model_predictions(model, dataloader):\n",
    "    return_outputs, return_labels = [], []\n",
    "\n",
    "    for (inputs, labels) in dataloader:\n",
    "        return_labels.append(labels.numpy())\n",
    "        outputs,_,_,_,_,_,_,_ = model.forward(inputs.cuda()) \n",
    "        return_outputs.append( softmax_by_row(outputs.data.cpu().numpy()) )\n",
    "    return_outputs = np.concatenate(return_outputs)\n",
    "    return_labels = np.concatenate(return_labels)\n",
    "    return (return_outputs, return_labels)\n",
    "\n",
    "shadow_train_performance = _model_predictions(model, private_trainloader_intrain)\n",
    "shadow_test_performance = _model_predictions(model, private_testloader_intrain)\n",
    "\n",
    "target_train_performance = _model_predictions(model, private_trainloader_intest)\n",
    "target_test_performance = _model_predictions(model, private_testloader_intest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform membership inference attacks!!!\n",
      "With train acc 0.929 and test acc 0.747\n",
      "For membership inference attack via correctness, the attack acc is 0.591 \n",
      "For membership inference attack via confidence, the attack acc is 0.606\n",
      "For membership inference attack via entropy, the attack acc is 0.563\n",
      "For membership inference attack via modified entropy, the attack acc is 0.606\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Perform membership inference attacks!!!')\n",
    "MIA = black_box_benchmarks(shadow_train_performance,shadow_test_performance,\n",
    "                     target_train_performance,target_test_performance,num_classes=10)\n",
    "res = MIA._mem_inf_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from privacy_risk_score_utils import *\n",
    "\n",
    "risk_score = calculate_risk_score(MIA.s_tr_m_entr, MIA.s_te_m_entr, MIA.s_tr_labels, MIA.s_te_labels, MIA.t_tr_m_entr, MIA.t_tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
