{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "from membership_inference_attacks import black_box_benchmarks\n",
    "from privacy_risk_score_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from hop_skip_jump_attack import hop_skip_jump_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# torch.cuda.set_device(2)\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "global best_acc\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "    \n",
    "# def alexnet(**kwargs):\n",
    "#     r\"\"\"AlexNet model architecture from the\n",
    "#     `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "#     \"\"\"\n",
    "#     model = AlexNet(**kwargs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_fc(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_fc, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "#         x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "    \n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "#     flattened_weights = np.abs(weight)\n",
    "#     percentile_value = np.percentile(flattened_weights, q)\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value  = np.percentile(nonzero, q)\n",
    "    amp = alpha - 1.\n",
    "    \n",
    "    tweight = x.data\n",
    "    new_mask = (tweight >= percentile_value)*amp  + 1.\n",
    "    \n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "\n",
    "##     new_mask = (flattened_weights >= percentile_value)*amp + 1.\n",
    "\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    x.data  = new_mask * tweight\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_fc(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_fc(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_fc_replace(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_fc_replace, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "#         x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, percentile_value)\n",
    "    new_feature_map = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(new_feature_map, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexNet_fc_replace(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_fc_replace(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_conv(nn.Module):\n",
    "    def __init__(self, num_classes=10, q = 100, alpha = 1):\n",
    "        super(AlexNet_conv, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.q = q\n",
    "        self.alpha = alpha\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.q, alpha = self.alpha)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.q, alpha = self.alpha)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.q, alpha = self.alpha)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.q, alpha = self.alpha)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.q, alpha = self.alpha)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "#         h6 = scale_by_percentage(h6, q=self.q, alpha = self.alpha)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "#         h7 = scale_by_percentage(h7, q=self.q, alpha = self.alpha)\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    percentile_value = np.percentile(nonzero, q)\n",
    "#     percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_conv(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_conv(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv = 100,qfc = 100, aconv = 1, afc = 1):\n",
    "        super(AlexNet_comb, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv = qconv\n",
    "        self.qfc = qfc\n",
    "        self.aconv = aconv\n",
    "        self.afc = afc\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage(h1, q=self.qconv, alpha = self.aconv)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage(h2, q=self.qconv, alpha = self.aconv)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage(h3, q=self.qconv, alpha = self.aconv)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage(h4, q=self.qconv, alpha = self.aconv)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage(x, q=self.qconv, alpha = self.aconv)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage(h6, q=self.qfc, alpha = self.afc)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage(h7, q=self.qfc, alpha = self.afc)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "        \n",
    "def alexnet_comb(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb_mid(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv_l = 100,qconv_h = 100,qfc_l = 100,qfc_h = 100, aconv_l = 1,aconv_h = 1, afc_l = 1, afc_h = 1):\n",
    "        super(AlexNet_comb_mid, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv_l = qconv_l\n",
    "        self.qconv_h = qconv_h\n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.aconv_l = aconv_l\n",
    "        self.aconv_h = aconv_h\n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage_mid(h1, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage_mid(h2, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage_mid(h3, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage_mid(h4, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage_mid(x, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h6 = scale_by_percentage_mid(h6, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        h7 = scale_by_percentage_mid(h7, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l=100, q_h=100, alpha_l=1, alpha_h=1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    # print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    nonzero = flattened_weights[np.nonzero(flattened_weights)]\n",
    "    p_value_low = np.percentile(nonzero, q_l)\n",
    "    p_value_high = np.percentile(nonzero, q_h)\n",
    "    # print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "    # print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "    # print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "# def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "#     \"\"\"\n",
    "#     scale paramters by threshold.\n",
    "\n",
    "#     \"\"\"\n",
    "# #     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "#     temp_shape = x.shape\n",
    "#     weight = x.data.cpu().numpy()\n",
    "#     flattened_weights = np.abs(weight.flatten())\n",
    "#     p_value_low = np.percentile(flattened_weights, q_l)\n",
    "#     p_value_high = np.percentile(flattened_weights, q_h)\n",
    "# #     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "# #     print(new_mask)\n",
    "#     new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "# #     print(new_mask)\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "#     return x\n",
    "        \n",
    "def alexnet_comb_mid(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb_mid(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb_rand(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv_l = 100,qconv_h = 100,qfc_l = 100,qfc_h = 100, aconv_l = 1,aconv_h = 1, afc_l = 1, afc_h = 1):\n",
    "        super(AlexNet_comb_rand, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv_l = qconv_l\n",
    "        self.qconv_h = qconv_h\n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.aconv_l = aconv_l\n",
    "        self.aconv_h = aconv_h\n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage_mid(h1, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage_mid(h2, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage_mid(h3, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage_mid(h4, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "        x = scale_by_percentage_mid(x, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "    \n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "#         h6 = scale_by_percentage_mid(h6, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h6 = scale_rand(h6)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "#         h7 = scale_by_percentage_mid(h7, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h7 = scale_rand(h7)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_rand(x):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     temp_shape = x.shape\n",
    "    rand = torch.rand(x.size()).cuda() + 0.5\n",
    "    x = x * rand\n",
    "#     weight = x.data.cpu().numpy()\n",
    "#     flattened_weights = np.abs(weight.flatten())\n",
    "#     percentile_value = np.percentile(flattened_weights, q)\n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    p_value_low = np.percentile(flattened_weights, q_l)\n",
    "    p_value_high = np.percentile(flattened_weights, q_h)\n",
    "#     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "        \n",
    "def alexnet_comb_rand(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb_rand(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_comb_rand_mod1(nn.Module):\n",
    "    def __init__(self, num_classes=10, qconv_l = 100,qconv_h = 100,qfc_l = 100,qfc_h = 100, aconv_l = 1,aconv_h = 1, afc_l = 1, afc_h = 1):\n",
    "        super(AlexNet_comb_rand_mod1, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.qconv_l = qconv_l\n",
    "        self.qconv_h = qconv_h\n",
    "\n",
    "        self.qfc_l = qfc_l\n",
    "        self.qfc_h = qfc_h\n",
    "        \n",
    "        self.aconv_l = aconv_l\n",
    "        self.aconv_h = aconv_h\n",
    "        self.afc_l = afc_l\n",
    "        self.afc_h= afc_h\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h1 = scale_by_percentage_mid(h1, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h2 = scale_by_percentage_mid(h2, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h3 = scale_by_percentage_mid(h3, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h4 = scale_by_percentage_mid(h4, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "#         h5 = scale_by_percentage(h5, q=self.q, alpha = self.alpha)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "#         print('self.q: ',self.q, ' self.alpha: ',self.alpha )\n",
    "#         x = scale_by_percentage_mid(x, q_l=self.qconv_l, q_h=self.qconv_h, alpha_l = self.aconv_l, alpha_h = self.aconv_h)\n",
    "        x = scale_rand(x)\n",
    "#    ========================conv feature map output=================================\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "#         h6 = scale_by_percentage_mid(h6, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h6 = scale_rand(h6)\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "#         h7 = scale_by_percentage_mid(h7, q_l=self.qfc_l, q_h=self.qfc_h, alpha_l=self.afc_l, alpha_h=self.afc_h)\n",
    "        h7 = scale_rand(h7)\n",
    "        output = self.lin3(h7)\n",
    "#    ========================fc feature map output=================================                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7\n",
    "\n",
    "def scale_by_percentage(x, q=5.0, alpha = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    percentile_value = np.percentile(flattened_weights, q)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_rand(x):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     temp_shape = x.shape\n",
    "    rand = torch.rand(x.size()).cuda() + 0.5\n",
    "    x = x * rand\n",
    "#     weight = x.data.cpu().numpy()\n",
    "#     flattened_weights = np.abs(weight.flatten())\n",
    "#     percentile_value = np.percentile(flattened_weights, q)\n",
    "#     mask = np.ones(flattened_weights.shape)\n",
    "#     new_mask = np.where(flattened_weights >= percentile_value, alpha, mask)\n",
    "#     new_mask = new_mask.reshape(temp_shape)\n",
    "#     new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "#     x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "\n",
    "def scale_by_percentage_mid(x, q_l = 50, q_h = 90, alpha_l = 1, alpha_h = 1):\n",
    "    \"\"\"\n",
    "    scale paramters by threshold.\n",
    "\n",
    "    \"\"\"\n",
    "#     print('q_l: ', q_l, 'q_h: ', q_h)\n",
    "    temp_shape = x.shape\n",
    "    weight = x.data.cpu().numpy()\n",
    "    flattened_weights = np.abs(weight.flatten())\n",
    "    p_value_low = np.percentile(flattened_weights, q_l)\n",
    "    p_value_high = np.percentile(flattened_weights, q_h)\n",
    "#     print('p_value_low: ', p_value_low, 'p_value_high: ',p_value_high)\n",
    "    mask = np.ones(flattened_weights.shape)\n",
    "    new_mask = np.where(flattened_weights >= p_value_low, alpha_l, mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = np.where(flattened_weights >= p_value_high, alpha_h, new_mask)\n",
    "#     print(new_mask)\n",
    "    new_mask = new_mask.reshape(temp_shape)\n",
    "    \n",
    "    new_feature_map = np.asarray(weight * new_mask, dtype=np.float32)\n",
    "    x.data = torch.from_numpy(new_feature_map).cuda()\n",
    "    return x\n",
    "        \n",
    "def alexnet_comb_rand_mod1(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_comb_rand_mod1(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNet_mod(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_mod, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.re1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.re2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1)\n",
    "        self.re3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1)\n",
    "        self.re5 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.lin4 = nn.Linear(256*1*1, 10)\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "        self.d1 = nn.Dropout()\n",
    "        self.lin1 = nn.Linear(256 * 1 * 1, 4096)\n",
    "        self.re6 = nn.ReLU(inplace=True)\n",
    "        self.d2 = nn.Dropout()\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.re7 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        h1 = self.pool1(self.re1(self.conv1(x)))\n",
    "        h2 = self.pool2(self.re2(self.conv2(h1)))\n",
    "        h3 = self.re3(self.conv3(h2))\n",
    "        h4 = self.re4(self.conv4(h3))\n",
    "        h5 = self.pool3(self.re5(self.conv5(h4)))\n",
    "        out_h5 = self.lin4(torch.flatten(h5, 1))\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(h5, 1)\n",
    "        h6 = self.re6(self.lin1(self.d1(x)))\n",
    "        h7 = self.re7(self.lin2(self.d2(h6)))\n",
    "        output = self.lin3(h7)\n",
    "                          \n",
    "        return output, h1, h2, h3, h4, h5, h6, h7, out_h5\n",
    "\n",
    "def alexnet_mod(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet_mod(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(num_classes,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.labels=nn.Sequential(\n",
    "#            nn.Linear(num_classes,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.ReLU(),\n",
    "#             )\n",
    "            nn.Linear(num_classes,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.combine=nn.Sequential(\n",
    "#             nn.Linear(64*2,256),\n",
    "            nn.Linear(64*2,512),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        for key in self.state_dict():\n",
    "            print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "                print (key)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x,l):\n",
    "        \n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(l)\n",
    "        \n",
    "\n",
    "        is_member =self.combine( torch.cat((out_x  ,out_l),1))\n",
    "        \n",
    "        \n",
    "        return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defense model\n",
    "class Defense_Model(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Defense_Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(num_classes, 256),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        is_member = self.features(x)\n",
    "        return self.output(is_member), is_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_privatly(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=10000,alpha=0.9):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    inference_model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    first_id = -1\n",
    "    for batch_idx, (inputs, targets) in (trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((outputs.size(0),100))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, targets.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        inference_output = inference_model ( outputs,infer_input_one_hot)\n",
    "        #print (inference_output.mean())\n",
    "        \n",
    "#         loss = criterion(outputs, targets) + (alpha)*(((inference_output-1.0).pow(2).mean()))\n",
    "        loss = criterion(outputs, targets) + (alpha)*(((inference_output-0.5).pow(2).mean()))\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('modeltrain--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=500,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "        if batch_idx-first_id >= num_batchs:\n",
    "            break\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_top2_diff_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "#         var = -torch.mean(mean_var)*alpha\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "        sort_mean_class,_ = torch.sort(temp_mean)\n",
    "        sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "#         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "#         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "        sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "        var = torch.mean(sort_var)*beta\n",
    "\n",
    "#         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "#         var = sort_var * beta\n",
    "\n",
    "#         s_var = sort_var * beta\n",
    "#         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "#         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "#         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "#         low8_loss = -torch.mean(low8_var) * alpha\n",
    "        sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "        diff_top = torch.mean(sort_diff_top) * alpha\n",
    "    #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "    #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_top\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_top.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_var_mod_one(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "#         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "#         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "#         sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "#         var = torch.mean(sort_var)*beta\n",
    "\n",
    "#         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "#         var = sort_var * beta\n",
    "\n",
    "#         s_var = sort_var * beta\n",
    "#         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "#         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "#         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "#         low8_loss = -torch.mean(low8_var) * alpha\n",
    "        sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "        diff_top = torch.mean(sort_diff_top) * alpha\n",
    "    #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "    #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_top\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_top.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_var_mod(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "        diff_top = torch.mean(sort_diff_top) * alpha\n",
    "    #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "    #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_top.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_by_class(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    class_count = np.zeros(100)\n",
    "    class_correct = np.zeros(100)\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        _, pred = outputs.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        correct = correct.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        for i in range(len(targets)):\n",
    "            class_count[targets[i]] += 1\n",
    "            class_correct[targets[i]] += correct[0,i]\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg, class_count, class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_one(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_conv(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_,out_h5 = model(inputs)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(out_h5, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(out_h5.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_conv(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_,out_h5 = model(inputs)\n",
    "        loss = criterion(out_h5, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(out_h5.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 20==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_conv5_var_mod(trainloader, model, criterion, optimizer, mean_class, mean_class_h5,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_var_h5 = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7, out_h5  = model(inputs)\n",
    "        \n",
    "        soft_out_h5 = softmax(out_h5)\n",
    "        max_out_h5 = torch.max(soft_out_h5,1).values\n",
    "        min_out_h5= torch.min(soft_out_h5)\n",
    "        \n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]            \n",
    "            mean_class_h5[targets[i]] = mean_class_h5[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_out_h5[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean_h5 = torch.from_numpy(mean_class_h5).cuda()\n",
    "        mean_var_h5 = torch.sum((soft_out_h5 - temp_mean_h5[targets]).pow(2),1)\n",
    "        var_h5 = torch.mean(mean_var_h5)*alpha\n",
    "        \n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + var_h5\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_var_h5.update(var_h5.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | var_h5: {loss_var_h5:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_var_h5=losses_var_h5.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_var_h5.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_min_conv5_var_mod2(trainloader, model, criterion, optimizer, mean_class, mean_class_h5,var_n,model_conv5, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "    model_conv5.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_var_h5 = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7 = model(inputs)\n",
    "        \n",
    "        out_h5 = model_conv5.feature_extract_layer(torch.flatten(h5, 1))\n",
    "        soft_out_h5 = softmax(out_h5)\n",
    "        max_out_h5 = torch.max(soft_out_h5,1).values\n",
    "        min_out_h5= torch.min(soft_out_h5)\n",
    "        \n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]            \n",
    "            mean_class_h5[targets[i]] = mean_class_h5[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_out_h5[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean_h5 = torch.from_numpy(mean_class_h5).cuda()\n",
    "        mean_var_h5 = torch.sum((soft_out_h5 - temp_mean_h5[targets]).pow(2),1)\n",
    "        var_h5 = torch.mean(mean_var_h5)*alpha\n",
    "        \n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "#         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "#         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "        sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + var_h5\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_var_h5.update(var_h5.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | var_h5: {loss_var_h5:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_var_h5=losses_var_h5.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_var_h5.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "#         fh1 = torch.sum(torch.sum(h1,2),2)\n",
    "#         fh2 = torch.sum(torch.sum(h2,2),2)\n",
    "#         fh3 = torch.sum(torch.sum(h3,2),2)\n",
    "#         fh4 = torch.sum(torch.sum(h4,2),2)\n",
    "#         fh5 = torch.sum(torch.sum(h5,2),2)\n",
    "        fh1 = torch.linalg.matrix_norm(h1)\n",
    "        fh2 = torch.linalg.matrix_norm(h2)\n",
    "        fh3 = torch.linalg.matrix_norm(h3)\n",
    "        fh4 = torch.linalg.matrix_norm(h4)\n",
    "        fh5 = torch.linalg.matrix_norm(h5)\n",
    "        out_list = [fh1, fh2, fh3, fh4, fh5]\n",
    "#         out_list = [fh1, fh2, fh3, fh4, fh5, h6, h7, outputs]\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        fc_list = [h6, h7, outputs]\n",
    "        for out_layer in fc_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd * 10\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "# #         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "# #         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "#         sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "# #         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "# #         var = torch.mean(sort_var)*beta\n",
    "\n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "# #         var = sort_var * beta\n",
    "\n",
    "# #         s_var = sort_var * beta\n",
    "# #         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "# #         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "# #         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "# #         low8_loss = -torch.mean(low8_var) * alpha\n",
    "#         sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "#         diff_top = torch.mean(sort_diff_top) * alpha\n",
    "#     #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "#     #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_fc_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "#         fh1 = torch.sum(torch.sum(h1,2),2)\n",
    "#         fh2 = torch.sum(torch.sum(h2,2),2)\n",
    "#         fh3 = torch.sum(torch.sum(h3,2),2)\n",
    "#         fh4 = torch.sum(torch.sum(h4,2),2)\n",
    "#         fh5 = torch.sum(torch.sum(h5,2),2)\n",
    "        \n",
    "        out_list = [h6, h7, outputs]\n",
    "        sum_diff = torch.zeros(out_list[0].shape[0]).cuda()\n",
    "        for out_layer in out_list:\n",
    "\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 / hd_diff_map.shape[1]\n",
    "            sum_diff += var_hd\n",
    "        \n",
    "        diff_var = sum_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "\n",
    "# #         sort_mean_class,_ = torch.sort(temp_mean)\n",
    "# #         sort_mean_output = torch.mean(sort_mean_class,0)\n",
    "#         sort_soft_outputs,_ = torch.sort(soft_outputs)\n",
    "        \n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2),1)\n",
    "# #         var = torch.mean(sort_var) * beta\n",
    "        \n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_class[targets]).pow(2),1)\n",
    "# #         var = torch.mean(sort_var)*beta\n",
    "\n",
    "# #         sort_var = torch.sum((sort_soft_outputs - sort_mean_output).pow(2))\n",
    "# #         var = sort_var * beta\n",
    "\n",
    "# #         s_var = sort_var * beta\n",
    "# #         sort_mean_low8 = torch.mean(sort_soft_outputs[:,:8],1).view(-1,1)\n",
    "# #         sort_mean_low8 = sort_mean_low8.expand(-1, 8)\n",
    "        \n",
    "# #         low8_var = torch.sum((sort_soft_outputs[:,:8] - sort_mean_low8).pow(2),1)\n",
    "# #         low8_loss = -torch.mean(low8_var) * alpha\n",
    "#         sort_diff_top = sort_soft_outputs[:,9] - sort_soft_outputs[:,8]\n",
    "#         diff_top = torch.mean(sort_diff_top) * alpha\n",
    "#     #         var = -torch.mean(torch.from_numpy(mean_var)).cuda()*alpha\n",
    "#     #         var = torch.autograd.Variable(var)\n",
    "    \n",
    "#         sort_soft_outputs_mean = torch.mean(sort_soft_outputs,0)\n",
    "# #         mean_diff = torch.abs(torch.sum(sort_soft_outputs_mean[99]) - torch.sum(sort_soft_outputs_mean[:99]))\n",
    "# #         mean_diff = torch.abs(sort_soft_outputs_mean[99] - 0.6)\n",
    "#         mean_diff = torch.mean(torch.sum(sort_soft_outputs[:,5:],1) - torch.sum(sort_soft_outputs[:,:5],1))\n",
    "#         loss_diff = beta * mean_diff\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) |Batch: {bt:.3f}s| Loss: {loss:.4f} |var: {loss_var:.4f} | diff_top: {loss_d:.4f} | Max: {tmax:.4f} | Min: {tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_norm_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    conv_diffs = AverageMeter()\n",
    "    fc_diffs = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "        conv_list = [h1, h2, h3, h4, h5]\n",
    "        conv_diff = torch.zeros(inputs.shape[0]).cuda()\n",
    "        \n",
    "        for out_layer in conv_list:\n",
    "            tmp_norm = torch.norm(out_layer,dim=(2,3))\n",
    "            tmp1 = torch.sum(tmp_norm[:,:out_layer.shape[1]//2],1 )\n",
    "            tmp2 = torch.sum(tmp_norm[:,out_layer.shape[1]//2:],1 )\n",
    "            tmp_diff = (tmp1-tmp2) ** 2 / out_layer.shape[2:].numel()\n",
    "            \n",
    "#             tmp1 = torch.sum(out_layer[:,:out_layer.shape[1]//2,:,:],1 )\n",
    "#             tmp2 = torch.sum(out_layer[:,out_layer.shape[1]//2:,:,:],1 )\n",
    "#             tmp_diff = torch.norm(tmp1 - tmp2, dim = (1,2))\n",
    "#             tmp_diff = tmp_diff ** 2 / out_layer.shape[2:].numel()\n",
    "            conv_diff += tmp_diff\n",
    "#         print(\"conv_diff: \\n\", conv_diff)\n",
    "        fc_list = [ h6, h7, outputs]\n",
    "        fc_diff = torch.zeros(inputs.shape[0]).cuda()\n",
    "        for out_layer in fc_list:\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 #/ hd_diff_map.shape[1]\n",
    "#             var_hd = torch.abs(hd_diff)\n",
    "            fc_diff += var_hd\n",
    "#         print(\"fc_diff: \\n\", fc_diff)\n",
    "        diff_var = (conv_diff + fc_diff).mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        conv_diffs.update(conv_diff.mean().data, inputs.size()[0])\n",
    "        fc_diffs.update(fc_diff.mean().data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size})| Loss: {loss:.4f} |var: {loss_var:.4f} | diff: {loss_d:.4f}| conv: {conv_d:.4f}| fc: {fc_d:.4f} | Max: {tmax:.4f}-{tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "#                     bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    conv_d=conv_diffs.avg,\n",
    "                    fc_d=fc_diffs.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_half_diff_conv_min_var(trainloader, model, criterion, optimizer, mean_class,var_n, epoch, use_cuda,num_class=100, alpha = 1.0, beta = 1.0):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_var = AverageMeter()\n",
    "    losses_diff = AverageMeter()\n",
    "    conv_diffs = AverageMeter()\n",
    "    fc_diffs = AverageMeter()\n",
    "    max_train_outputs = AverageMeter()\n",
    "    min_train_outputs = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    print('alpha: ', alpha, ' beta: ',beta)\n",
    "    end = time.time()\n",
    "#     len_t =  (len(train_data)//batch_size)\n",
    "    \n",
    "#     mean_class = torch.from_numpy(np.zeros((num_class,num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_class = np.zeros((num_class,num_class))\n",
    "#     var_n = np.zeros(num_class)\n",
    "#     mean_var = torch.from_numpy(np.zeros((num_class))).cuda().type(torch.cuda.FloatTensor)\n",
    "#     mean_var = np.zeros((num_class))\n",
    " \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,h1,h2,h3,h4,h5,h6,h7  = model(inputs)\n",
    "        \n",
    "        conv_list = [h1, h2, h3, h4, h5]\n",
    "        conv_diff = torch.zeros(inputs.shape[0]).cuda()\n",
    "        \n",
    "        for out_layer in conv_list:\n",
    "            tmp_norm = torch.norm(out_layer,dim=(2,3))\n",
    "            tmp1 = torch.sum(tmp_norm[:,:out_layer.shape[1]//2],1 )\n",
    "            tmp2 = torch.sum(tmp_norm[:,out_layer.shape[1]//2:],1 )\n",
    "            tmp_diff = (tmp1-tmp2) ** 2 / out_layer.shape[2:].numel()\n",
    "            \n",
    "#             tmp1 = torch.sum(out_layer[:,:out_layer.shape[1]//2,:,:],1 )\n",
    "#             tmp2 = torch.sum(out_layer[:,out_layer.shape[1]//2:,:,:],1 )\n",
    "#             tmp_diff = torch.norm(tmp1 - tmp2, dim = (1,2))\n",
    "#             tmp_diff = tmp_diff ** 2 / out_layer.shape[2:].numel()\n",
    "            conv_diff += tmp_diff\n",
    "#         print(\"conv_diff: \\n\", conv_diff)\n",
    "        fc_list = [ h6, h7, outputs]\n",
    "        fc_diff = torch.zeros(inputs.shape[0]).cuda()\n",
    "        for out_layer in fc_list:\n",
    "            # hidden_outputs.shape\n",
    "            hidden_map = torch.ones(out_layer.shape[1]).cuda()\n",
    "            hidden_map[out_layer.shape[1]//2:] = -1\n",
    "        #     torch.sum(hidden_map)\n",
    "            hd_diff_map = out_layer * hidden_map\n",
    "            # hd_diff_map.shape\n",
    "            hd_diff = torch.sum(hd_diff_map, 1)\n",
    "            var_hd = hd_diff ** 2 #/ hd_diff_map.shape[1]\n",
    "#             var_hd = torch.abs(hd_diff)\n",
    "            fc_diff += var_hd\n",
    "#         print(\"fc_diff: \\n\", fc_diff)\n",
    "\n",
    "#         diff_var = (conv_diff + fc_diff).mean() * alpha\n",
    "        diff_var = conv_diff.mean() * alpha\n",
    "        \n",
    "        soft_outputs = softmax(outputs)\n",
    "        max_outputs = torch.max(soft_outputs,1).values\n",
    "        min_outputs = torch.min(max_outputs)\n",
    "        \n",
    "#         mean_outputs = torch.mean(soft_outputs,0)\n",
    "#         var = -torch.mean(torch.sum((soft_outputs - mean_outputs).pow(2),1))\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "#         var_n[targets[i]] = var_n[targets[i]] + 1\n",
    "            var_n[targets[i]] += 1\n",
    "    #         mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i]/var_n[targets[i]]\n",
    "            mean_class[targets[i]] = mean_class[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + soft_outputs[i].data.cpu().numpy()/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - mean_class[targets[i]]).pow(2))/var_n[targets[i]]\n",
    "    #         mean_var[targets[i]] = mean_var[targets[i]]*(var_n[targets[i]]-1)/var_n[targets[i]] + torch.sum((soft_outputs[i] - torch.from_numpy(mean_class[targets[i]]).cuda()).pow(2))/var_n[targets[i]]\n",
    "\n",
    "        temp_mean = torch.from_numpy(mean_class).cuda()\n",
    "#         mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2))\n",
    "        mean_var = torch.sum((soft_outputs - temp_mean[targets]).pow(2),1)\n",
    "    \n",
    "        var = torch.mean(mean_var)*beta\n",
    "#         var = -mean_var * alpha\n",
    "        \n",
    "        loss = criterion(outputs, targets) + var + diff_var\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        losses_var.update(var.data, inputs.size()[0])\n",
    "        losses_diff.update(diff_var.data, inputs.size()[0])\n",
    "        conv_diffs.update(conv_diff.mean().data, inputs.size()[0])\n",
    "        fc_diffs.update(fc_diff.mean().data, inputs.size()[0])\n",
    "        top1.update(prec1, inputs.size()[0])\n",
    "        top5.update(prec5, inputs.size()[0])\n",
    "        max_train_outputs.update(torch.mean(max_outputs), inputs.size()[0])\n",
    "        min_train_outputs.update(min_outputs, inputs.size()[0])\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size})| Loss: {loss:.4f} |var: {loss_var:.4f} | diff: {loss_d:.4f}| conv: {conv_d:.4f}| fc: {fc_d:.4f} | Max: {tmax:.4f}-{tmin:.4f}| top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "#                     bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_var=losses_var.avg,\n",
    "                    loss_d=losses_diff.avg,\n",
    "                    conv_d=conv_diffs.avg,\n",
    "                    fc_d=fc_diffs.avg,\n",
    "                    tmax = max_train_outputs.avg,\n",
    "                    tmin = min_train_outputs.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "    return (losses.avg,losses_var.avg, losses_diff.avg,top1.avg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# h2n = torch.norm(h2, dim = (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_,_  = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs ,_,_,_,_,_,_,_,_ = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),10))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = pred_outputs#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "            if epoch == 9:\n",
    "                print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "                print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_train_softmax(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "#         pred_outputs = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),pred_outputs.size(1)))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs) #torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "#         print(attack_model_input.shape, infer_input_one_hot.shape)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "        \n",
    "#         print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#         print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('attack_train--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def privacy_test_softmax(trainloader, model, inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    first_id = -1\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs,_,_,_,_,_,_,_  = model(model_input)\n",
    "#         pred_outputs = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),pred_outputs.size(1)))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        attack_model_input = softmax(pred_outputs)#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "#         is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.ones(v_tr_input.size(0)),np.zeros(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data, model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "#         plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('privacy_test--({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=100,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "#             if epoch == 9:\n",
    "#                 print ( \"member_output: \" ,member_output.data.cpu().numpy())\n",
    "#                 print(\"v_is_member_labels: \",v_is_member_labels.data.cpu().numpy())\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_sort_softmax(trainloader, model, attack_model, criterion,\n",
    "                              attack_criterion, optimizer, attack_optimizer, epoch, use_cuda, num_batchs=100000,\n",
    "                              skip_batch=0):\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    attack_model.train()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // att_batch_size), (len(train_data) // att_batch_size)) + 1\n",
    "\n",
    "    # print (skip_batch, len_t)\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_ = model(inputs_attack)\n",
    "\n",
    "        # classifier_input = torch.cat((inputs, inputs_attack))\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        #         classifier_targets = comb_targets.clone().view([-1]).type(torch.cuda.LongTensor)\n",
    "\n",
    "        loss_attack = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss_attack.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        # print ( attack_output.data.cpu().numpy(),v_is_member_labels.data.cpu().numpy() ,attack_input.data.cpu().numpy())\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        attack_optimizer.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        attack_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attack_sort_softmax(testloader, model, attack_model, criterion, attack_criterion,\n",
    "                             optimizer, attack_optimizer, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    attack_model.eval()\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "    batch_size = att_batch_size\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    sum_correct = 0.0\n",
    "\n",
    "    end = time.time()\n",
    "    # len_t = min((len(attack_data) // batch_size), (len(train_data) // batch_size)) + 1\n",
    "\n",
    "    for ind, ((tr_input, tr_target), (te_input, te_target)) in testloader:\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        if use_cuda:\n",
    "            inputs, targets = tr_input.cuda(), tr_target.cuda()\n",
    "            inputs_attack, targets_attack = te_input.cuda(), te_target.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        inputs_attack, targets_attack = torch.autograd.Variable(inputs_attack), torch.autograd.Variable(targets_attack)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_,_,_,_,_,_  = model(inputs)\n",
    "        outputs_non,_,_,_,_,_,_,_  = model(inputs_attack)\n",
    "\n",
    "        comb_inputs = torch.cat((outputs, outputs_non))\n",
    "        sort_inputs, indices = torch.sort(comb_inputs)\n",
    "        # print (comb_inputs.size(),comb_targets.size())\n",
    "        attack_input = softmax(sort_inputs)  # torch.cat((comb_inputs,comb_targets),1)\n",
    "\n",
    "        # attack_output = attack_model(att_inp).view([-1])\n",
    "        attack_output, _ = attack_model(attack_input)\n",
    "        attack_output = attack_output.view([-1])\n",
    "        # attack_output = attack_model(attack_input).view([-1])\n",
    "        att_labels = np.zeros((inputs.size()[0] + inputs_attack.size()[0]))\n",
    "        att_labels[:inputs.size()[0]] = 1.0\n",
    "        att_labels[inputs.size()[0]:] = 0.0\n",
    "        is_member_labels = torch.from_numpy(att_labels).type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            is_member_labels = is_member_labels.cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels)\n",
    "\n",
    "        loss = attack_criterion(attack_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1,p5 = accuracy(attack_output.data, v_is_member_labels.data, topk=(1,2))\n",
    "\n",
    "        prec1 = np.mean(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        losses.update(loss.data, attack_input.size()[0])\n",
    "        top1.update(prec1, attack_input.size()[0])\n",
    "\n",
    "        correct = np.sum(\n",
    "            np.equal((attack_output.data.cpu().numpy() > 0.5), (v_is_member_labels.data.cpu().numpy() > 0.5)))\n",
    "        sum_correct += correct\n",
    "        # raise\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if ind % 10 == 0:\n",
    "            print(\n",
    "                '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind + 1,\n",
    "                    size=25,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                ))\n",
    "\n",
    "    #         break\n",
    "    #     return (losses.avg, top1.avg)  #,prec1,attack_output,  v_is_member_labels)\n",
    "\n",
    "    return (losses.avg, top1.avg, sum_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset='cifar100'\n",
    "checkpoint_path='./checkpoints_cifar100'\n",
    "train_batch=200\n",
    "test_batch=100\n",
    "lr=0.001\n",
    "epochs=60\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))\n",
    "        \n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    mkdir_p(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# global best_acc\n",
    "start_epoch = 0  # start_ from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing dataset %s' % dataset)\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "\n",
    "# prepare test data parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    dataloader = datasets.CIFAR10\n",
    "    num_classes = 10\n",
    "else:\n",
    "    dataloader = datasets.CIFAR100\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"==> creating model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AlexNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = model.cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_attack = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_admm = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Resume\n",
    "title = 'cifar-100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_privacy=200\n",
    "trainset = dataloader(root='./data100', train=True, download=True, transform=transform_train)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset_private = dataloader(root='./data100', train=True, download=True, transform=transform_test)\n",
    "trainloader_private = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "testset = dataloader(root='./data100', train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data100', train=True, download=True, transform=transform_test)\n",
    "testset = dataloader(root='./data100', train=False, download=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r = np.arange(50000)\n",
    "# np.random.shuffle(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(25000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(25000,50000):\n",
    "    private_testset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.shuffle(r)\n",
    "  \n",
    "for i in range(5000):\n",
    "    private_trainset_intest.append(testset[r[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start train\n",
    "dataset='cifar100'\n",
    "checkpoint_path='./checkpoints_cifar100'\n",
    "# train_batch=400\n",
    "# test_batch=200\n",
    "lr=0.01\n",
    "# lr=0.0001\n",
    "epochs=100\n",
    "state={}\n",
    "state['lr']=lr\n",
    "print(checkpoint_path)\n",
    "# model = AlexNet(num_classes)\n",
    "# model = AlexNet_fc(num_classes, q = 90, alpha = 0.5)\n",
    "# model = AlexNet_fc_replace(num_classes, q = 90, alpha = 1)\n",
    "model = AlexNet_conv(num_classes, q = 90, alpha = 1.5)\n",
    "# model = AlexNet_comb_mid(num_classes,qconv_l = 100,qconv_h = 90, qfc_h =100, aconv_h = 2, afc_h = 1)\n",
    "# model = AlexNet_comb_rand(num_classes, qconv_h = 90, aconv_h = 2)\n",
    "# model = AlexNet_comb_rand_mod1(num_classes, qconv_h = 90, aconv_h = 2)\n",
    "\n",
    "# model = AlexNet_b(num_classes)\n",
    "# model = AlexNet_mod(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [700, 1500]:\n",
    "        state['lr'] *= 10\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "    if epoch in [1800,100,900,150]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "#     elif epoch == 60:\n",
    "#         state['lr'] = 0.005\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  with random clip\n",
    "is_best=False\n",
    "best_acc=0.0\n",
    "start_epoch=0\n",
    "alpha = 2\n",
    "beta = 0\n",
    "num_class = 100\n",
    "epochs=150\n",
    "mean_class = np.zeros((num_class,num_class))\n",
    "var_n = np.zeros(num_class)\n",
    "\n",
    "test_acc_res = []\n",
    "test_loss_res = []\n",
    "train_acc_res = []\n",
    "train_loss_res = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "#     train_enum = enumerate(trainloader)\n",
    "#     train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "    train_loss,train_var,train_svar, train_acc = train_half_diff_fc_min_var(trainloader, model, criterion, optimizer, mean_class, var_n, epoch, use_cuda, num_class = num_class, alpha = alpha, beta = beta)\n",
    "\n",
    "    train_acc_res.append(train_acc.item())\n",
    "    train_loss_res.append(train_loss.item())\n",
    "    \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "    test_acc_res.append(test_acc.item())\n",
    "    test_loss_res.append(test_loss.item())\n",
    "    \n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}, best_acc: {best_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc, best_acc=best_acc))\n",
    "    \n",
    "    # save model\n",
    "#     is_best = test_acc>best_acc\n",
    "#     best_acc = max(test_acc, best_acc) \n",
    "    if (is_best) or epoch+1 == epochs:\n",
    "        if test_acc > 36:\n",
    "            save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, is_best, checkpoint=checkpoint_path,filename='cifar100_fc_balance_ouput_partition_alpha2_conv90a1.5_defense_epoch%d'%(epoch+1))\n",
    "\n",
    "    \n",
    "print('Best acc:')\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  with random clip\n",
    "is_best=False\n",
    "best_acc=0.0\n",
    "start_epoch=0\n",
    "alpha = 2\n",
    "beta = 0\n",
    "num_class = 100\n",
    "epochs=150\n",
    "mean_class = np.zeros((num_class,num_class))\n",
    "var_n = np.zeros(num_class)\n",
    "\n",
    "test_acc_res = []\n",
    "test_loss_res = []\n",
    "train_acc_res = []\n",
    "train_loss_res = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "#     train_enum = enumerate(trainloader)\n",
    "#     train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "    train_loss,train_var,train_svar, train_acc = train_half_diff_fc_min_var(trainloader, model, criterion, optimizer, mean_class, var_n, epoch, use_cuda, num_class = num_class, alpha = alpha, beta = beta)\n",
    "\n",
    "    train_acc_res.append(train_acc.item())\n",
    "    train_loss_res.append(train_loss.item())\n",
    "    \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "    test_acc_res.append(test_acc.item())\n",
    "    test_loss_res.append(test_loss.item())\n",
    "    \n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}, best_acc: {best_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc, best_acc=best_acc))\n",
    "    \n",
    "    # save model\n",
    "#     is_best = test_acc>best_acc\n",
    "#     best_acc = max(test_acc, best_acc) \n",
    "    if (is_best) or epoch+1 == epochs:\n",
    "        if test_acc > 36:\n",
    "            save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, is_best, checkpoint=checkpoint_path,filename='cifar100_fc_balance_ouput_partition_alpha2_conv90a2_defense_epoch%d'%(epoch+1))\n",
    "\n",
    "    \n",
    "print('Best acc:')\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  with random clip\n",
    "is_best=False\n",
    "# best_acc=0.0\n",
    "start_epoch=60\n",
    "# test_acc_res = []\n",
    "# test_loss_res = []\n",
    "# train_acc_res = []\n",
    "# train_loss_res = []\n",
    "epochs=100\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "\n",
    "    train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    train_acc_res.append(train_acc.item())\n",
    "    train_loss_res.append(train_loss.item())\n",
    "    \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "    test_acc_res.append(test_acc.item())\n",
    "    test_loss_res.append(test_loss.item())\n",
    "    \n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}, best_acc: {best_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc, best_acc=best_acc))\n",
    "    \n",
    "    # save model\n",
    "#     is_best = test_acc>best_acc\n",
    "#     best_acc = max(test_acc, best_acc) \n",
    "    if (is_best) or epoch+1 == epochs:\n",
    "        if best_acc > 30:\n",
    "            save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'acc': test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, is_best, checkpoint=checkpoint_path,filename='cifar100_ori_epoch%d'%(epoch+1))\n",
    "    \n",
    "    \n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "# model = AlexNet(num_classes)\n",
    "# model = AlexNet_fc(num_classes, q = 50, alpha = 20)\n",
    "# model = AlexNet_conv(num_classes, q = 75, alpha = 2)\n",
    "model = AlexNet_comb_mid(num_classes,qconv_l = 100,qconv_h = 75, qfc_h =100, afc_h =1, aconv_l = 1, aconv_h=2)\n",
    "# model = AlexNet_comb_rand(num_classes, qconv_l = 70, aconv_l = 2, qconv_h = 100, aconv_h = 1.5)\n",
    "# model = AlexNet_comb_rand(num_classes,  qconv_h = 70, aconv_h = 2)\n",
    "# model = AlexNet_comb_rand_mod1(num_classes, qconv_h = 60, aconv_h = 2)\n",
    "# model = AlexNet_mod(num_classes)s\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#training and testing the attack result from the checkpoint\n",
    "\n",
    "resume='./checkpoints_cifar100/cifar100_min_var_beta1000_conv90a2_nonzero_defense_epoch44'\n",
    "# resume='./checkpoints_cifar100/cifar100_alexnet_epoch19'\n",
    "# resume='./checkpoints_cifar100/cifar100_fc_balance_ouput_partition_alpha2_conv90a1.5_defense_epoch61'\n",
    "# resume='./checkpoints_cifar100/cifar100_epoch21'\n",
    "# resume='./checkpoints_cifar100/adversary_reg_1_alpha6_epoch61'\n",
    "# resume='./checkpoints_cifar100/cifar100_min_var_beta2000_defense_epoch62'\n",
    "# resume='./checkpoints_cifar100/cifar100_min_var_beta200_defense_epoch62'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load(resume)\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer']) \n",
    "epoch=checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "train_loss, final_train_acc = test(trainloader, model, criterion, epoch, use_cuda)\n",
    "print ('Trainset Classification accuracy: %.2f'%(final_train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_loss, final_test_acc, test_class, test_correct = test_by_class(testloader, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "print(test_class)\n",
    "print(test_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loss, final_train_acc, train_class, train_correct = test_by_class(trainloader, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_train_acc))\n",
    "print(train_class)\n",
    "print(train_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "defense_model = Defense_Model(100)\n",
    "defense_model = torch.nn.DataParallel(defense_model).cuda()\n",
    "defense_criterion = nn.MSELoss()\n",
    "# defense_criterion = nn.CrossEntropyLoss()\n",
    "at_lr = 0.0001\n",
    "state={}\n",
    "state['lr']=at_lr\n",
    "defense_optimizer = optim.Adam(defense_model.parameters(),lr=at_lr)\n",
    "defense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "resume_defense='./checkpoints_cifar100/cifar100_alexnet_epoch88_softmax_sort_NN_epoch100'\n",
    "# resume_defense='./checkpoints_cifar100/cifar100_min_var_beta1000_conv90a2_nonzero_defense_epoch44_infer_c70a2_c90a1.7_softmax_sort_NN_epoch67'\n",
    "\n",
    "print('==> Resuming attack model from checkpoint..')\n",
    "assert os.path.isfile(resume_defense), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "defense_model.load_state_dict(checkpoint_defense['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "att_batch_size=200\n",
    "attack_train_loss, attack_train_acc, attack_correct_sum = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                            criterion, defense_criterion, optimizer,\n",
    "                                                            defense_optimizer,\n",
    "                                                            epoch, use_cuda)\n",
    "\n",
    "print ('attack loss: {test_loss: .4f}, attack acc: {test_acc: .4f}, attack correct sum: {attack_correct_sum: .1f}'.format(test_loss=attack_train_loss, test_acc=attack_train_acc,attack_correct_sum=attack_correct_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss( reduction = 'none')\n",
    "model.eval()\n",
    "test_loss_list = []\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "test_prob = []\n",
    "test_logits = []\n",
    "test_label = []\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    # measure data loading time\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "    # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "    outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss_value = loss.data.cpu().numpy()\n",
    "    outputs_value = outputs.data.cpu().numpy()\n",
    "    prob_outputs = softmax(outputs)\n",
    "    prob_outputs_value = prob_outputs.data.cpu().numpy()\n",
    "    targets_value = targets.data.cpu().numpy()\n",
    "    for i in range(len(loss_value)):\n",
    "        test_loss_list.append(loss_value[i])\n",
    "        test_logits.append(outputs_value[i])\n",
    "        test_prob.append(prob_outputs_value[i])\n",
    "        test_label.append(targets_value[i])\n",
    "#     break\n",
    "test_loss_array = np.asarray(test_loss_list)\n",
    "\n",
    "test_prob_array = np.asarray(test_prob)\n",
    "test_logits_array = np.asarray(test_logits)\n",
    "test_label = np.asarray(test_label)\n",
    "# test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(test_prob_array.shape, test_label.shape)\n",
    "\n",
    "sort_test_prob=np.sort(test_prob_array,axis=1)\n",
    "sort_test_logits=np.sort(test_logits_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.mean(np.var(test_prob_array,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss_array\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "# plt.hist(test_loss_array, range = (test_loss_array.min(), 0.05), bins=100, density=True, log=True)\n",
    "plt.hist(test_loss_array, bins=100, density=True, log=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss( reduction = 'none')\n",
    "softmax = nn.Softmax()\n",
    "model.eval()\n",
    "train_loss_list = []\n",
    "train_prob = []\n",
    "train_logits = []\n",
    "train_label = []\n",
    "for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "    # measure data loading time\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "    # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "    outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "#         outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    outputs_value = outputs.data.cpu().numpy()\n",
    "    loss_value = loss.data.cpu().numpy()\n",
    "    prob_outputs = softmax(outputs)\n",
    "    prob_outputs_value = prob_outputs.data.cpu().numpy()\n",
    "    targets_value = targets.data.cpu().numpy()\n",
    "    \n",
    "    for i in range(len(loss_value)):\n",
    "        train_loss_list.append(loss_value[i])\n",
    "        train_logits.append(outputs_value[i])\n",
    "        train_prob.append(prob_outputs_value[i])\n",
    "        train_label.append(targets_value[i])\n",
    "#     break\n",
    "\n",
    "train_loss_array = np.asarray(train_loss_list)\n",
    "train_prob_array = np.asarray(train_prob)\n",
    "train_logits_array = np.asarray(train_logits)\n",
    "train_label = np.asarray(train_label)\n",
    "# test_hidden_layer_logits_array = np.asarray(test_hidden_layer_logits)\n",
    "print(train_prob_array.shape, train_label.shape)\n",
    "\n",
    "sort_train_prob=np.sort(train_prob_array,axis=1)\n",
    "sort_train_logits=np.sort(train_logits_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.mean(np.var(train_prob_array,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "# plt.hist(train_loss_array, range = (train_loss_array.min(), 0.05), bins = 100, density=True)\n",
    "plt.hist(train_loss_array[:10000], bins=100, density=True, log=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = pyplot.figure(figsize=(15,10))\n",
    "pyplot.hist(train_loss_array[:10000], bins=100, density=True, log=True, alpha=0.5, label = 'trainset')\n",
    "pyplot.hist(test_loss_array, bins=100, density=True, log=True, alpha=0.5, label = 'testset')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(1000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if test_label[i] == 0:\n",
    "        plt.plot(test_prob_array[i])\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(30,20))\n",
    "count1 = 0\n",
    "for i in range(10000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if test_label[i] == 0:\n",
    "        count1 += 1\n",
    "        axs[0,0].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 1:\n",
    "#         count1 += 1\n",
    "        axs[0,1].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 2:\n",
    "        axs[1,0].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 3:\n",
    "        axs[1,1].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 4:\n",
    "        axs[2,0].plot(test_prob_array[i], 'g')\n",
    "    if test_label[i] == 5:\n",
    "        axs[2,1].plot(test_prob_array[i], 'g')      \n",
    "    if test_label[i] == 6:\n",
    "        axs[3,0].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 7:\n",
    "        axs[3,1].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 8:\n",
    "        axs[4,0].plot(test_prob_array[i], 'm')\n",
    "    if test_label[i] == 9:\n",
    "        axs[4,1].plot(test_prob_array[i], 'm')  \n",
    "\n",
    "count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 4, figsize=(60,40))\n",
    "count1 = 0\n",
    "for i in range(10000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if test_label[i] == 0:\n",
    "        count1 += 1\n",
    "        axs[0,0].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 1:\n",
    "#         count1 += 1\n",
    "        axs[0,1].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 2:\n",
    "        axs[1,0].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 3:\n",
    "        axs[1,1].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 4:\n",
    "        axs[2,0].plot(test_prob_array[i], 'g')\n",
    "    if test_label[i] == 5:\n",
    "        axs[2,1].plot(test_prob_array[i], 'g')      \n",
    "    if test_label[i] == 6:\n",
    "        axs[3,0].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 7:\n",
    "        axs[3,1].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 8:\n",
    "        axs[4,0].plot(test_prob_array[i], 'm')\n",
    "    if test_label[i] == 9:\n",
    "        axs[4,1].plot(test_prob_array[i], 'm')  \n",
    "    \n",
    "    if test_label[i] == 10:\n",
    "        count1 += 1\n",
    "        axs[0,2].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 11:\n",
    "#         count1 += 1\n",
    "        axs[0,3].plot(test_prob_array[i], 'r')\n",
    "    if test_label[i] == 12:\n",
    "        axs[1,2].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 13:\n",
    "        axs[1,3].plot(test_prob_array[i], 'y')\n",
    "    if test_label[i] == 14:\n",
    "        axs[2,2].plot(test_prob_array[i], 'g')\n",
    "    if test_label[i] == 15:\n",
    "        axs[2,3].plot(test_prob_array[i], 'g')      \n",
    "    if test_label[i] == 16:\n",
    "        axs[3,2].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 17:\n",
    "        axs[3,3].plot(test_prob_array[i], 'b')\n",
    "    if test_label[i] == 18:\n",
    "        axs[4,2].plot(test_prob_array[i], 'm')\n",
    "    if test_label[i] == 19:\n",
    "        axs[4,3].plot(test_prob_array[i], 'm')  \n",
    "\n",
    "count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(30,20))\n",
    "count1 = 0\n",
    "for i in range(10000):\n",
    "#     plt.plot(f_evaluate_origin[i])\n",
    "    if train_label[i] == 0:\n",
    "        count1 += 1\n",
    "        axs[0,0].plot(train_prob_array[i], 'r')\n",
    "    if train_label[i] == 1:\n",
    "        axs[0,1].plot(train_prob_array[i], 'r')\n",
    "    if train_label[i] == 2:\n",
    "        axs[1,0].plot(train_prob_array[i], 'y')\n",
    "    if train_label[i] == 3:\n",
    "        axs[1,1].plot(train_prob_array[i], 'y')\n",
    "    if train_label[i] == 4:\n",
    "        axs[2,0].plot(train_prob_array[i], 'g')\n",
    "    if train_label[i] == 5:\n",
    "        axs[2,1].plot(train_prob_array[i], 'g')      \n",
    "    if train_label[i] == 6:\n",
    "        axs[3,0].plot(train_prob_array[i], 'b')\n",
    "    if train_label[i] == 7:\n",
    "        axs[3,1].plot(train_prob_array[i], 'b')\n",
    "    if train_label[i] == 8:\n",
    "        axs[4,0].plot(train_prob_array[i], 'm')\n",
    "    if train_label[i] == 9:\n",
    "        axs[4,1].plot(train_prob_array[i], 'm')  \n",
    "\n",
    "count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_max_accuracy(y_true, probs, thresholds=None):\n",
    "    \n",
    "    \"\"\"Return the max accuracy possible given the correct labels and guesses. Will try all thresholds unless passed.\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        y_true: True label of `in' or `out' (member or non-member, 1/0)\n",
    "        probs: The scalar to threshold\n",
    "        thresholds: In a blackbox setup with a shadow/source model, the threshold obtained by the source model can be passed\n",
    "          here for attackin the target model. This threshold will then be used.\n",
    "\n",
    "      Returns: max accuracy possible, accuracy at the threshold passed (if one was passed), the max precision possible,\n",
    "       and the precision at the threshold passed.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    for thresh in thresholds:\n",
    "        accuracy_scores.append(accuracy_score(y_true,\n",
    "                                              [1 if m > thresh else 0 for m in probs]))\n",
    "        precision_scores.append(precision_score(y_true, [1 if m > thresh else 0 for m in probs]))\n",
    "\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    precisions = np.array(precision_scores)\n",
    "    max_accuracy = accuracies.max()\n",
    "    max_precision = precisions.max()\n",
    "    max_accuracy_threshold = thresholds[accuracies.argmax()]\n",
    "    max_precision_threshold = thresholds[precisions.argmax()]\n",
    "    return max_accuracy, max_accuracy_threshold, max_precision, max_precision_threshold\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold(source_m, source_stats, target_m, target_stats):\n",
    "    \"\"\" Train a threshold attack model and get teh accuracy on source and target models.\n",
    "\n",
    "  Args:\n",
    "    source_m: membership labels for source dataset (1 for member, 0 for non-member)\n",
    "    source_stats: scalar values to threshold (attack features) for source dataset\n",
    "    target_m: membership labels for target dataset (1 for member, 0 for non-member)\n",
    "    target_stats: scalar values to threshold (attack features) for target dataset\n",
    "\n",
    "  Returns: best acc from source thresh, precision @ same threshold, threshold for best acc,\n",
    "    precision at the best threshold for precision. all tuned on source model.\n",
    "\n",
    "    \"\"\"\n",
    "    # find best threshold on source data\n",
    "    acc_source, t, prec_source, tprec = get_max_accuracy(source_m, source_stats)\n",
    "\n",
    "    # find best accuracy on test data (just to check how much we overfit)\n",
    "    acc_test, _, prec_test, _ = get_max_accuracy(target_m, target_stats)\n",
    "\n",
    "    # get the test accuracy at the threshold selected on the source data\n",
    "    acc_test_t, _, _, _ = get_max_accuracy(target_m, target_stats, thresholds=[t])\n",
    "    _, _, prec_test_t, _ = get_max_accuracy(target_m, target_stats, thresholds=[tprec])\n",
    "    print(\"acc src: {}, acc test (best thresh): {}, acc test (src thresh): {}, thresh: {}\".format(acc_source, acc_test,\n",
    "                                                                                                acc_test_t, t))\n",
    "    print(\n",
    "    \"prec src: {}, prec test (best thresh): {}, prec test (src thresh): {}, thresh: {}\".format(prec_source, prec_test,\n",
    "                                                                                               prec_test_t, tprec))\n",
    "\n",
    "    return acc_test_t, prec_test_t, t, tprec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "INF = float(\"inf\")\n",
    "\n",
    "\n",
    "def carlini_wagner_l2(\n",
    "    model_fn,\n",
    "    x,\n",
    "    n_classes,\n",
    "    y=None,\n",
    "    targeted=False,\n",
    "    lr=5e-3,\n",
    "    confidence=0,\n",
    "    clip_min=0,\n",
    "    clip_max=1,\n",
    "    initial_const=1e-2,\n",
    "    binary_search_steps=5,\n",
    "    max_iterations=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    This attack was originally proposed by Carlini and Wagner. It is an\n",
    "    iterative attack that finds adversarial examples on many defenses that\n",
    "    are robust to other attacks.\n",
    "    Paper link: https://arxiv.org/abs/1608.04644\n",
    "\n",
    "    At a high level, this attack is an iterative attack using Adam and\n",
    "    a specially-chosen loss function to find adversarial examples with\n",
    "    lower distortion than other attacks. This comes at the cost of speed,\n",
    "    as this attack is often much slower than others.\n",
    "\n",
    "    :param model_fn: a callable that takes an input tensor and returns\n",
    "              the model logits. The logits should be a tensor of shape\n",
    "              (n_examples, n_classes).\n",
    "    :param x: input tensor of shape (n_examples, ...), where ... can\n",
    "              be any arbitrary dimension that is compatible with\n",
    "              model_fn.\n",
    "    :param n_classes: the number of classes.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true,\n",
    "              then provide the target label. Otherwise, only provide\n",
    "              this parameter if you'd like to use true labels when\n",
    "              crafting adversarial samples. Otherwise, model predictions\n",
    "              are used as labels to avoid the \"label leaking\" effect\n",
    "              (explained in this paper:\n",
    "              https://arxiv.org/abs/1611.01236). If provide y, it\n",
    "              should be a 1D tensor of shape (n_examples, ).\n",
    "              Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or\n",
    "              untargeted? Untargeted, the default, will try to make the\n",
    "              label incorrect. Targeted will instead try to move in the\n",
    "              direction of being more like y.\n",
    "    :param lr: (optional) float. The learning rate for the attack\n",
    "              algorithm. Default is 5e-3.\n",
    "    :param confidence: (optional) float. Confidence of adversarial\n",
    "              examples: higher produces examples with larger l2\n",
    "              distortion, but more strongly classified as adversarial.\n",
    "              Default is 0.\n",
    "    :param clip_min: (optional) float. Minimum float value for\n",
    "              adversarial example components. Default is 0.\n",
    "    :param clip_max: (optional) float. Maximum float value for\n",
    "              adversarial example components. Default is 1.\n",
    "    :param initial_const: The initial tradeoff-constant to use to tune the\n",
    "              relative importance of size of the perturbation and\n",
    "              confidence of classification. If binary_search_steps is\n",
    "              large, the initial constant is not important. A smaller\n",
    "              value of this constant gives lower distortion results.\n",
    "              Default is 1e-2.\n",
    "    :param binary_search_steps: (optional) int. The number of times we\n",
    "              perform binary search to find the optimal tradeoff-constant\n",
    "              between norm of the perturbation and confidence of the\n",
    "              classification. Default is 5.\n",
    "    :param max_iterations: (optional) int. The maximum number of\n",
    "              iterations. Setting this to a larger value will produce\n",
    "              lower distortion results. Using only a few iterations\n",
    "              requires a larger learning rate, and will produce larger\n",
    "              distortion results. Default is 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def compare(pred, label, is_logits=False):\n",
    "        \"\"\"\n",
    "        A helper function to compare prediction against a label.\n",
    "        Returns true if the attack is considered successful.\n",
    "\n",
    "        :param pred: can be either a 1D tensor of logits or a predicted\n",
    "                class (int).\n",
    "        :param label: int. A label to compare against.\n",
    "        :param is_logits: (optional) bool. If True, treat pred as an\n",
    "                array of logits. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert logits to predicted class if necessary\n",
    "        if is_logits:\n",
    "            pred_copy = pred.clone().detach()\n",
    "            pred_copy[label] += -confidence if targeted else confidence\n",
    "            pred = torch.argmax(pred_copy)\n",
    "\n",
    "        return pred == label if targeted else pred != label\n",
    "\n",
    "    if y is None:\n",
    "        # Using model predictions as ground truth to avoid label leaking\n",
    "        pred = model_fn(x)\n",
    "        y = torch.argmax(pred, 1)\n",
    "\n",
    "    # Initialize some values needed for binary search on const\n",
    "    lower_bound = [0.0] * len(x)\n",
    "    upper_bound = [1e10] * len(x)\n",
    "    const = x.new_ones(len(x), 1) * initial_const\n",
    "\n",
    "    o_bestl2 = [INF] * len(x)\n",
    "    o_bestscore = [-1.0] * len(x)\n",
    "    x = torch.clamp(x, clip_min, clip_max)\n",
    "    ox = x.clone().detach()  # save the original x\n",
    "    o_bestattack = x.clone().detach()\n",
    "\n",
    "    # Map images into the tanh-space\n",
    "    x = (x - clip_min) / (clip_max - clip_min)\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    x = x * 2 - 1\n",
    "    x = torch.arctanh(x * 0.999999)\n",
    "    # x = torch.atanh(x * 0.999999)\n",
    "\n",
    "    # Prepare some variables\n",
    "    modifier = torch.zeros_like(x, requires_grad=True)\n",
    "    y_onehot = torch.nn.functional.one_hot(y, n_classes).to(torch.float)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    f_fn = lambda real, other, targeted: torch.max(\n",
    "        ((other - real) if targeted else (real - other)) + confidence,\n",
    "        torch.tensor(0.0).to(real.device),\n",
    "    )\n",
    "    l2dist_fn = lambda x, y: torch.pow(x - y, 2).sum(list(range(len(x.size())))[1:])\n",
    "    optimizer = torch.optim.Adam([modifier], lr=lr)\n",
    "\n",
    "    # Outer loop performing binary search on const\n",
    "    for outer_step in range(binary_search_steps):\n",
    "        # Initialize some values needed for the inner loop\n",
    "        bestl2 = [INF] * len(x)\n",
    "        bestscore = [-1.0] * len(x)\n",
    "\n",
    "        # Inner loop performing attack iterations\n",
    "        for i in range(max_iterations):\n",
    "            # One attack step\n",
    "            new_x = (torch.tanh(modifier + x) + 1) / 2\n",
    "            new_x = new_x * (clip_max - clip_min) + clip_min\n",
    "            logits,_,_,_,_,_,_,_ = model_fn(new_x)\n",
    "#             print(logits.shape)\n",
    "#             print(logits[0])\n",
    "#             print(y_onehot.shape)\n",
    "#             print(y_onehot[0])\n",
    "            real = torch.sum(y_onehot * logits, 1)\n",
    "            other, _ = torch.max((1 - y_onehot) * logits - y_onehot * 1e4, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            f = f_fn(real, other, targeted)\n",
    "            l2 = l2dist_fn(new_x, ox)\n",
    "            loss = (const * f + l2).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update best results\n",
    "            for n, (l2_n, logits_n, new_x_n) in enumerate(zip(l2, logits, new_x)):\n",
    "                y_n = y[n]\n",
    "                succeeded = compare(logits_n, y_n, is_logits=True)\n",
    "                if l2_n < o_bestl2[n] and succeeded:\n",
    "                    pred_n = torch.argmax(logits_n)\n",
    "                    o_bestl2[n] = l2_n\n",
    "                    o_bestscore[n] = pred_n\n",
    "                    o_bestattack[n] = new_x_n\n",
    "                    # l2_n < o_bestl2[n] implies l2_n < bestl2[n] so we modify inner loop variables too\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = pred_n\n",
    "                elif l2_n < bestl2[n] and succeeded:\n",
    "                    bestl2[n] = l2_n\n",
    "                    bestscore[n] = torch.argmax(logits_n)\n",
    "\n",
    "        # Binary search step\n",
    "        for n in range(len(x)):\n",
    "            y_n = y[n]\n",
    "\n",
    "            if compare(bestscore[n], y_n) and bestscore[n] != -1:\n",
    "                # Success, divide const by two\n",
    "                upper_bound[n] = min(upper_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "            else:\n",
    "                # Failure, either multiply by 10 if no solution found yet\n",
    "                # or do binary search with the known upper bound\n",
    "                lower_bound[n] = max(lower_bound[n], const[n])\n",
    "                if upper_bound[n] < 1e9:\n",
    "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
    "                else:\n",
    "                    const[n] *= 10\n",
    "\n",
    "    return o_bestattack.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dists(model_fn, dataloader, attack=\"CW\", max_samples=100, input_dim=[None, 32, 32, 3], n_classes=10):\n",
    "    \"\"\"Calculate untargeted distance to decision boundary for Adv-x MI attack.\n",
    "      :param model: model to approximate distances on (attack).\n",
    "      :param ds: tf dataset should be either the training set or the test set.\n",
    "      :param attack: \"CW\" for carlini wagner or \"HSJ\" for hop skip jump\n",
    "      :param max_samples: maximum number of samples to take from the ds\n",
    "      :return: an array of the first samples from the ds, of len max_samples, with the untargeted distances. \n",
    "    \"\"\"\n",
    "#   # switch to TF1 style\n",
    "#   sess = K.get_session()\n",
    "#   x = tf.placeholder(dtype=tf.float32, shape=input_dim)\n",
    "#   y = tf.placeholder(dtype=tf.int32, shape=[None, n_classes])\n",
    "#   output = model_(x)\n",
    "#   model = CallableModelWrapper(lambda x: model_(x), \"logits\")\n",
    "    \n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "    if attack == \"CW\":\n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "                    x_adv_curr = carlini_wagner_l2(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), \n",
    "                                                   n_classes, targeted=False, y=targets[i].reshape(1),clip_min=inputs[i].min(),\n",
    "                                                   clip_max = inputs[i].max())\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "            \n",
    "    elif attack == \"HSJ\":\n",
    "        \n",
    "        acc = []\n",
    "        acc_adv = []\n",
    "        dist_adv = []\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # measure data loading time\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "        #         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "            outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "\n",
    "            outputs_value = outputs.data.cpu().numpy()\n",
    "            correct = torch.argmax(outputs, axis=-1) == targets\n",
    "            acc.extend(correct)\n",
    "            \n",
    "            input_shape = inputs.shape\n",
    "            x_adv_list = []\n",
    "            for i in range(len(inputs)):\n",
    "                if correct[i]:\n",
    "#                     stime = time.time()\n",
    "                    x_adv_curr = hop_skip_jump_attack(model, inputs[i:i+1],\n",
    "                                                      verbose=False,clip_min=inputs[i:i+1].min(),clip_max = inputs[i:i+1].max())\n",
    "#                     print('generate one data takes: ', time.time()-stime)\n",
    "                else:\n",
    "                    x_adv_curr = inputs[i:i+1]\n",
    "                x_adv_list.append(x_adv_curr)\n",
    "            x_adv_list = torch.cat(x_adv_list, axis=0)\n",
    "            y_pred_adv,_,_,_,_,_,_,_ = model(x_adv_list)\n",
    "            corr_adv = torch.argmax(y_pred_adv, axis=-1) == targets\n",
    "            acc_adv.extend(corr_adv)\n",
    "            n_img = inputs.permute(0,2,3,1).data.cpu().numpy()\n",
    "            img = (n_img*std)+mean\n",
    "            n_x_adv_list = x_adv_list.permute(0,2,3,1).data.cpu().numpy()\n",
    "            x = (n_x_adv_list*std)+mean\n",
    "\n",
    "            d = np.sqrt(np.sum(np.square(x-img), axis=(1,2,3)))\n",
    "#             d = torch.sqrt(torch.sum(torch.square(x_adv_list-inputs), axis=(1,2,3)))\n",
    "#             dist_adv.extend(d.data.cpu().numpy())\n",
    "            dist_adv.extend(d)\n",
    "            num_samples += len(outputs)\n",
    "            print(\"processed {} examples\".format(num_samples))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unknown attack {}\".format(attack))\n",
    "\n",
    "#   next_element = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "\n",
    "    return dist_adv[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    # measure data loading time\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "    # compute output\n",
    "#         outputs,_,_,_,_,_,_,_,_ = model(inputs)\n",
    "    outputs,_,_,_,_,_,_,_ = model(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i=0\n",
    "inputs[i:i+1].shape\n",
    "input_shape = inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stime= time.time()\n",
    "\n",
    "x_adv_curr = hop_skip_jump_attack(model, inputs[i].reshape(1,input_shape[1],input_shape[2],input_shape[3]), verbose=False,clip_min=inputs[i:i+1].min(),\n",
    "                                    clip_max = inputs[i:i+1].max())\n",
    "print(time.time() - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stime= time.time()\n",
    "\n",
    "new_x_untargeted = carlini_wagner_l2(model, inputs[i:i+1], 100, targeted=False, y=targets[i].reshape(1),clip_min=inputs[i:i+1].min(),\n",
    "                                    clip_max = inputs[i:i+1].max())\n",
    "# new_pred_untargeted = model(new_x_untargeted)\n",
    "\n",
    "print(time.time() - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "\n",
    "r = np.arange(50000)\n",
    "batch_privacy = 50\n",
    "np.random.seed(1111)\n",
    "np.random.shuffle(r)\n",
    "for i in range(1000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "    \n",
    "for i in range(25000,26000): \n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "    \n",
    "r = np.arange(10000)\n",
    "# np.random.seed(6)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(1000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,6000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================attack==============================\n",
    "\n",
    "source_train_ds = private_trainloader_intrain \n",
    "target_train_ds = private_trainloader_intest \n",
    "source_test_ds = private_testloader_intrain \n",
    "target_test_ds = private_testloader_intest\n",
    "\n",
    "source_model = model\n",
    "target_model = model\n",
    "\n",
    "input_dim = [None, 32,32,3]\n",
    "max_samples = 1000\n",
    "n_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss, train_acc1 = test(source_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc1))\n",
    "\n",
    "test_loss, train_acc2 = test(target_train_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=train_acc2))\n",
    "\n",
    "test_loss, test_acc1 = test(source_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('train_loss: {test_loss: .4f}, train_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc1))\n",
    "\n",
    "test_loss, test_acc2 = test(target_test_ds, model, criterion, epoch, use_cuda)\n",
    "print ('test_loss: {test_loss: .4f}, test_acc: {test_acc: .4f}'.format(test_loss=test_loss, test_acc=test_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print((train_acc1 - test_acc1)/2 + 50)\n",
    "\n",
    "print((train_acc2 - test_acc2)/2 + 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 1000\n",
    "n_classes = 100\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw1 = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw1 = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw1 = np.concatenate([dists_source_in_cw1, dists_source_out_cw1], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw1 = dists_source_in_cw1\n",
    "dists_target_out_cw1 = dists_source_out_cw1\n",
    "dists_target_cw1 = np.concatenate([dists_target_in_cw1, dists_target_out_cw1], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc11, prec11, _, _ = get_threshold(source_m, dists_source_cw1, target_m, dists_target_cw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"threshold on C&W:\")\n",
    "acc11, prec11, _, _ = get_threshold(source_m, dists_source_cw1, target_m, dists_target_cw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 1000\n",
    "n_classes = 100\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw = np.concatenate([dists_source_in_cw, dists_source_out_cw], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw = dists_source_in_cw\n",
    "dists_target_out_cw = dists_source_out_cw\n",
    "dists_target_cw = np.concatenate([dists_target_in_cw, dists_target_out_cw], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc1, prec1, _, _ = get_threshold(source_m, dists_source_cw, target_m, dists_target_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================HSJ==============================\n",
    "\n",
    "\n",
    "max_samples = 1000\n",
    "n_classes = 100\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                       np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with HipSkipJump (very slow)\n",
    "dists_source_in_500 = dists(source_model, source_train_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "# np.save('dists_source_in_2500.npy', dists_source_in1)\n",
    "dists_source_out_500 = dists(source_model, source_test_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "# np.save('dists_source_out_2500.npy', dists_source_out1)\n",
    "dists_source_500 = np.concatenate([dists_source_in_500, dists_source_out_500], axis=0)\n",
    "\n",
    "dists_target_in_500 = dists_source_in_500\n",
    "# dists_target_in_500 = dists(target_model, target_train_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# # np.save('dists_target_in_2500.npy', dists_target_in1)\n",
    "\n",
    "dists_target_out_500 = dists_source_out_500\n",
    "# dists_target_out_500 = dists(target_model, target_test_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# np.save('dists_target_out_2500.npy', dists_target_out1)\n",
    "dists_target_500 = np.concatenate([dists_target_in_500, dists_target_out_500], axis=0)\n",
    "print(\"threshold on HSJ:\")\n",
    "acc_500, prec_500, _, _ = get_threshold(source_m, dists_source_500, target_m, dists_target_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================cw==============================\n",
    "\n",
    "\n",
    "max_samples = 500\n",
    "n_classes = 100\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with C&W\n",
    "dists_source_in_cw = dists(source_model, source_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_out_cw = dists(source_model, source_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "dists_source_cw = np.concatenate([dists_source_in_cw, dists_source_out_cw], axis=0)\n",
    "# dists_target_in = dists(target_model, target_train_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# dists_target_out = dists(target_model, target_test_ds, attack=\"CW\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "dists_target_in_cw = dists_source_in_cw\n",
    "dists_target_out_cw = dists_source_out_cw\n",
    "dists_target_cw = np.concatenate([dists_target_in_cw, dists_target_out_cw], axis=0)\n",
    "print(\"threshold on C&W:\")\n",
    "acc1, prec1, _, _ = get_threshold(source_m, dists_source_cw, target_m, dists_target_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================HSJ==============================\n",
    "\n",
    "\n",
    "max_samples = 500\n",
    "n_classes = 100\n",
    "\n",
    "source_m = np.concatenate([np.ones(max_samples),\n",
    "                       np.zeros(max_samples)], axis=0)\n",
    "target_m = np.concatenate([np.ones(max_samples),\n",
    "                         np.zeros(max_samples)], axis=0)\n",
    "# attack with HipSkipJump (very slow)\n",
    "dists_source_in_500 = dists(source_model, source_train_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "# np.save('dists_source_in_2500.npy', dists_source_in1)\n",
    "dists_source_out_500 = dists(source_model, source_test_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "                                          input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "                                          n_classes=n_classes)\n",
    "# np.save('dists_source_out_2500.npy', dists_source_out1)\n",
    "dists_source_500 = np.concatenate([dists_source_in_500, dists_source_out_500], axis=0)\n",
    "\n",
    "dists_target_in_500 = dists_source_in_500\n",
    "# dists_target_in_500 = dists(target_model, target_train_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# # np.save('dists_target_in_2500.npy', dists_target_in1)\n",
    "\n",
    "dists_target_out_500 = dists_source_out_500\n",
    "# dists_target_out_500 = dists(target_model, target_test_ds, attack=\"HSJ\", max_samples=max_samples,\n",
    "#                                           input_dim=[None, input_dim[0], input_dim[1], input_dim[2]],\n",
    "#                                           n_classes=n_classes)\n",
    "# np.save('dists_target_out_2500.npy', dists_target_out1)\n",
    "dists_target_500 = np.concatenate([dists_target_in_500, dists_target_out_500], axis=0)\n",
    "print(\"threshold on HSJ:\")\n",
    "acc_500, prec_500, _, _ = get_threshold(source_m, dists_source_500, target_m, dists_target_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load membership inference attack \n",
    "\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root='./data100', train=True, download=True, transform=transform_train)\n",
    "testset = dataloader(root='./data100', train=False, download=False, transform=transform_test)\n",
    "\n",
    "r = np.arange(50000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,25000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(25000,50000):\n",
    "    private_trainset_intest.append(trainset[r[i]])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "# private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "# private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "# private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inferenece_model = InferenceAttack_HZ(100).cuda()\n",
    "# inferenece_model = torch.nn.DataParallel(inferenece_model).cuda()\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "\n",
    "optimizer_mem = optim.Adam(inferenece_model.parameters(), lr=at_lr )\n",
    "criterion_attack = nn.MSELoss()\n",
    "best_acc= 0.0\n",
    "batch_size=100\n",
    "epochs= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_tr_attack = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "train_te_attack = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "test_tr_attack = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "test_te_attack = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_defense='./checkpoints_cifar100/NSH_attack_softmax_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "inferenece_model.load_state_dict(checkpoint_defense['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_nsh(optimizer, epoch):\n",
    "    global state\n",
    "#     if epoch in [10, 80, 150]:\n",
    "#         state['lr'] *= 0.1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = state['lr']\n",
    "    if epoch in [30]:\n",
    "        state['lr'] *= 0.1 \n",
    "#         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(train_tr_attack, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(train_te_attack, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(test_tr_attack, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(test_te_attack, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs= 200\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_nsh(optimizer, epoch)\n",
    "    train_attack_enum = enumerate(zip(train_tr_attack,test_tr_attack))\n",
    "    \n",
    "    print('\\nEpoch: [%d | %d]' % (epoch + 1, epochs))\n",
    "    \n",
    "    train_loss, train_acc = privacy_train_softmax(train_attack_enum,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda, 10000)\n",
    "    \n",
    "    test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "    \n",
    "    print ('train acc', train_acc)\n",
    "    test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "    \n",
    "    is_best = (test_acc>best_acc) #or ((1-test_acc)>best_acc)\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "#     best_acc = max( best_acc, (1-test_acc))\n",
    "\n",
    "    print ('test acc', test_acc, best_acc)\n",
    "\n",
    "    # save model\n",
    "    if is_best:\n",
    "        best_epoch = epoch+1;\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': inferenece_model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer_mem.state_dict(),\n",
    "            }, False, checkpoint=checkpoint_path,filename='NSH_attack_softmax_best')\n",
    "        \n",
    "print('model train acc: ', final_train_acc, '  model test acc: ', final_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cifar100_fc_balance_ouput_partition_alpha2_conv90a1.5_defense_epoch135\n",
    "\n",
    "epochs= 200\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_nsh(optimizer, epoch)\n",
    "    train_attack_enum = enumerate(zip(train_tr_attack,test_tr_attack))\n",
    "    \n",
    "    print('\\nEpoch: [%d | %d]' % (epoch + 1, epochs))\n",
    "    \n",
    "    train_loss, train_acc = privacy_train_softmax(train_attack_enum,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda, 10000)\n",
    "    \n",
    "    test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "    \n",
    "    print ('train acc', train_acc)\n",
    "    test_loss, test_acc = privacy_test_softmax(test_attack_enum, model, inferenece_model, criterion_attack, optimizer_mem, epoch, use_cuda, 1000)\n",
    "    \n",
    "    is_best = (test_acc>best_acc) #or ((1-test_acc)>best_acc)\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "#     best_acc = max( best_acc, (1-test_acc))\n",
    "\n",
    "    print ('test acc', test_acc, best_acc)\n",
    "\n",
    "    # save model\n",
    "    if is_best:\n",
    "        best_epoch = epoch+1;\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': inferenece_model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer_mem.state_dict(),\n",
    "            }, False, checkpoint=checkpoint_path,filename='cifar100_fc_balance_ouput_partition_alpha2_conv90a1.5_defense_epoch135_infer60a1.5_NSH_attack_softmax_best')\n",
    "        \n",
    "print('model train acc: ', final_train_acc, '  model test acc: ', final_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "#  NN attack model\n",
    "defense_model = Defense_Model(100)\n",
    "defense_model = torch.nn.DataParallel(defense_model).cuda()\n",
    "defense_criterion = nn.MSELoss()\n",
    "# defense_criterion = nn.CrossEntropyLoss()\n",
    "att_batch_size = 100\n",
    "at_lr = 0.001\n",
    "state = {}\n",
    "state['lr'] = at_lr\n",
    "defense_optimizer = torch.optim.Adam(defense_model.parameters(), lr=at_lr)\n",
    "print(defense_model)\n",
    "best_acc = 0.0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_learning_rate_attack(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [40, 80]:\n",
    "        #     if (epoch+1)%100 == 0:\n",
    "        state['lr'] *= 0.1\n",
    "        #         state['lr'] *= 1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_defense='./checkpoints_cifar100/softmax_sort_NN_best'\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "checkpoint_defense = os.path.dirname(resume_defense)\n",
    "checkpoint_defense = torch.load(resume_defense)\n",
    "defense_model.load_state_dict(checkpoint_defense['state_dict'])\n",
    "\n",
    "\n",
    "test_attack_enum = enumerate(zip(train_te_attack,test_te_attack))\n",
    "test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "print ('test acc', test_acc, 'test_loss: ',test_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate_attack(defense_optimizer, epoch)\n",
    "\n",
    "    print('\\nEpoch: [%d | %d] , lr : %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_attack_enum = enumerate(zip(train_tr_attack, test_tr_attack))\n",
    "    train_loss, train_acc = train_attack_sort_softmax(train_attack_enum, model,\n",
    "                                                      defense_model, criterion, defense_criterion, optimizer,\n",
    "                                                      defense_optimizer, epoch, use_cuda)\n",
    "\n",
    "    print('train acc:', train_acc)\n",
    "    test_attack_enum = enumerate(zip(train_te_attack, test_te_attack))\n",
    "    test_loss, test_acc, sum_correct = test_attack_sort_softmax(test_attack_enum, model, defense_model,\n",
    "                                                                criterion, defense_criterion, optimizer,\n",
    "                                                                defense_optimizer,\n",
    "                                                                epoch, use_cuda)\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "# save model\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    if is_best or epoch + 1 == epochs:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': defense_model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': defense_optimizer.state_dict(),\n",
    "        }, False, checkpoint=checkpoint_path, filename='softmax_sort_NN_best')\n",
    "\n",
    "    print('test acc', test_acc, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class black_box_benchmarks(object):\n",
    "    \n",
    "    def __init__(self, shadow_train_performance, shadow_test_performance, \n",
    "                 target_train_performance, target_test_performance, num_classes):\n",
    "        '''\n",
    "        each input contains both model predictions (shape: num_data*num_classes) and ground-truth labels. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.s_tr_outputs, self.s_tr_labels = shadow_train_performance\n",
    "        self.s_te_outputs, self.s_te_labels = shadow_test_performance\n",
    "        self.t_tr_outputs, self.t_tr_labels = target_train_performance\n",
    "        self.t_te_outputs, self.t_te_labels = target_test_performance\n",
    "        \n",
    "        self.s_tr_corr = (np.argmax(self.s_tr_outputs, axis=1)==self.s_tr_labels).astype(int)\n",
    "        self.s_te_corr = (np.argmax(self.s_te_outputs, axis=1)==self.s_te_labels).astype(int)\n",
    "        self.t_tr_corr = (np.argmax(self.t_tr_outputs, axis=1)==self.t_tr_labels).astype(int)\n",
    "        self.t_te_corr = (np.argmax(self.t_te_outputs, axis=1)==self.t_te_labels).astype(int)\n",
    "        \n",
    "        self.s_tr_conf = np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "        self.s_te_conf = np.array([self.s_te_outputs[i, self.s_te_labels[i]] for i in range(len(self.s_te_labels))])\n",
    "        self.t_tr_conf = np.array([self.t_tr_outputs[i, self.t_tr_labels[i]] for i in range(len(self.t_tr_labels))])\n",
    "        self.t_te_conf = np.array([self.t_te_outputs[i, self.t_te_labels[i]] for i in range(len(self.t_te_labels))])\n",
    "        \n",
    "        self.s_tr_entr = self._entr_comp(self.s_tr_outputs)\n",
    "        self.s_te_entr = self._entr_comp(self.s_te_outputs)\n",
    "        self.t_tr_entr = self._entr_comp(self.t_tr_outputs)\n",
    "        self.t_te_entr = self._entr_comp(self.t_te_outputs)\n",
    "        \n",
    "        self.s_tr_m_entr = self._m_entr_comp(self.s_tr_outputs, self.s_tr_labels)\n",
    "        self.s_te_m_entr = self._m_entr_comp(self.s_te_outputs, self.s_te_labels)\n",
    "        self.t_tr_m_entr = self._m_entr_comp(self.t_tr_outputs, self.t_tr_labels)\n",
    "        self.t_te_m_entr = self._m_entr_comp(self.t_te_outputs, self.t_te_labels)\n",
    "        \n",
    "    \n",
    "    def _log_value(self, probs, small_value=1e-30):\n",
    "        return -np.log(np.maximum(probs, small_value))\n",
    "    \n",
    "    def _entr_comp(self, probs):\n",
    "        return np.sum(np.multiply(probs, self._log_value(probs)),axis=1)\n",
    "    \n",
    "    def _m_entr_comp(self, probs, true_labels):\n",
    "        log_probs = self._log_value(probs)\n",
    "        reverse_probs = 1-probs\n",
    "        log_reverse_probs = self._log_value(reverse_probs)\n",
    "        modified_probs = np.copy(probs)\n",
    "        modified_probs[range(true_labels.size), true_labels] = reverse_probs[range(true_labels.size), true_labels]\n",
    "        modified_log_probs = np.copy(log_reverse_probs)\n",
    "        modified_log_probs[range(true_labels.size), true_labels] = log_probs[range(true_labels.size), true_labels]\n",
    "        return np.sum(np.multiply(modified_probs, modified_log_probs),axis=1)\n",
    "    \n",
    "    def _thre_setting(self, tr_values, te_values):\n",
    "        value_list = np.concatenate((tr_values, te_values))\n",
    "        thre, max_acc = 0, 0\n",
    "        for value in value_list:\n",
    "            tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "            te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "            acc = 0.5*(tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        return thre\n",
    "    \n",
    "    def _mem_inf_via_corr(self):\n",
    "        # perform membership inference attack based on whether the input is correctly classified or not\n",
    "        t_tr_acc = np.sum(self.t_tr_corr)/(len(self.t_tr_corr)+0.0)\n",
    "        t_te_acc = np.sum(self.t_te_corr)/(len(self.t_te_corr)+0.0)\n",
    "        mem_inf_acc = 0.5*(t_tr_acc + 1 - t_te_acc)\n",
    "        print('For membership inference attack via correctness, the attack acc is {acc1:.3f}, with train acc {acc2:.3f} and test acc {acc3:.3f}'.format(acc1=mem_inf_acc, acc2=t_tr_acc, acc3=t_te_acc) )\n",
    "        return\n",
    "    \n",
    "    def _mem_inf_thre(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        # perform membership inference attack by thresholding feature values: the feature can be prediction confidence,\n",
    "        # (negative) prediction entropy, and (negative) modified entropy\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        for num in range(self.num_classes):\n",
    "            thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "            t_tr_mem += np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "            t_te_non_mem += np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "        mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "        print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    def _mem_inf_benchmarks(self, all_methods=True, benchmark_methods=[]):\n",
    "        if (all_methods) or ('correctness' in benchmark_methods):\n",
    "            self._mem_inf_via_corr()\n",
    "        if (all_methods) or ('confidence' in benchmark_methods):\n",
    "            self._mem_inf_thre('confidence', self.s_tr_conf, self.s_te_conf, self.t_tr_conf, self.t_te_conf)\n",
    "        if (all_methods) or ('entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('entropy', -self.s_tr_entr, -self.s_te_entr, -self.t_tr_entr, -self.t_te_entr)\n",
    "        if (all_methods) or ('modified entropy' in benchmark_methods):\n",
    "            self._mem_inf_thre('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr, -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load me evaluation\n",
    "\n",
    "# np.random.seed(1123)\n",
    "r = np.arange(25000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "for i in range(0,5000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_trainset_intest.append(trainset[r[i]+25000])\n",
    "\n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    private_testset_intrain.append(testset[r[i]])\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n",
    "\n",
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc = test(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(private_testloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc = test(private_testloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loss, final_test_acc, s_tr_class, s_tr_correct = test_by_class(private_trainloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_tr_class, t_tr_correct = test_by_class(private_trainloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, s_te_class, s_te_correct = test_by_class(private_testloader_intrain, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "test_loss, final_test_acc, t_te_class, t_te_correct = test_by_class(private_testloader_intest, model, criterion, epoch, use_cuda)\n",
    "print ('Classification accuracy: %.2f'%(final_test_acc))\n",
    "\n",
    "print( s_tr_class + t_tr_class)\n",
    "print(s_tr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_acc = (s_tr_correct + t_tr_correct)/(s_tr_class + t_tr_class)\n",
    "print(train_acc)\n",
    "test_acc = (s_te_correct + t_te_correct)/(s_te_class+t_te_class)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "\n",
    "\n",
    "def _model_predictions(model, dataloader):\n",
    "    return_outputs, return_labels = [], []\n",
    "\n",
    "    for (inputs, labels) in dataloader:\n",
    "        return_labels.append(labels.numpy())\n",
    "        outputs, h1, h2, h3, h4, h5, h6, h7 = model.forward(inputs.cuda()) \n",
    "        return_outputs.append( softmax_by_row(outputs.data.cpu().numpy()) )\n",
    "    return_outputs = np.concatenate(return_outputs)\n",
    "    return_labels = np.concatenate(return_labels)\n",
    "    return (return_outputs, return_labels)\n",
    "\n",
    "shadow_train_performance = _model_predictions(model, private_trainloader_intrain)\n",
    "shadow_test_performance = _model_predictions(model, private_testloader_intrain)\n",
    "\n",
    "target_train_performance = _model_predictions(model, private_trainloader_intest)\n",
    "target_test_performance = _model_predictions(model, private_testloader_intest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target_test_performance[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Perform membership inference attacks!!!')\n",
    "MIA = black_box_benchmarks(shadow_train_performance,shadow_test_performance,\n",
    "                     target_train_performance,target_test_performance,num_classes=100)\n",
    "res = MIA._mem_inf_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "risk_score = calculate_risk_score(MIA.s_tr_m_entr, MIA.s_te_m_entr, MIA.s_tr_labels, MIA.s_te_labels, MIA.t_tr_m_entr, MIA.t_tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=22)\n",
    "matplotlib.rc('ytick', labelsize=20)\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 22,\n",
    "        }\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "matplotlib.rcParams['ps.useafm'] = True\n",
    "matplotlib.rcParams['pdf.use14corefonts'] = True\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "params = {'legend.fontsize': 18}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_percent(y,position):\n",
    "    return str(y/5000.)+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.sort(risk_score)\n",
    "y = range(risk_score.size)\n",
    "plt.plot(x,y, label = 'risk score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "np.sum(risk_score<=0.5)\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*(i+1)))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size),'-', label = 'Shadow trainset')\n",
    "# plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),'-',  label = 'Shadow testset')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(res.t_tr_m_entr.size), '-', linewidth = 3, label = 'Test trainset')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(res.t_tr_m_entr.size), '-.',linewidth = 3, label = 'Test testset')\n",
    "# plt.plot(s1,range(risk_score.size)) #, label = 's_tr_m_entr')\n",
    "# plt.plot(s2,range(risk_score.size)) #,  label = 's_te_m_entr')\n",
    "# plt.plot(s3,range(risk_score.size)) #,  label = 't_tr_m_entr')\n",
    "# plt.plot(s4,range(risk_score.size)) #,  label = 't_te_m_entr')\n",
    "\n",
    "plt.yticks(range(0,5500,1000))\n",
    "# plt.xticks([0,50,100,150])\n",
    "# plt.xlim(3,5)\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.subplots_adjust(left=0.13, right=0.9, top=0.9, bottom=0.13)\n",
    "# plt.title('Normal model')\n",
    "plt.xlabel(\"Modified entropy value\")\n",
    "plt.ylabel(\"Cumulative distribution \")\n",
    "pyplot.legend(loc=4)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def kl(p, q):\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "    \n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kl(np.sort(res.t_tr_m_entr), np.sort(res.t_te_m_entr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dist = np.linalg.norm(np.sort(res.t_tr_m_entr) - np.sort(res.t_te_m_entr))\n",
    "dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tr_sort = np.sort(res.t_tr_m_entr) \n",
    "te_sort = np.sort(res.t_te_m_entr)\n",
    "\n",
    "i = 0.2\n",
    "j = 2.6\n",
    "n = 100\n",
    "t = i\n",
    "list_tr = []\n",
    "tmp0 = 0\n",
    "for ii in range(n):\n",
    "    tmp1 = np.sum(tr_sort<=t)\n",
    "    tmp2 = tmp1 - tmp0\n",
    "    list_tr.append(tmp2/5000.0)\n",
    "    tmp0 = tmp1\n",
    "    print(tmp2, t)\n",
    "    t += 0.2\n",
    "    \n",
    "list_te = []\n",
    "tmp0 = 0\n",
    "i = 0.2\n",
    "t = i\n",
    "for ii in range(n):\n",
    "    tmp1 = np.sum(te_sort<=t)\n",
    "    tmp2 = tmp1 - tmp0\n",
    "    list_te.append(tmp2/5000.0)\n",
    "    tmp0 = tmp1\n",
    "    print(tmp2, t)\n",
    "    t += 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def kl(p, q):\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "    \n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "kl(np.asarray(list_tr), np.asarray(list_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "0.5 * np.sum(np.abs(np.asarray(list_tr) - np.asarray(list_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size),'-', label = 'Shadow trainset')\n",
    "# plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),'-',  label = 'Shadow testset')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(res.t_tr_m_entr.size), '-', linewidth = 3, label = 'Test trainset')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(res.t_tr_m_entr.size), '-.',linewidth = 3, label = 'Test testset')\n",
    "# plt.plot(s1,range(risk_score.size)) #, label = 's_tr_m_entr')\n",
    "# plt.plot(s2,range(risk_score.size)) #,  label = 's_te_m_entr')\n",
    "# plt.plot(s3,range(risk_score.size)) #,  label = 't_tr_m_entr')\n",
    "# plt.plot(s4,range(risk_score.size)) #,  label = 't_te_m_entr')\n",
    "\n",
    "plt.yticks(range(0,5500,1000))\n",
    "plt.xticks([0,50,100,150])\n",
    "# plt.xlim(3,5)\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.subplots_adjust(left=0.13, right=0.9, top=0.9, bottom=0.13)\n",
    "plt.title('Normal model')\n",
    "plt.xlabel(\"Modified entropy value\")\n",
    "plt.ylabel(\"Cumulative distribution \")\n",
    "pyplot.legend(loc=4)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "save_file_name = 'me_base_c100a1000_cifar100.pdf'\n",
    "\n",
    "pp = PdfPages(save_file_name)\n",
    "pp.savefig(bbox_inches=\"tight\")\n",
    "pp.close()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size),'-', label = 'Shadow trainset')\n",
    "# plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),'-',  label = 'Shadow testset')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(res.t_tr_m_entr.size), '-', linewidth = 3, label = 'Test trainset')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(res.t_tr_m_entr.size), '-.',linewidth = 3, label = 'Test testset')\n",
    "# plt.plot(s1,range(risk_score.size)) #, label = 's_tr_m_entr')\n",
    "# plt.plot(s2,range(risk_score.size)) #,  label = 's_te_m_entr')\n",
    "# plt.plot(s3,range(risk_score.size)) #,  label = 't_tr_m_entr')\n",
    "# plt.plot(s4,range(risk_score.size)) #,  label = 't_te_m_entr')\n",
    "\n",
    "plt.yticks(range(0,5500,1000))\n",
    "plt.xticks([0,50,100,150])\n",
    "# plt.xlim(3,5)\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.subplots_adjust(left=0.13, right=0.9, top=0.9, bottom=0.13)\n",
    "plt.title('Normal model')\n",
    "plt.xlabel(\"Modified entropy value\")\n",
    "plt.ylabel(\"Cumulative distribution \")\n",
    "pyplot.legend(loc=4)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "save_file_name = 'me_base_fc50a20_cifar100.pdf'\n",
    "\n",
    "pp = PdfPages(save_file_name)\n",
    "pp.savefig(bbox_inches=\"tight\")\n",
    "pp.close()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cifar100_epoch21_infer_c100a1000\n",
    "\n",
    "# with train acc 0.686 and test acc 0.430\n",
    "# For membership inference attack via correctness, the attack acc is 0.628, with train acc 0.686 and test acc 0.430\n",
    "# For membership inference attack via confidence, the attack acc is 0.635\n",
    "# For membership inference attack via entropy, the attack acc is 0.555\n",
    "# For membership inference attack via modified entropy, the attack acc is 0.638\n",
    "\n",
    "x = np.sort(risk_score)\n",
    "y = range(risk_score.size)\n",
    "plt.plot(x,y, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "np.sum(risk_score<=0.5)\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.hist(res.s_tr_conf, bins=20, label = 't_tr_m_entr')\n",
    "plt.hist(res.s_te_conf, bins=20, label = 't_te_m_entr')\n",
    "plt.hist(res.t_tr_conf, bins=20, label = 's_tr_m_entr')     #, density=True, log=True\n",
    "plt.hist(res.t_te_conf, bins=20, label = 's_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cifar100_alexnet_epoch19_infer_c100a1000\n",
    "\n",
    "# with train acc 0.599 and test acc 0.412\n",
    "# For membership inference attack via correctness, the attack acc is 0.594, \n",
    "# For membership inference attack via confidence, the attack acc is 0.595\n",
    "# For membership inference attack via entropy, the attack acc is 0.538\n",
    "# For membership inference attack via modified entropy, the attack acc is 0.597\n",
    "\n",
    "x = np.sort(risk_score)\n",
    "y = range(risk_score.size)\n",
    "plt.plot(x,y, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "np.sum(risk_score<=0.5)\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.hist(res.s_tr_conf, bins=20, label = 't_tr_m_entr')\n",
    "plt.hist(res.s_te_conf, bins=20, label = 't_te_m_entr')\n",
    "plt.hist(res.t_tr_conf, bins=20, label = 's_tr_m_entr')     #, density=True, log=True\n",
    "plt.hist(res.t_te_conf, bins=20, label = 's_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cifar100_min_var_beta1000_conv90a2_nonzero_defense_epoch44 infer c80a2 fc50_a20\n",
    "\n",
    "# with train acc 0.605 and test acc 0.434\n",
    "# For membership inference attack via correctness, the attack acc is 0.585, \n",
    "# For membership inference attack via confidence, the attack acc is 0.589\n",
    "# For membership inference attack via entropy, the attack acc is 0.556\n",
    "# For membership inference attack via modified entropy, the attack acc is 0.589\n",
    "\n",
    "\n",
    "x = np.sort(risk_score)\n",
    "y = range(risk_score.size)\n",
    "plt.plot(x,y, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "np.sum(risk_score<=0.5)\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.hist(res.s_tr_conf, bins=100, label = 't_tr_m_entr')\n",
    "plt.hist(res.s_te_conf, bins=100, label = 't_te_m_entr')\n",
    "plt.hist(res.t_tr_conf, bins=100, label = 's_tr_m_entr')     #, density=True, log=True\n",
    "plt.hist(res.t_te_conf, bins=100, label = 's_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "self = res\n",
    "np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "\n",
    "t_tr_mem, t_te_non_mem = 0, 0\n",
    "class_mem_res = []\n",
    "class_nonmem_res = []\n",
    "class_acc = []\n",
    "class_mem_num = []\n",
    "class_nonmem_num = []\n",
    "s_tr_values = self.s_tr_conf\n",
    "s_te_values = self.s_te_conf\n",
    "t_tr_values = self.t_tr_conf\n",
    "t_te_values = self.t_te_conf\n",
    "v_name = 'confidence'\n",
    "for num in range(self.num_classes):\n",
    "    thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "    class_mem = np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "    class_nonmem = np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "    class_mem_res.append(class_mem)\n",
    "    class_nonmem_res.append(class_nonmem)\n",
    "    t_tr_mem += class_mem\n",
    "    t_te_non_mem += class_nonmem\n",
    "    class_mem_num.append(np.sum(self.t_tr_labels==num))\n",
    "    class_nonmem_num.append(np.sum(self.t_te_labels==num))\n",
    "    class_acc.append( 0.5*(class_mem/(np.sum(self.t_tr_labels==num)+0.0) + class_nonmem/(np.sum(self.t_te_labels==num)+0.0)) )\n",
    "    \n",
    "mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "\n",
    "class_mem = np.asarray(class_mem_res)\n",
    "class_nonmem = np.asarray(class_nonmem_res)\n",
    "class_acc = np.asarray(class_acc)\n",
    "class_mem_num = np.asarray(class_mem_num)\n",
    "class_nonmem_num = np.asarray(class_nonmem_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "s_tr_class = []\n",
    "s_te_class = []\n",
    "t_tr_class = []\n",
    "t_te_class = []\n",
    "for num in range(self.num_classes):\n",
    "    s_tr_labels_num = np.sum(self.s_tr_labels == num)\n",
    "    s_te_labels_num = np.sum(self.s_te_labels == num)\n",
    "    t_tr_labels_num = np.sum(self.t_tr_labels == num)\n",
    "    t_te_labels_num = np.sum(self.t_te_labels == num)\n",
    "    s_tr_class.append(s_tr_labels_num)\n",
    "    s_te_class.append(s_te_labels_num)\n",
    "    t_tr_class.append(t_tr_labels_num)\n",
    "    t_te_class.append(t_te_labels_num)\n",
    "#     print(s_tr_labels_num,\n",
    "# s_te_labels_num,\n",
    "# t_tr_labels_num,\n",
    "# t_te_labels_num)\n",
    "\n",
    "s_tr_class = np.asarray(s_tr_class)\n",
    "s_te_class = np.asarray(s_te_class)\n",
    "t_tr_class = np.asarray(t_tr_class)\n",
    "t_te_class = np.asarray(t_te_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset_class = s_tr_class + t_tr_class\n",
    "t_class_index = np.argsort(trainset_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.sum(self.t_te_labels==0)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "plt.plot(class_mem_num[t_class_index], label = 'class_mem')\n",
    "plt.plot(class_nonmem_num[t_class_index],  label = 'class_nonmem')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mem_index = np.argsort(class_mem)\n",
    "mem_index\n",
    "class_acc\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(class_acc), label = 'sort class_attack_acc')\n",
    "plt.plot(class_acc[t_class_index], label = 'class_attack_acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "att_index = np.argsort(class_acc)\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(np.sort(test_acc), label = 'sort class_acc')\n",
    "# plt.plot(test_acc[t_class_index], label = 'class_acc')\n",
    "plt.plot(class_acc[att_index], label = 'sort class_attack_acc')\n",
    "plt.plot(test_acc[att_index], label = 'class_acc_sort_by_attack_acc')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(class_mem, label = 'class_mem')\n",
    "plt.plot(class_nonmem,  label = 'class_nonmem')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mem_index = np.argsort(class_mem)\n",
    "nonmem_index = np.argsort(class_nonmem)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(class_mem[mem_index], label = 'class_mem')\n",
    "plt.plot(class_nonmem[mem_index],  label = 'class_nonmem')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(class_mem[nonmem_index], label = 'class_mem')\n",
    "plt.plot(class_nonmem[nonmem_index],  label = 'class_nonmem')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cifar100_min_var_beta1000_conv90a2_nonzero_defense_epoch44 infer c70a2 c100a1000\n",
    "\n",
    "# with train acc 0.551 and test acc 0.409\n",
    "# For membership inference attack via correctness, the attack acc is 0.571, \n",
    "# For membership inference attack via confidence, the attack acc is 0.580\n",
    "# For membership inference attack via entropy, the attack acc is 0.536\n",
    "# For membership inference attack via modified entropy, the attack acc is 0.581\n",
    "\n",
    "\n",
    "x = np.sort(risk_score)\n",
    "y = range(risk_score.size)\n",
    "plt.plot(x,y, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n",
    "np.sum(risk_score<=0.5)\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*(i+1)))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.hist(res.t_tr_m_entr, label = 't_tr_m_entr')\n",
    "plt.hist(res.t_te_m_entr, label = 't_te_m_entr')\n",
    "plt.hist(res.s_tr_m_entr, label = 's_tr_m_entr')     #, density=True, log=True\n",
    "plt.hist(res.s_te_m_entr, label = 's_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cifar100_min_var_beta1000_conv90a2_nonzero_defense_epoch44 infer c60a2\n",
    "\n",
    "# print(risk_score)\n",
    "plt.plot(np.sort(risk_score), label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(risk_score)\n",
    "plt.plot(np.sort(risk_score), label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(risk_score)\n",
    "plt.plot(np.sort(risk_score1), label = 'risk_score')\n",
    "plt.plot(np.sort(risk_score), label = 'cifar100_epoch21')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score1, bins=20, label = 'risk_score')\n",
    "plt.hist(risk_score, bins=20, label = 'cifar100_epoch21')\n",
    "\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    tmp1 = np.sum(risk_score1>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp, tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(risk_score)\n",
    "plt.plot(np.sort(risk_score), label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(risk_score, bins=20, label = 'risk_score')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "count = []\n",
    "for i in range(10):\n",
    "    tmp = np.sum(risk_score>(0.1*i+0.1))\n",
    "    count.append(tmp)\n",
    "    print(0.1*i+0.1, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res.s_tr_m_entr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "# #     plt.plot(f_evaluate_origin[i])\n",
    "#     if test_label[i] == 0:\n",
    "#         plt.plot(res.s_tr_m_entr)\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_m_entr), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_m_entr), label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_m_entr), label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_m_entr), label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.hist(res.s_tr_m_entr, bins=100, label = 's_tr_m_entr')     #, density=True, log=True\n",
    "plt.hist(res.s_te_m_entr, bins=100, label = 's_te_m_entr')\n",
    "plt.hist(res.t_tr_m_entr, bins=100, label = 't_tr_m_entr')\n",
    "plt.hist(res.t_te_m_entr, bins=100, label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load class wise \n",
    "\n",
    "self = res\n",
    "np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "\n",
    "t_tr_mem, t_te_non_mem = 0, 0\n",
    "class_mem_res = []\n",
    "class_nonmem_res = []\n",
    "class_attack_acc = []\n",
    "class_mem_num = []\n",
    "class_nonmem_num = []\n",
    "conf_thre = []\n",
    "s_tr_values = self.s_tr_conf\n",
    "s_te_values = self.s_te_conf\n",
    "t_tr_values = self.t_tr_conf\n",
    "t_te_values = self.t_te_conf\n",
    "v_name = 'confidence'\n",
    "for num in range(self.num_classes):\n",
    "    thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "    conf_thre.append(thre)\n",
    "    class_mem = np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "    class_nonmem = np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "    class_mem_res.append(class_mem)\n",
    "    class_nonmem_res.append(class_nonmem)\n",
    "    t_tr_mem += class_mem\n",
    "    t_te_non_mem += class_nonmem\n",
    "    class_mem_num.append(np.sum(self.t_tr_labels==num))\n",
    "    class_nonmem_num.append(np.sum(self.t_te_labels==num))\n",
    "    class_attack_acc.append( 0.5*(class_mem/(np.sum(self.t_tr_labels==num)+0.0) + class_nonmem/(np.sum(self.t_te_labels==num)+0.0)) )\n",
    "    \n",
    "mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "\n",
    "class_mem_res = np.asarray(class_mem_res)\n",
    "class_nonmem_res = np.asarray(class_nonmem_res)\n",
    "class_attack_acc = np.asarray(class_attack_acc)\n",
    "class_mem_num = np.asarray(class_mem_num)\n",
    "class_nonmem_num = np.asarray(class_nonmem_num)\n",
    "conf_thre = np.asarray(conf_thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(res.s_tr_conf),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(res.s_te_conf),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(res.t_tr_conf),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(res.t_te_conf),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t_tr_acc =  t_tr_correct/ t_tr_class\n",
    "# print(t_tr_acc )\n",
    "t_te_acc = t_te_correct/t_te_class\n",
    "# print(t_te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.plot(np.sort(conf_thre), label = 'conf_thre')\n",
    "\n",
    "# plt.plot(class_attack_acc, label = 'class_attack_acc')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "conf_thre[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "acc_gap = train_acc - test_acc\n",
    "gap_index = np.argsort(acc_gap)\n",
    "\n",
    "t_acc_gap = t_tr_acc - t_te_acc\n",
    "t_gap_index = np.argsort(t_acc_gap)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "# plt.plot(train_acc[gap_index], label = 'train_acc')\n",
    "# plt.plot(test_acc[gap_index],  label = 'test_acc')\n",
    "# plt.plot(t_tr_acc[t_gap_index], label = 't_tr_acc')\n",
    "# plt.plot(t_te_acc[t_gap_index], label = 't_te_acc')\n",
    "plt.plot(class_attack_acc[t_gap_index], label = 'class_attack_acc')\n",
    "plt.plot(t_acc_gap[t_gap_index],  label = 't_acc_gap')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load class wise  m_entr_thre\n",
    "\n",
    "self = res\n",
    "# np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "\n",
    "t_tr_mem, t_te_non_mem = 0, 0\n",
    "class_mem_res = []\n",
    "class_nonmem_res = []\n",
    "class_attack_acc = []\n",
    "class_mem_num = []\n",
    "class_nonmem_num = []\n",
    "m_entr_thre = []\n",
    "s_tr_values = -self.s_tr_m_entr\n",
    "s_te_values = -self.s_te_m_entr\n",
    "t_tr_values = -self.t_tr_m_entr\n",
    "t_te_values = -self.t_te_m_entr\n",
    "v_name = 'modified entropy'\n",
    "for num in range(self.num_classes):\n",
    "    thre = self._thre_setting(s_tr_values[self.s_tr_labels==num], s_te_values[self.s_te_labels==num])\n",
    "#     thre = -2\n",
    "    m_entr_thre.append(thre)\n",
    "    class_mem = np.sum(t_tr_values[self.t_tr_labels==num]>=thre)\n",
    "    class_nonmem = np.sum(t_te_values[self.t_te_labels==num]<thre)\n",
    "    class_mem_res.append(class_mem)\n",
    "    class_nonmem_res.append(class_nonmem)\n",
    "    t_tr_mem += class_mem\n",
    "    t_te_non_mem += class_nonmem\n",
    "    class_mem_num.append(np.sum(self.t_tr_labels==num))\n",
    "    class_nonmem_num.append(np.sum(self.t_te_labels==num))\n",
    "    class_attack_acc.append( 0.5*(class_mem/(np.sum(self.t_tr_labels==num)+0.0) + class_nonmem/(np.sum(self.t_te_labels==num)+0.0)) )\n",
    "    \n",
    "mem_inf_acc = 0.5*(t_tr_mem/(len(self.t_tr_labels)+0.0) + t_te_non_mem/(len(self.t_te_labels)+0.0))\n",
    "print('For membership inference attack via {n}, the attack acc is {acc:.3f}'.format(n=v_name,acc=mem_inf_acc))\n",
    "\n",
    "class_mem_res = np.asarray(class_mem_res)\n",
    "class_nonmem_res = np.asarray(class_nonmem_res)\n",
    "class_attack_acc = np.asarray(class_attack_acc)\n",
    "class_mem_num = np.asarray(class_mem_num)\n",
    "class_nonmem_num = np.asarray(class_nonmem_num)\n",
    "m_entr_thre = np.asarray(m_entr_thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# plt.ylim(0, 0.7)\n",
    "plt.plot(np.sort(-res.s_tr_m_entr),range(risk_score.size), label = 's_tr_m_entr')\n",
    "plt.plot(np.sort(-res.s_te_m_entr),range(risk_score.size),  label = 's_te_m_entr')\n",
    "plt.plot(np.sort(-res.t_tr_m_entr),range(risk_score.size),  label = 't_tr_m_entr')\n",
    "plt.plot(np.sort(-res.t_te_m_entr),range(risk_score.size),  label = 't_te_m_entr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.sort(-res.t_te_m_entr)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t_tr_acc =  t_tr_correct/ t_tr_class\n",
    "# print(t_tr_acc )\n",
    "t_te_acc = t_te_correct/t_te_class\n",
    "# print(t_te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m_entr_index = np.argsort(m_entr_thre)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.plot(m_entr_thre[m_entr_index], label = 'm_entr_thre')\n",
    "\n",
    "# plt.plot(class_attack_acc, label = 'class_attack_acc')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "m_entr_thre[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "acc_gap = train_acc - test_acc\n",
    "gap_index = np.argsort(acc_gap)\n",
    "\n",
    "t_acc_gap = t_tr_acc - t_te_acc\n",
    "t_gap_index = np.argsort(t_acc_gap)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "# plt.plot(train_acc[gap_index], label = 'train_acc')\n",
    "# plt.plot(test_acc[gap_index],  label = 'test_acc')\n",
    "# plt.plot(t_tr_acc[t_gap_index], label = 't_tr_acc')\n",
    "# plt.plot(t_te_acc[t_gap_index], label = 't_te_acc')\n",
    "plt.plot(class_attack_acc[t_gap_index], label = 'class_attack_acc')\n",
    "plt.plot(t_acc_gap[t_gap_index],  label = 't_acc_gap')\n",
    "# plt.plot(class_attack_acc[m_entr_index], label = 'class_attack_acc')\n",
    "# plt.plot(t_acc_gap[m_entr_index],  label = 't_acc_gap')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "acc_gap = train_acc - test_acc\n",
    "gap_index = np.argsort(acc_gap)\n",
    "\n",
    "t_acc_gap = t_tr_acc - t_te_acc\n",
    "t_gap_index = np.argsort(t_acc_gap)\n",
    "\n",
    "class_attack_acc_index = np.argsort(class_attack_acc)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# plt.ylim(0, 0.7)\n",
    "# plt.plot(class_mem_num, label = 'class_mem')\n",
    "# plt.plot(class_nonmem_num,  label = 'class_nonmem')\n",
    "# plt.plot(train_acc[gap_index], label = 'train_acc')\n",
    "# plt.plot(test_acc[gap_index],  label = 'test_acc')\n",
    "# plt.plot(t_tr_acc[t_gap_index], label = 't_tr_acc')\n",
    "# plt.plot(t_te_acc[t_gap_index], label = 't_te_acc')\n",
    "plt.plot(class_attack_acc[class_attack_acc_index], label = 'class_attack_acc')\n",
    "plt.plot(t_acc_gap[class_attack_acc_index],  label = 't_acc_gap')\n",
    "# plt.plot(class_attack_acc[m_entr_index], label = 'class_attack_acc')\n",
    "# plt.plot(t_acc_gap[m_entr_index],  label = 't_acc_gap')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res.t_te_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
